{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOklosRfM5MqcG/dHzX/8mA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Handlingclassimbalance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHqy4Mr-rDqP"
      },
      "outputs": [],
      "source": [
        "#example class to handle class imbalance by looking into various samplinb techniques like SMOTE (Minority class over sampling), Stratified Sampling\n",
        "#why do we use F1 as metric to evaluate class imbalance as Precision = tp/tp+fp and Recall = tp/tp+fn\n",
        "#Credit - https://www.geeksforgeeks.org/machine-learning/handling-imbalanced-data-for-classification/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#need to handling class imbalance as -> machine learning models get biased to majority class and that hampers generalization and leads to overfitting\n",
        "#overfitting should be avoided and addressed"
      ],
      "metadata": {
        "id": "bRha2rBprJz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's code\n",
        "\n",
        "#case for RandomSampler and balancing the majority and miniority classes\n",
        "import numpy as np\n",
        "from sklearn.datasets import  make_classification\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#create a class imbalanced dataset\n",
        "X,y = make_classification(n_classes = 2 , class_sep = 2 , weights = [0.1,0.9],\n",
        "                          n_informative = 3 , n_redundant = 1, flip_y=0,\n",
        "                          n_features = 20,n_clusters_per_class = 1 ,\n",
        "                          n_samples = 1000, random_state = 42)\n",
        "print(\"Original Class Distribution:\", Counter(y))\n",
        "\n",
        "#oversampling using RandomSampler\n",
        "oversample = RandomOverSampler(sampling_strategy = 'minority')\n",
        "x_over, y_over = oversample.fit_resample(X,y)\n",
        "print(\"Random Oversampling Class Distribution:\", Counter(y_over))\n",
        "\n",
        "#undersampling using RandomSampler\n",
        "undersample = RandomUnderSampler(sampling_strategy = 'majority')\n",
        "x_under, y_under = undersample.fit_resample(X,y)\n",
        "#yields a balanced dataset of classes --------------------------------->\n",
        "print(\"Random Undersampling Class Distribution:\", Counter(y_under))"
      ],
      "metadata": {
        "id": "aI1vA5NitmLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.ensemble import BalancedBaggingClassifier\n",
        "from sklearn.metrics import accuracy_score , classification_report\n",
        "\n",
        "\n",
        "#create a class imbalanced dataset\n",
        "X,y = make_classification(n_classes = 2 , class_sep = 2 , weights = [0.1,0.9],\n",
        "                          n_informative = 3 , n_redundant = 1, flip_y=0,\n",
        "                          n_features = 20,n_clusters_per_class = 1 ,\n",
        "                          n_samples = 1000, random_state = 42)\n",
        "#split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "base_classifier = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
        "\n",
        "#create a Balanaced Bagging Classifier\n",
        "balanced_bagging_classifier = BalancedBaggingClassifier(base_classifier,\n",
        "                                                        n_estimators = 10,\n",
        "                                                        sampling_strategy = 'auto',\n",
        "                                                        replacement = False,\n",
        "                                                        random_state = 42)\n",
        "\n",
        "#fit the model\n",
        "balanced_bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "#make predictions\n",
        "y_pred = balanced_bagging_classifier.predict(X_test)\n",
        "\n",
        "#evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "print(\"Classification Report:\\n\",classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "suY1tUf-v_eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SMOTE\n",
        "#SMOTE uses k-NN for synthetically oversampling the minority class\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "#create an imbalanced dataset\n",
        "#create a class imbalanced dataset\n",
        "X,y = make_classification(n_classes = 2 , class_sep = 2 , weights = [0.1,0.9],\n",
        "                          n_informative = 3 , n_redundant = 1, flip_y=0,\n",
        "                          n_features = 20,n_clusters_per_class = 1 ,\n",
        "                          n_samples = 1000, random_state = 42)\n",
        "#split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "#display class distribution before SMOTE\n",
        "print(\"Original Class Distribution before SMOTE:\", Counter(y_train))\n",
        "\n",
        "#apply SMOTE to oversample minority class\n",
        "smote = SMOTE(sampling_strategy = 'auto', random_state = 42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "#display class distribution after SMOTE\n",
        "print(\"Class Distribution after SMOTE:\", Counter(y_train_smote))\n",
        "\n"
      ],
      "metadata": {
        "id": "-TUidyRszhGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Threshold Moving to handle class imbalance\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score , roc_auc_score\n",
        "\n",
        "#create an imbalance dataset\n",
        "X,y = make_classification(n_classes = 2 , class_sep = 2 , weights = [0.1,0.9],\n",
        "                          n_informative = 3 , n_redundant = 1, flip_y=0,\n",
        "                          n_features = 20,n_clusters_per_class = 1 ,\n",
        "                          n_samples = 1000, random_state = 42)\n",
        "#split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "#train a classification model (Random Forest as an example)\n",
        "model = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#predit the probabilities\n",
        "y_pred_proba = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "#define a threshold for classification\n",
        "threshold = 0.5\n",
        "\n",
        "#adjust the threshold based on your criteria\n",
        "while threshold >=0:\n",
        "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    print(f\"Threshold:{threshold:.2f} - F1 Score:{f1:.4f}\")\n",
        "    threshold -= 0.02"
      ],
      "metadata": {
        "id": "4DOu9qzc1F3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RFE is leveraging models to determine which set of features are most important - in our case in MMM models we used Lasso to do RFE and then used the features in Ridge Regression\n",
        "#Rationale for using RFE is we need to have better accuracy of models with righ features else you have 200 data points\n",
        "#another technique that we used for FE is Correlation between two Covariates and also tested correlation between depdendent and independent variables\n",
        "#Regression is used to model a linear relationship between dependent and independent\n",
        "#Sales is time series data and data points are time series data\n",
        "#glm - - diff is constrained - UB and LB and then use LGBFS to get to tehe coeff\n",
        "#PYMC3 - Hirearchial - is used when you have National Level Spends and you want to measure the impact of that on DMA level and that can't exceed a certain limit\n",
        "#so that is where the usage comes of these models\n",
        "#TTFN are CF .... and are ID based .... and hence to handle cold start (when user or item is new)\n",
        "#the way to handle is map these new users and items to a bucket and that solves - Real Time Recomm Models\n",
        "#Built Next Best Action Recommendation Engine leveraging Two Tower Neural Networks, resulting to 20% uptick in revenue for the Retail and Banking customer\n",
        "#Retrieval Model was batch - generate embeddings and store them in VDB and Index - using FAISS - Bi Encoder\n",
        "#Ranking is online inference - where you rank based on user features\n",
        "#Niche of model is - leveraging customer sequence , comments as ranker.... and we use cross feature interactions  to caluate nuances of item and user interaction\n",
        "#Demand Sensing and Forecasting for a CPG Brand - Soap Category . Signals used were - relatively stable category ....\n",
        "#DF - batch base for every month we do\n",
        "#DS - Orders data (demand) -> Dependent is Demand/ Orders -> : CPG on Amazon\n",
        "#1)Price , Trade Data (0-2% , 2-5% ,>5%) , Marketing Promo(run a campain y/n) . Comp Price, Comp offer. Positive . Negative Sentiments on the product (1/0)\n",
        "#2)Tertiray Sales data (POS sales data from Amazon)\n",
        "#3)Inventory data (supply)\n",
        "#4)Retail inflation\n",
        "#5)Seasonality , Holidays , Trends , Cyclicity is key part ... you take these things out .... detrend , deseasonlize the time series , holidays etc...., Summers etc..\n",
        "#6)XGBoost Regression Model - why ? - accuracy\n",
        "#7)Autocorrelartion - acf and pacf plot ....\n",
        "#took last two years data for forecasting\n",
        "#time window split for CV of model\n",
        "#predicted demand T+7 and T+15 to align to manufacturing cycle\n",
        "#models were refreshed evey week as planning cycle for fortnightly\n",
        "#amazon data streams were available on daily basis\n",
        "#\n"
      ],
      "metadata": {
        "id": "sEdaETIElxdz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}