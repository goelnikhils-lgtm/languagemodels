{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3lQNCIkAJNk601hBKRagK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Handlingclassimbalance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHqy4Mr-rDqP"
      },
      "outputs": [],
      "source": [
        "#example class to handle class imbalance by looking into various samplinb techniques like SMOTE (Minority class over sampling), Stratified Sampling\n",
        "#why do we use F1 as metric to evaluate class imbalance as Precision = tp/tp+fp and Recall = tp/tp+fn\n",
        "#Credit - https://www.geeksforgeeks.org/machine-learning/handling-imbalanced-data-for-classification/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#need to handling class imbalance as -> machine learning models get biased to majority class and that hampers generalization and leads to overfitting\n",
        "#overfitting should be avoided and addressed"
      ],
      "metadata": {
        "id": "bRha2rBprJz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's code\n",
        "\n",
        "#case for RandomSampler and balancing the majority and miniority classes\n",
        "import numpy as np\n",
        "from sklearn.datasets import  make_classification\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#create a class imbalanced dataset\n",
        "X,y = make_classification(n_classes = 2 , class_sep = 2 , weights = [0.1,0.9],\n",
        "                          n_informative = 3 , n_redundant = 1, flip_y=0,\n",
        "                          n_features = 20,n_clusters_per_class = 1 ,\n",
        "                          n_samples = 1000, random_state = 42)\n",
        "print(\"Original Class Distribution:\", Counter(y))\n",
        "\n",
        "#oversampling using RandomSampler\n",
        "oversample = RandomOverSampler(sampling_strategy = 'minority')\n",
        "x_over, y_over = oversample.fit_resample(X,y)\n",
        "print(\"Random Oversampling Class Distribution:\", Counter(y_over))\n",
        "\n",
        "#undersampling using RandomSampler\n",
        "undersample = RandomUnderSampler(sampling_strategy = 'majority')\n",
        "x_under, y_under = undersample.fit_resample(X,y)\n",
        "#yields a balanced dataset of classes --------------------------------->\n",
        "print(\"Random Undersampling Class Distribution:\", Counter(y_under))"
      ],
      "metadata": {
        "id": "aI1vA5NitmLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.ensemble import BalancedBaggingClassifier\n",
        "from sklearn.metrics import accuracy_score , classification_report\n",
        "\n",
        "\n",
        "#create a class imbalanced dataset\n",
        "X,y = make_classification(n_classes = 2 , class_sep = 2 , weights = [0.1,0.9],\n",
        "                          n_informative = 3 , n_redundant = 1, flip_y=0,\n",
        "                          n_features = 20,n_clusters_per_class = 1 ,\n",
        "                          n_samples = 1000, random_state = 42)\n",
        "#split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "base_classifier = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
        "\n",
        "#create a Balanaced Bagging Classifier\n",
        "balanced_bagging_classifier = BalancedBaggingClassifier(base_classifier,\n",
        "                                                        n_estimators = 10,\n",
        "                                                        sampling_strategy = 'auto',\n",
        "                                                        replacement = False,\n",
        "                                                        random_state = 42)\n",
        "\n",
        "#fit the model\n",
        "balanced_bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "#make predictions\n",
        "y_pred = balanced_bagging_classifier.predict(X_test)\n",
        "\n",
        "#evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "print(\"Classification Report:\\n\",classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "suY1tUf-v_eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SMOTE\n",
        "#SMOTE uses k-NN for synthetically oversampling the minority class\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "#create an imbalanced dataset\n",
        "#create a class imbalanced dataset\n",
        "X,y = make_classification(n_classes = 2 , class_sep = 2 , weights = [0.1,0.9],\n",
        "                          n_informative = 3 , n_redundant = 1, flip_y=0,\n",
        "                          n_features = 20,n_clusters_per_class = 1 ,\n",
        "                          n_samples = 1000, random_state = 42)\n",
        "#split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "#display class distribution before SMOTE\n",
        "print(\"Original Class Distribution before SMOTE:\", Counter(y_train))\n",
        "\n",
        "#apply SMOTE to oversample minority class\n",
        "smote = SMOTE(sampling_strategy = 'auto', random_state = 42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "#display class distribution after SMOTE\n",
        "print(\"Class Distribution after SMOTE:\", Counter(y_train_smote))\n",
        "\n"
      ],
      "metadata": {
        "id": "-TUidyRszhGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Threshold Moving to handle class imbalance\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score , roc_auc_score\n",
        "\n",
        "#create an imbalance dataset\n",
        "X,y = make_classification(n_classes = 2 , class_sep = 2 , weights = [0.1,0.9],\n",
        "                          n_informative = 3 , n_redundant = 1, flip_y=0,\n",
        "                          n_features = 20,n_clusters_per_class = 1 ,\n",
        "                          n_samples = 1000, random_state = 42)\n",
        "#split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "#train a classification model (Random Forest as an example)\n",
        "model = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#predit the probabilities\n",
        "y_pred_proba = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "#define a threshold for classification\n",
        "threshold = 0.5\n",
        "\n",
        "#adjust the threshold based on your criteria\n",
        "while threshold >=0:\n",
        "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    print(f\"Threshold:{threshold:.2f} - F1 Score:{f1:.4f}\")\n",
        "    threshold -= 0.02"
      ],
      "metadata": {
        "id": "4DOu9qzc1F3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FE - Correlation with dependent , RFE ...\n",
        "#RFE is leveraging models to determine which set of features are most important - in our case in MMM models we used Lasso to do RFE and then used the features in Ridge Regression\n",
        "#Rationale for using RFE is we need to have better accuracy of models with righ features else you have 200 data points\n",
        "#another technique that we used for FE is Correlation between two Covariates and also tested correlation between depdendent and independent variables\n",
        "#Regression is used to model a linear relationship between dependent and independent\n",
        "#Sales is time series data and data points are time series data\n",
        "#glm - - diff is constrained - UB and LB and then use LGBFS to get to tehe coeff\n",
        "#PYMC3 - Hirearchial - is used when you have National Level Spends and you want to measure the impact of that on DMA level and that can't exceed a certain limit\n",
        "#so that is where the usage comes of these models\n",
        "#TTFN are CF .... and are ID based .... and hence to handle cold start (when user or item is new)\n",
        "#the way to handle is map these new users and items to a bucket and that solves - Real Time Recomm Models\n",
        "#Built Next Best Action Recommendation Engine leveraging Two Tower Neural Networks, resulting to 20% uptick in revenue for the Retail and Banking customer\n",
        "#Retrieval Model was batch - generate embeddings and store them in VDB and Index - using FAISS - Bi Encoder\n",
        "#Ranking is online inference - where you rank based on user features\n",
        "#Niche of model is - leveraging customer sequence , comments as ranker.... and we use cross feature interactions  to caluate nuances of item and user interaction\n",
        "#Demand Sensing and Forecasting for a CPG Brand - Soap Category . Signals used were - relatively stable category ....\n",
        "#DF - batch base for every month we do\n",
        "#DS - Orders data (demand) -> Dependent is Demand/ Orders -> : CPG on Amazon\n",
        "#1)Price , Trade Data (0-2% , 2-5% ,>5%) , Marketing Promo(run a campain y/n) . Comp Price, Comp offer. Positive . Negative Sentiments on the product (1/0)\n",
        "#2)Tertiray Sales data (POS sales data from Amazon)\n",
        "#3)Inventory data (supply)\n",
        "#4)Retail inflation\n",
        "#5)Seasonality , Holidays , Trends , Cyclicity is key part ... you take these things out .... detrend , deseasonlize the time series , holidays etc...., Summers etc..\n",
        "#Fourier Transform for cyclicity\n",
        "#price impact as leading indicators\n",
        "#holidays\n",
        "#seasonality - wow seasonality calc - calculated separately ... trend calculated seprately and to make time series stationry we use differencing\n",
        "#6)XGBoost Regression Model - why ? - accuracy - additive model\n",
        "#7)Autocorrelartion - acf and pacf plot ....\n",
        "#took last two years data for forecasting\n",
        "#time window split for CV of model\n",
        "#predicted demand T+7 and T+15 to align to manufacturing cycle\n",
        "#models were refreshed evey week as planning cycle for fortnightly\n",
        "#amazon data streams were available on daily basis\n",
        "#metrics is RMSE , MAPE\n",
        "#exponential smoothing to handle seasonality and trends\n",
        "#we used XGBoost as it was we wanted to have more accuracy .... we compared various methods . used stats - holtwinters etc...\n",
        "#Exogenous variables .... and hence XGBoost"
      ],
      "metadata": {
        "id": "sEdaETIElxdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bootstrapping for CI for Classification model\n",
        "#boot strapping with replacement\n",
        "#boot strapping is used to estimate true value (mean , SD or Variance) of a population parameter"
      ],
      "metadata": {
        "id": "R1FawqGbPcac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#full example of CI\n",
        "import numpy as np\n",
        "from pandas import read_csv\n",
        "from sklearn.utils import resample\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib import pyplot\n",
        "\n",
        "#load dataset\n",
        "data = read_csv('/content/sample_data/pima-indians-diabetes.csv', header=None)\n",
        "values = data.values\n",
        "#configure bootstrap\n",
        "n_iterations = 100\n",
        "n_size = int(len(data) * 0.50)\n",
        "#run bootstrap\n",
        "stats = list()\n",
        "for i in range(n_iterations):\n",
        "  train = resample(values, n_samples=n_size)\n",
        "  test = np.array([x for x in values if x.tolist() not in train.tolist()])\n",
        "  #fit model #running classifier 100 times\n",
        "  model = DecisionTreeClassifier()\n",
        "  #train the model\n",
        "  model.fit(train[:,:-1], train[:,-1])\n",
        "  #evaluate model\n",
        "  predictions = model.predict(test[:,:-1])\n",
        "  #statistics of interest for CI is model accuracy score\n",
        "  score = accuracy_score(test[:,-1], predictions)\n",
        "  #print(score)\n",
        "  stats.append(score)\n",
        "#plot score\n",
        "pyplot.hist(stats)\n",
        "pyplot.show()\n",
        "#calc confidence intervals\n",
        "alpha = 0.95\n",
        "p = ((1-alpha)/2) * 100\n",
        "lower = max(0.0, np.percentile(stats, p))\n",
        "p = (alpha + (1-alpha)/2) * 100\n",
        "upper = min(1.0, np.percentile(stats, p))\n",
        "#95% confidence that model predicions/classification lie in 65% to 72.2%\n",
        "print(\"Confidence Interval %.1f confidence interval %.1f%% and %.1f%% \" % (alpha*100, lower*100, upper*100))\n"
      ],
      "metadata": {
        "id": "4RyFxBLPQsPK",
        "outputId": "abd49b1c-c229-485e-83e4-43668cb5954f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confidence Interval 95.0 confidence interval 65.5% and 72.9% \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bootstrapping for CI for regression  model\n",
        "#boot strapping with replacement"
      ],
      "metadata": {
        "id": "AzpBR-3-ZKGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "#1.Generate sample data\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "X = np.random.rand(n_samples,1) *10 #independent variable\n",
        "true_beta_0 = 2 # true intercept\n",
        "true_beta_1 = 0.5 #true slope\n",
        "y = true_beta_0 + true_beta_1 * X + np.random.normal(0,1,n_samples)\n",
        "\n",
        "X_const = sm.add_constant(X)\n",
        "#2. Define no of bootstrap samples\n",
        "n_bootstraps = 1000\n",
        "#3.Intialize a list to store the bootstrapped regression coeff\n",
        "boot_coeffs_list = []\n",
        "\n",
        "#3. Start bootstrapping\n",
        "for i in range(n_bootstraps):\n",
        "    #resample with replacement\n",
        "    indices = np.random.choice(n_samples, n_samples, replace = True)\n",
        "\n",
        "    #create bootstrapped samples\n",
        "    X_bootstrap = X_const[indices]\n",
        "    y_bootstrap = y[indices]\n",
        "\n",
        "    model_boot = sm.OLS(y_bootstrap,X_bootstrap).fit()\n",
        "    boot_coeffs_list.append(model_boot.params)\n",
        "\n",
        "# Convert the list of coefficients to a NumPy array\n",
        "boot_coeffs = np.array(boot_coeffs_list)\n",
        "\n",
        "# Calculate confidence intervals for the coefficients\n",
        "alpha = 0.95\n",
        "p = ((1 - alpha) / 2) * 100\n",
        "lower_bounds = np.percentile(boot_coeffs, p, axis=0)\n",
        "p = (alpha + (1 - alpha) / 2) * 100\n",
        "upper_bounds = np.percentile(boot_coeffs, p, axis=0)\n",
        "\n",
        "# Print the confidence intervals\n",
        "print(f\"{alpha*100:.1f}% Confidence Intervals for Regression Coefficients:\")\n",
        "print(f\"Intercept: [{lower_bounds[0].item():.4f}, {upper_bounds[0].item():.4f}]\")\n",
        "print(f\"Slope: [{lower_bounds[1].item():.4f}, {upper_bounds[1].item():.4f}]\")"
      ],
      "metadata": {
        "id": "4PYL3yFqZOyv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}