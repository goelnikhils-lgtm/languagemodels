{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWmYkh9aiuUehNju1ZBDvy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Handlingclassimbalance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHqy4Mr-rDqP"
      },
      "outputs": [],
      "source": [
        "#example class to handle class imbalance by looking into various samplinb techniques like SMOTE (Minority class over sampling), Stratified Sampling\n",
        "#why do we use F1 as metric to evaluate class imbalance as Precision = tp/tp+fp and Recall = tp/tp+fn\n",
        "#Credit - https://www.geeksforgeeks.org/machine-learning/handling-imbalanced-data-for-classification/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#need to handling class imbalance as -> machine learning models get biased to majority class and that hampers generalization and leads to overfitting\n",
        "#overfitting should be avoided and addressed"
      ],
      "metadata": {
        "id": "bRha2rBprJz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's code\n",
        "\n",
        "#case for RandomSampler and balancing the majority and miniority classes\n",
        "import numpy as np\n",
        "from sklearn.datasets import  make_classification\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#create a class imbalanced dataset\n",
        "X,y = make_classification(n_classes = 2 , class_sep = 2 , weights = [0.1,0.9],\n",
        "                          n_informative = 3 , n_redundant = 1, flip_y=0,\n",
        "                          n_features = 20,n_clusters_per_class = 1 ,\n",
        "                          n_samples = 1000, random_state = 42)\n",
        "print(\"Original Class Distribution:\", Counter(y))\n",
        "\n",
        "#oversampling using RandomSampler\n",
        "oversample = RandomOverSampler(sampling_strategy = 'minority')\n",
        "x_over, y_over = oversample.fit_resample(X,y)\n",
        "print(\"Random Oversampling Class Distribution:\", Counter(y_over))\n",
        "\n",
        "#undersampling using RandomSampler\n",
        "undersample = RandomUnderSampler(sampling_strategy = 'majority')\n",
        "x_under, y_under = undersample.fit_resample(X,y)\n",
        "#yields a balanced dataset of classes --------------------------------->\n",
        "print(\"Random Undersampling Class Distribution:\", Counter(y_under))"
      ],
      "metadata": {
        "id": "aI1vA5NitmLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.ensemble import BalancedBaggingClassifier\n",
        "from sklearn.metrics import accuracy_score , classification_report\n",
        "\n",
        "\n",
        "#create a class imbalanced dataset\n",
        "X,y = make_classification(n_classes = 2 , class_sep = 2 , weights = [0.1,0.9],\n",
        "                          n_informative = 3 , n_redundant = 1, flip_y=0,\n",
        "                          n_features = 20,n_clusters_per_class = 1 ,\n",
        "                          n_samples = 1000, random_state = 42)\n",
        "#split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "base_classifier = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
        "\n",
        "#create a Balanaced Bagging Classifier\n",
        "balanced_bagging_classifier = BalancedBaggingClassifier(base_classifier,\n",
        "                                                        n_estimators = 10,\n",
        "                                                        sampling_strategy = 'auto',\n",
        "                                                        replacement = False,\n",
        "                                                        random_state = 42)\n",
        "\n",
        "#fit the model\n",
        "balanced_bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "#make predictions\n",
        "y_pred = balanced_bagging_classifier.predict(X_test)\n",
        "\n",
        "#evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "print(\"Classification Report:\\n\",classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "suY1tUf-v_eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SMOTE\n",
        "#SMOTE uses k-NN for synthetically oversampling the minority class\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "#create an imbalanced dataset\n",
        "#create a class imbalanced dataset\n",
        "X,y = make_classification(n_classes = 2 , class_sep = 2 , weights = [0.1,0.9],\n",
        "                          n_informative = 3 , n_redundant = 1, flip_y=0,\n",
        "                          n_features = 20,n_clusters_per_class = 1 ,\n",
        "                          n_samples = 1000, random_state = 42)\n",
        "#split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "#display class distribution before SMOTE\n",
        "print(\"Original Class Distribution before SMOTE:\", Counter(y_train))\n",
        "\n",
        "#apply SMOTE to oversample minority class\n",
        "smote = SMOTE(sampling_strategy = 'auto', random_state = 42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "#display class distribution after SMOTE\n",
        "print(\"Class Distribution after SMOTE:\", Counter(y_train_smote))\n",
        "\n"
      ],
      "metadata": {
        "id": "-TUidyRszhGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Threshold Moving to handle class imbalance\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score , roc_auc_score\n",
        "\n",
        "#create an imbalance dataset\n",
        "X,y = make_classification(n_classes = 2 , class_sep = 2 , weights = [0.1,0.9],\n",
        "                          n_informative = 3 , n_redundant = 1, flip_y=0,\n",
        "                          n_features = 20,n_clusters_per_class = 1 ,\n",
        "                          n_samples = 1000, random_state = 42)\n",
        "#split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "#train a classification model (Random Forest as an example)\n",
        "model = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#predit the probabilities\n",
        "y_pred_proba = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "#define a threshold for classification\n",
        "threshold = 0.5\n",
        "\n",
        "#adjust the threshold based on your criteria\n",
        "while threshold >=0:\n",
        "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    print(f\"Threshold:{threshold:.2f} - F1 Score:{f1:.4f}\")\n",
        "    threshold -= 0.02"
      ],
      "metadata": {
        "id": "4DOu9qzc1F3E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}