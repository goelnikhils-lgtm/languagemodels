{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4rmgWGBFD1YAi8kGjyzyJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/DynamicFunctionCalling_AGENT_MCP_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!pip install langchain-core\n",
        "!pip install langchain-community\n",
        "!pip install tiktoken\n",
        "!pip install faiss-cpu\n",
        "!pip install openai\n",
        "!pip install pypdf2\n",
        "!pip install langgraph\n",
        "\n",
        "#https://abvijaykumar.medium.com/hands-on-agentic-rag-2-3-agentic-reranking-rag-773b04cf4cdd"
      ],
      "metadata": {
        "id": "7BkZBTZeJ6eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdCcmy1ImI2H"
      },
      "outputs": [],
      "source": [
        "#agentic RAG\n",
        "from typing import List\n",
        "import tiktoken\n",
        "ENC = tiktoken.get_encoding(\"cl100k_base\") # Changed encoding name to a known one\n",
        "def num_tokens(text:str) ->int:\n",
        "  return len(ENC.encode(text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "\n",
        "class PDFLoaderAgent():\n",
        "  def __init__(self,chunk_size=500,chunk_overlap=50,verbose=False):\n",
        "    self.chunk_size=chunk_size\n",
        "    self.chunk_overlap=chunk_overlap\n",
        "    self.verbose=verbose\n",
        "\n",
        "  def load_and_split(self,pdf_path:str) -> List[str]:\n",
        "    reader = PdfReader(pdf_path)\n",
        "    full_text = \"\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
        "    tokens = ENC.encod(full_text)\n",
        "    chunks=[]\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "      end = min(start + self.chunk_size,len(tokens))\n",
        "      chunk = ENC.decode(tokens[start:end])\n",
        "      chunks.append(chunk)\n",
        "      start+= self.chunk_size - self.chunk_overlap\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "vaBAXNmYE_3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding Agent\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "import faiss\n",
        "import openai\n",
        "import numpy as np\n",
        "\n",
        "class EmbeddingAgent():\n",
        "  def __init__(self,dim=1536):\n",
        "    self.dim = dim\n",
        "    self.index = faiss.IndexFlatL2(self.dim) #using inverted flat file based index ....\n",
        "  def embed(self,texts:List[str]) ->List[List[float]]:\n",
        "    response = openai.Embedding.create(input=texts,model=EMBED_MODEL)\n",
        "    return [item.embedding for item in response.data]\n",
        "  def add_to_index(self,texts:List[str]):\n",
        "    embeddings = self.embed(texts)\n",
        "    vecs = np.array(embeddings).astype(\"float32\")\n",
        "    self.index.add(vecs)"
      ],
      "metadata": {
        "id": "Z5_pPF1eKQPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Retrieval Agent\n",
        "class RetrievalAgent():\n",
        "  def __init__(self,index):\n",
        "    self.index = index\n",
        "  def retrieve_candidates(self,query,texts,n_candidates=3,k=5):\n",
        "    base_emb = EmbeddingAgent.embed([query])[0]\n",
        "    candidates=[]\n",
        "    for i in range(n_candidates):\n",
        "      perturbed = np.array(base_emb)+np.random.normal(0,0.01,len(base_emb))\n",
        "      D,I = self.index.search(np.array([perturbed]).astype(\"float32\"),k)\n",
        "      retrieved = [texts[j] for j in I[0] if j < len(texts)]\n",
        "      candidates.append(retrieved)\n",
        "    return candidates"
      ],
      "metadata": {
        "id": "vElPM5LpM6fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QA Agent\n",
        "class QAAgent():\n",
        "  def __init__(self,model = \"CHAT_MODEL\"):\n",
        "    self.model = model\n",
        "  def answer(self,question,context):\n",
        "    context_str = \"---\\n\".join(context)\n",
        "    prompt = (\"You are an expert assistant. Use the following context to answer the question.\\n\\n\"\n",
        "              f\"Context:\\n{context_str}\\n\\n\"\n",
        "              f\"Question: {question}\\n\"\n",
        "              \"Answer:\")\n",
        "    resp = openai.chat.completions.create(\n",
        "      model=self.model,\n",
        "      messages=[{\"role\": \"system\", \"content\":prompt}],\n",
        "      temperature=0.2,\n",
        "      max_tokens=500\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "  def answer_parallel(self,question,candidate_contexts):\n",
        "    from concurrent.futures import ThreadPoolExecutor\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "      return list(executor.map(lambda ctx:self.answer(question,ctx),candidate_contexts))"
      ],
      "metadata": {
        "id": "pIUrV5rOOL5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ranking Agent\n",
        "class RankingAgent():\n",
        "  def __init__(self, model = \"CHAT_MODEL\"):\n",
        "    self.model = model\n",
        "  def rank(self,question,candidate_answers,candidate_contexts):\n",
        "    print(\"[Ranking Agent] All candidates contexts and answers\")\n",
        "    for idx,(ctx,ans) in enumerate(zip(candidate_contexts,candidate_answers),1):\n",
        "      print(f\"\\n Candidate #{idx} Context: \\n{ctx}\\n Answer: {ans}\")\n",
        "      for chunk in ctx:\n",
        "        print(f\"\\n Chunk: {chunk}\")\n",
        "      print(f\"Candidate #{idx} Answer: {ans}\")\n",
        "    ranking_prompt = f\"....\" #detailed LLM Prompt\n",
        "    summary = \"....\"\n",
        "    full_prompt = ranking_prompt + summary\n",
        "    resp = openai.chat.completions.create(\n",
        "      model=self.model,\n",
        "      messages=[{\"role\": \"system\", \"content\":full_prompt}],\n",
        "      temperature=0.2,\n",
        "      max_tokens=500\n",
        "    )\n",
        "    response_text = resp.choices[0].message.content.strip()\n",
        "    print(f\"[Ranking Agent] Ranking Response\" + response_text)\n",
        "    import re\n",
        "    m = re.search(r\"Candidate #(\\d+)\\s*\\n Reason:([^\\n]*)\\n+Best Answer:\\n(.+)\",response_text,re.DOTALL)\n",
        "    if m:\n",
        "      candidate_idx = int(m.group(1)) - 1\n",
        "      reason = m.group(2).strip()\n",
        "      best_answer = m.group(3).strip()\n",
        "      print(f\"Selected Candidates #{candidate_idx+1}. Reason : {reason}\")\n",
        "    else:\n",
        "      candidate_idx = 0\n",
        "      answer = candidate_answers[0]\n",
        "      print(\"Could not parse ranking output , returning first candidate\")\n",
        "    return answer , candidate_idx"
      ],
      "metadata": {
        "id": "Vrk7vKHZVcAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RAG Orchestrator\n",
        "class RAGOrchestrator():\n",
        "  def __init__(self,n_candidates =3, k=5):\n",
        "    self.loader_agent = PDFLoaderAgent()\n",
        "    self.embedding_agent = EmbeddingAgent()\n",
        "    self.retrieval_agent = RetrievalAgent(self.embedding_agent.index)\n",
        "    self.qa_agent = QAAgent()\n",
        "    self.ranking_agent = RankingAgent()\n",
        "    self.text_chunks = []\n",
        "    self.retriver = None\n",
        "    self.n_candidates = n_candidates\n",
        "    self.k = k\n",
        "  def ingest(self,pdf_path):\n",
        "    self.text_chunks = self.loader_agent.load_and_split(pdf_path)\n",
        "    self.embedding_agent.add_to_index(self.text_chunks)\n",
        "    self.retriver = RetrievalAgent(self.embedding_agent.index)\n",
        "  def query(self,question):\n",
        "    candidates = self.retriver.retrieve_candidates(question,self.text_chunks,n_candidate=self.n_candidates,k=self.k)\n",
        "    candidate_answers = self.qa_agent.answer_parallel(question,candidates)\n",
        "    final_answer , chosen_candidate_idx = self.ranking_agent.rank(question,candidate_answers,candidates)\n",
        "    return final_answer\n"
      ],
      "metadata": {
        "id": "xBx7KW8MaKSJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}