{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOGbbRDV4k/aERhfS4CSW0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/TokenClassificationTask_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.youtube.com/watch?v=dzyDHMycx_c&t=1s"
      ],
      "metadata": {
        "id": "qJ43kzFAE_M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrO5Si4d7lER"
      },
      "outputs": [],
      "source": [
        "#code for token classification\n",
        "!pip install transformers  tokenizers seqeval\n",
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Token Classification\n",
        "#NER - B-PER / I-PER - person entity similarily for other Location and other entities .\"O\" indicates that token respresents no Entity\n",
        "#Part of Speech Tagging\n",
        "#Chunking"
      ],
      "metadata": {
        "id": "k6wYGoQz75-z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from tokenizers import Tokenizer"
      ],
      "metadata": {
        "id": "jl4fPWKF8nQC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "conll2003 = datasets.load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "wNA09HH59Fpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "fBLSx_IH-3Kn"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = conll2003['train'][0]\n",
        "tokenized_input = tokenizer(example_text['tokens'], is_split_into_words=True) #tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])\n",
        "word_ids = tokenized_input.word_ids()"
      ],
      "metadata": {
        "id": "_OVCkL3w_TOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(example , label_all_tokens = True):\n",
        "  #set - 100 as the label for the special tokens \\\n",
        "  tokenized_input = tokenizer(example['tokens'], truncation=True, is_split_into_words=True)\n",
        "  labels = []\n",
        "  for idx , label in enumerate(example['ner_tags']): # NER tags column name in dataset\n",
        "    word_ids = tokenized_input.word_ids(batch_index=idx) # returns a word corresponding to each token from the dataset\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "    for word_idx in word_ids:\n",
        "      if word_idx is None:\n",
        "        label_ids.append(-100) #- 100 ignored by PyTorch\n",
        "      elif word_idx != previous_word_idx:\n",
        "        label_ids.append(label[word_idx])\n",
        "      else:\n",
        "        label_ids.append(label[word_idx] if label_all_tokens else -100) #for sub words\n",
        "      previous_word_idx = word_idx\n",
        "    labels.append(label_ids)\n",
        "  tokenized_input['labels'] = labels\n",
        "  return tokenized_input"
      ],
      "metadata": {
        "id": "z-YGofhcAoIT"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = tokenize_and_align_labels(conll2003['train'][4:5])"
      ],
      "metadata": {
        "id": "N5yhFVR2DQiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token , label in zip(tokenizer.convert_ids_to_tokens(q['input_ids'][0]),q['labels'][0]):\n",
        "  print((token,label))"
      ],
      "metadata": {
        "id": "U2JKIK4YC285"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = conll2003.map(tokenize_and_align_labels,batched=True) #apply tokenize and align labels across dataset"
      ],
      "metadata": {
        "id": "1HWWFIg_Ei2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained('bert-base-uncased',num_labels=9) # as we have 9 classes (PER, LOC etc and hence numlabels = 9)\n"
      ],
      "metadata": {
        "id": "AUAijQOnEwVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments , Trainer\n",
        "args = TrainingArguments(\n",
        "    'bert-finetuned-ner',\n",
        "    eval_strategy = 'epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    learning_rate = 2e-5,\n",
        "    num_train_epochs = 3,\n",
        "    weight_decay = 0.01,\n",
        "    per_device_train_batch_size = 16,\n",
        "    per_device_eval_batch_size = 16,\n",
        "    load_best_model_at_end = True\n",
        ")"
      ],
      "metadata": {
        "id": "UceXnGiUFJSn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff53375b"
      },
      "source": [
        "import evaluate # Import the evaluate library\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "metric = evaluate.load(\"seqeval\") # Use evaluate.load instead\n",
        "example = conll2003['train'][0]\n",
        "label_list = conll2003['train'].features['ner_tags'].feature.names\n",
        "labels = [label_list[i] for i in example_text[\"ner_tags\"]]\n",
        "metric.compute(predictions=[labels], references=[labels])\n",
        "\n",
        "# Function to compute metrics\n",
        "def compute_metrics(eval_preds):\n",
        "    pred_logits, labels = eval_preds\n",
        "    pred_logits = np.argmax(pred_logits, axis=2)\n",
        "    predictions = [\n",
        "        [label_list[eval_preds] for (eval_preds,l) in zip(prediction, label) if l != -100] for prediction, label in zip(pred_logits, labels)\n",
        "        ]\n",
        "\n",
        "    # Remove ignored index (all labels = -100)\n",
        "    true_labels = [label_list[l] for (eval_preds,l) in zip(predictions,label) if l!= -100 for prediction , label in zip(pred_logits,labels) ]\n",
        "\n",
        "    results = metric.compute(predictions=predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "model.save_pretrained('bert-finetuned-ner')\n",
        "tokenizer.save_pretrained('bert-finetuned-ner')"
      ],
      "metadata": {
        "id": "KDzvbxTZSiDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label= {\n",
        "    str(i): label for i , label in enumerate(label_list)\n",
        "}\n",
        "label2id = {\n",
        "    label: str(i) for i , label in enumerate(label_list)\n",
        "}\n"
      ],
      "metadata": {
        "id": "5MjOIW_eSqVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "config = json.load(open('bert-finetuned-ner/config.json'))\n",
        "config['id2label'] = id2label\n",
        "config['label2id'] = label2id\n",
        "json.dump(config,open('bert-finetuned-ner/config.json','w'))\n",
        "model_fine_tuned = AutoModelForTokenClassification.from_pretrained('bert-finetuned-ner',config=config)"
      ],
      "metadata": {
        "id": "ncmZ_FHSTJZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd31dbbc"
      },
      "source": [
        "from transformers import pipeline\n",
        "ner = pipeline('ner',model=model_fine_tuned,tokenizer=tokenizer)\n",
        "example = \"Bill Gates is from Microsoft\"\n",
        "ner_results = ner(example)\n",
        "print(ner_results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}