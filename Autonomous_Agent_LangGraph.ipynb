{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOr4/cLNdsbl6wRDQ8mJiC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Autonomous_Agent_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF1VyHqMYI7l"
      },
      "outputs": [],
      "source": [
        "#Autonomus agent built using LangGraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uv\n",
        "!pip install langchain langgraph langchain_community\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "DDSTYVMKczmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict , List , Tuple , Any , Optional , TypedDict , Annotated\n",
        "#from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langgraph.graph import StateGraph, END\n",
        "from google.colab import userdata\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "from langchain_community.chat_models import ChatHuggingFace\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "# Initialize HuggingFaceHub with your Hugging Face API token and the model repo ID\n",
        "# Set the OpenAI API key from Colab secrets\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "llm = HuggingFaceHub(repo_id=\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "\n",
        "# Wrap it with ChatHuggingFace\n",
        "llm = ChatHuggingFace(llm=llm)\n",
        "\n",
        "#model_name = \"meta-llama/Llama-2-7b-chat-hf\" # Example: Replace with your desired Llama model\n",
        "#model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "#pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100)\n",
        "#llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "#llm = ChatOpenAI(model=\"meta-llama/Llama-3.1-8B-Instruct\", temperature=0)\n",
        "#define the agent state\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"State of the agent\"\"\"\n",
        "    question: str\n",
        "    sub__questions: List[str]\n",
        "    research_findings: List[str]\n",
        "    current_sub_question: Optional[str]\n",
        "    final_answer: Optional[str]\n",
        "    evaluation: Optional[str]\n",
        "    messages: List[Any]\n",
        "#intialize the state graph\n",
        "graph = StateGraph(AgentState)"
      ],
      "metadata": {
        "id": "YyfM1410m_kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict , List , Tuple , Any , Optional , TypedDict , Annotated\n",
        "#from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langgraph.graph import StateGraph, END\n",
        "from google.colab import userdata\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "from langchain_community.chat_models import ChatHuggingFace\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "# Initialize HuggingFaceHub with your Hugging Face API token and the model repo ID\n",
        "# Set the OpenAI API key from Colab secrets\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "# llm = HuggingFaceHub(repo_id=\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "\n",
        "# Wrap it with ChatHuggingFace\n",
        "#llm = ChatHuggingFace(llm=llm)\n",
        "\n",
        "\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\" # Example: Replace with your desired Llama model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100)\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "\n",
        "#llm = ChatOpenAI(model=\"meta-llama/Llama-3.1-8B-Instruct\", temperature=0)\n",
        "#define the agent state\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"State of the agent\"\"\"\n",
        "    question: str\n",
        "    sub_questions: List[str]\n",
        "    research_findings: Dict[str, str]\n",
        "    current_sub_question: Optional[str]\n",
        "    final_answer: Optional[str]\n",
        "    evaluation: Optional[str]\n",
        "    messages: List[Any]\n",
        "#intialize the state graph\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "#define the breakdown node - divides the main question into sub-questions\n",
        "def breakdown_question(state: AgentState) -> AgentState:\n",
        "  \"\"\"Breakdown the main question into sub-questions\"\"\"\n",
        "  messages = [\n",
        "      SystemMessage(content = \"You are a research expert skilled at breaking complex questions into specific sub-questions. Your job is to analyze provided research questions and break it down into clear 3-5 sub-questions.\"),\n",
        "      HumanMessage(content = f\"Please break down the following research question into 3-5 sub questions: \\n\\n{state['question']}\\n\\n. Format your response as a list of sub-questions one per line , withour numbering or bullets\")\n",
        "      ]\n",
        "  response = llm.invoke(messages)\n",
        "  sub_questions = [q.strip() for q in response.split('\\n') if q.strip()]\n",
        "  return {\n",
        "      ** state,\n",
        "      \"sub_questions\":sub_questions,\n",
        "      \"research_findings\":{},\n",
        "      \"messages\":state[\"messages\"] + [\n",
        "          HumanMessage(content = f\"Breaking down the question:{state['question']}\"),\n",
        "          AIMessage(content = f\"I have broken down your question into these sub-questions:\\n{response}\")\n",
        "          ]\n",
        "      }\n",
        "#define the research node - researches the current sub-question\n",
        "def research_sub_question(state:AgentState) ->AgentState:\n",
        "    \"\"\"Research the current sub-question\"\"\"\n",
        "    current_sub_question = state[\"sub_questions\"][0] if not state [\"current_sub_question\"] else state[\"current_sub_question\"]\n",
        "    messages = [\n",
        "      SystemMessage(content = \"You are a thorough research assistant. Your taks is to provide adetailed answer to a specific research question using your knowledge\"),\n",
        "      HumanMessage(content = f\"Please research the following question and provide a detailed answer: \\n\\n{current_sub_question}\")\n",
        "      ]\n",
        "    response = llm.invoke(messages)\n",
        "    research_findings = state[\"research_findings\"].copy()\n",
        "    research_findings[current_sub_question] = response\n",
        "\n",
        "    return {\n",
        "      ** state,\n",
        "      \"current_sub_question\":current_sub_question,\n",
        "      \"research_findings\":research_findings,\n",
        "      \"messages\":state[\"messages\"] + [\n",
        "          HumanMessage(content = f\"Researching:{current_sub_question}\"),\n",
        "          AIMessage(content = f\"Research Findings:\\n{response[:100]}...\")\n",
        "          ]\n",
        "      }\n",
        "#Define the next question selector - determines if we should research another question or synthesize\n",
        "def select_next_steps(state:AgentState) ->str:\n",
        "  \"\"\"Determine whether to research the next question or move to synthesis\"\"\"\n",
        "  researched_questions = set(state[\"research_findings\"].keys())\n",
        "  remanining_questions = [q for q in state[\"sub_questions\"] if q not in researched_questions]\n",
        "  if remanining_questions:\n",
        "    state[\"current_sub_question\"] = remanining_questions[0]\n",
        "    return \"research\"\n",
        "  else:\n",
        "    return \"synthesize\"\n",
        "\n",
        "#Define the synthesis node - combines research findings into a cohesive manner\n",
        "def synthesize_findings(state:AgentState) ->AgentState:\n",
        "  \"\"\"Synthesize all research findings into a comprehensive answer\"\"\"\n",
        "  findings_text = \"\\n\".join([f\"Sub-question: {q}:\\n Findings: {f}\" for q, f in state[\"research_findings\"].items()])\n",
        "  messages = [\n",
        "         SystemMessage(content = \"You are a research synthesis expert. Your task is to combine separate research findings into a cohesive,  well-structured answer to the original question.\"),\n",
        "         HumanMessage(content =f\"Original question:{state['question']}\\n\\n Research Findings:\\n{findings_text}\\n\\n Please synthesize these findings into a comprehensive answer to the original question.\")\n",
        "         ]\n",
        "  response = llm.invoke(messages)\n",
        "  return{\n",
        "         **state,\n",
        "         \"final_answer\":response,\n",
        "         \"messages\":state[\"messages\"] +[\n",
        "             HumanMessage(content=\"Synthesizing rsearch findings\"),\n",
        "             AIMessage(content=f\"I 've synthesized the findings into a comprehensive answer:\\n{response[:100]}...\")\n",
        "         ]\n",
        "     }\n",
        "\n",
        "#Define the evaluation node - assess the quality of the final answer\n",
        "def evaluate_answer(state:AgentState) ->AgentState:\n",
        "  \"\"\"Evaluate the quality of the synthesized answer\"\"\"\n",
        "  messages = [\n",
        "         SystemMessage(content = \"You are a critical evaluator of research answers. Your task is to identify any gaps , logical flows, or areas where the answer could be improved\"),\n",
        "         HumanMessage(content =f\"Original question:{state['question']}\\n\\n Synthesized answer:{state['final_answer']}\\n\\n Please evauuate this answer , identifying any weakness or areas of improvement.\")\n",
        "         ]\n",
        "  response = llm.invoke(messages)\n",
        "  return{\n",
        "         **state,\n",
        "         \"evaluation\":response,\n",
        "         \"messages\":state[\"messages\"] +[\n",
        "             HumanMessage(content=\"Evaluating the answer quality\"),\n",
        "             AIMessage(content=f\"Evaluation:\\n{response}\")\n",
        "         ]\n",
        "     }\n",
        "#define the refinement decision node - determines if the answer needs refinement\n",
        "def needs_refinement(state:AgentState) ->str:\n",
        "  \"\"\"Determine if the answer needs refinement based on evaluation\"\"\"\n",
        "  if not state[\"evaluation\"] or \"excellent\" in state[\"evaluation\"].lower() or \"adequate\" in state[\"evaluation\"].lower():\n",
        "    return \"complete\"\n",
        "  else:\n",
        "    return \"refine\"\n",
        "\n",
        "#Define the evaluation node - assess the quality of the final answer\n",
        "def refine_answer(state:AgentState) ->AgentState:\n",
        "  \"\"\"Refine the answer based on evaluation feedback\"\"\"\n",
        "  messages = [\n",
        "         SystemMessage(content = \"You are a expert at refining research answers. Your task is to improve an answer based on specific evaluation feedback\"),\n",
        "         HumanMessage(content =f\"Original question:{state['question']}\\n\\n Current answer:{state['final_answer']}\\n\\n Evaluation feedback:{state['evaluation']}\\n\\n Please provide an improved version of the answer that addresses the weakness identified in the evaluation.\")\n",
        "         ]\n",
        "  response = llm.invoke(messages)\n",
        "  return{\n",
        "         **state,\n",
        "         \"final_answer\":response,\n",
        "         \"messages\":state[\"messages\"] +[\n",
        "             HumanMessage(content=\"Refining the answer based on evaluation...\"),\n",
        "             AIMessage(content=f\"Refined answer :\\n{response[:100]}...\")\n",
        "         ]\n",
        "     }"
      ],
      "metadata": {
        "id": "kI3SvlILnZeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the state graph again to avoid \"Node already present\" errors on re-execution\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "#create the agent execution graph\n",
        "graph.add_node(\"breakdown\",breakdown_question)\n",
        "graph.add_node(\"research\",research_sub_question)\n",
        "graph.add_node(\"synthesize\",synthesize_findings)\n",
        "graph.add_node(\"evaluate\",evaluate_answer)\n",
        "graph.add_node(\"refine\",refine_answer)\n",
        "\n",
        "#define the edges - the flow of execution between nodes\n",
        "graph.add_edge(\"breakdown\",\"research\")\n",
        "graph.add_conditional_edges(\n",
        "    \"research\",\n",
        "    select_next_steps,\n",
        "    {\n",
        "        \"research\": \"research\", #continue researching\n",
        "        \"synthesize\":\"synthesize\" #move to Synthesis\n",
        "    }\n",
        ")\n",
        "graph.add_edge(\"synthesize\",\"evaluate\")\n",
        "graph.add_conditional_edges(\n",
        "    \"evaluate\",\n",
        "    needs_refinement,\n",
        "    {\n",
        "        \"refine\": \"refine\", #continue researching\n",
        "        \"complete\":END #Finish execution\n",
        "    }\n",
        ")\n",
        "graph.add_edge(\"refine\",\"evaluate\")\n",
        "#set the entry point\n",
        "graph.set_entry_point(\"breakdown\")\n",
        "#Compile the graph into an executable\n",
        "research_agent = graph.compile()\n",
        "\n",
        "#user interface\n",
        "def run_research_agent(question:str):\n",
        "  \"\"\"Run the research agent on a given question\"\"\"\n",
        "  initial_state={\n",
        "      \"question\":question,\n",
        "      \"sub_questions\":[],\n",
        "      \"research_findings\":{},\n",
        "      \"current_sub_question\":None,\n",
        "      \"final_answer\":None,\n",
        "      \"evaluation\":None,\n",
        "      \"messages\":[]\n",
        "  }\n",
        "  print(f\"Researching:{question}\\n\")\n",
        "  print(f\"Start the research process...\\n\")\n",
        "\n",
        "  final_state = research_agent.invoke(initial_state)\n",
        "\n",
        "  print(\"\\n=== Sub-Questions===\")\n",
        "  for i , q in enumerate(final_state[\"sub_questions\"],1):\n",
        "    print(f\"{i+1}.{q}\")\n",
        "\n",
        "  print(\"\\n=== Final Answer===\")\n",
        "  print(final_state[\"final_answer\"])\n",
        "\n",
        "  print(\"\\n=== Evaluation===\")\n",
        "  print(final_state[\"evaluation\"])\n",
        "  return final_state\n",
        "\n",
        "#Example usage\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  question = \"What are the enviornmental and economic impacts of switching to renewable energy sources , what are the main challenges to widespread adoption?\"\n",
        "  run_research_agent(question)"
      ],
      "metadata": {
        "id": "93tbXmvW3Ayi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating memory and storing the conversation in the memory for retreival later....\n",
        "!pip install langchain langchain_openai mem0ai python-dotenv"
      ],
      "metadata": {
        "id": "rXZEly6s-y8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Literal , Dict , AnyStr , List\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage , HumanMessage , AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate , MessagesPlaceholder\n",
        "from mem0 import MemoryClient\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoQdOohhGAHV",
        "outputId": "27b62d33-0e68-4de7-b50c-316bb62130a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "# Set the OpenAI API key from Colab secrets\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"MEM0_API_KEY\"] = userdata.get(\"MEM0_API_KEY\")\n",
        "\n",
        "load_dotenv()\n",
        "#intialize llm\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\")\n",
        "mem0=MemoryClient()"
      ],
      "metadata": {
        "id": "4RcA0pVuHTj6",
        "outputId": "9cfd69e3-4369-4cd3-d224-21c827e16298",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setup the coversation by setting up the prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content=\"\"\"You are helpful AI Marketing Assistant. Use the provided context to persoanalize your responses and remember user preferences and past interactions\n",
        "    Peronsalize the marketing plan generation and ask question from the use on specific campaigns that require user input. You can develop the marketing plan\n",
        "    with the campaign information available to you in your context , campaign information such as Channel Name , Campaign Name , Campaign Efficiency , Campaign Spend\n",
        "    Campaign ROI . Use also the simulated flat budget optimizations and other non flat budget optimizations . These optimizations are available via tool call to you\"\"\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    HumanMessage(content=\"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "LfmDKfuYJQPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define function for storing , retreiving chat into memory\n",
        "def retrieve_context(query:str , user_id:str) ->List[Dict]:\n",
        "  \"\"\"retreive the conetext from Mem0\"\"\"\n",
        "  try:\n",
        "    memories = mem0.search(query , user_id = user_id)\n",
        "    memory_list = memories[\"results\"]\n",
        "    serialized_memories = \" \".join([mem[memories]] for mem in memory_list)\n",
        "    chat_history = [\n",
        "        {\n",
        "            \"role\":\"System\",\n",
        "            \"content\":f\"Relevant Information{serialized_memories}\"\n",
        "        },\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\": query\n",
        "        }\n",
        "    ]\n",
        "    return chat_history\n",
        "  except Exception as e:\n",
        "    print(f\"Error Retrieving Memories:{e}\")\n",
        "    return [{\"role\":\"user\", \"content\":query}]\n"
      ],
      "metadata": {
        "id": "c_vRi2IvLQU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(input:str , context:List[Dict]) ->str:\n",
        "  \"\"\" Generate a response using the lnaguage model\"\"\"\n",
        "  chain = prompt | llm\n",
        "  response = chain.invoke({\"input\":input , \"chat_history\":context})\n",
        "  return response.content\n",
        "def save_interaction(user_id:str, user_input:str , assistant_response:str):\n",
        "  \"\"\" Save the interaction to Mem0\"\"\"\n",
        "  #use exception handling in these scenarios to continue the flow of conversation\n",
        "  try:\n",
        "      interaction = [{\n",
        "          \"role\":\"user\",\n",
        "          \"content\":user_input\n",
        "      },\n",
        "      {\n",
        "          \"role\":\"assistant\",\n",
        "          \"content\":assistant_response\n",
        "      }]\n",
        "      result = mem0.add(interaction , user_id=user_id)\n",
        "      print(f\"Memory saved successfully:{len(result.get(\"results\",[]))} memories added\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error saving memory:{e}\")\n"
      ],
      "metadata": {
        "id": "m0lACp4lODsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create chat function\n",
        "def chat_turn(user_input:str, user_id:str) ->str:\n",
        "  #Retrieve context\n",
        "  context = retrieve_context(user_input , user_id)\n",
        "  #Generate response\n",
        "  assistant_response = generate_response(user_input , context)\n",
        "  #Save interaction\n",
        "  save_interaction(user_id , user_input , assistant_response)\n",
        "  return assistant_response\n"
      ],
      "metadata": {
        "id": "l74fm1XXQK6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main interaction function\n",
        "if __name__ ==\"__main__\":\n",
        "  print(\"Welcome to Marketing Business Assistant . I can help generating marketing plans. How can I assist you today\")\n",
        "  user_id =\"Nikhil\"\n",
        "\n",
        "  while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input in [\"quit\",\"exit\",\"bye\"]:\n",
        "      print(f\"Marketing Business Agent : Thanks for using my services. Have a great day ahead\")\n",
        "      break # break the conversation loop\n",
        "    response = chat_turn(user_input , user_id)\n",
        "    print(f\"Marketing Business Agent: {response}\")\n"
      ],
      "metadata": {
        "id": "8TJcWifOQfw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code for ReACT Agent\n",
        "%%capture\n",
        "!pip install together\n",
        "#if colab , download the data in current session\n",
        "import os\n",
        "if \"COLAB_\" in \"\".join(os.environ.keys()):\n",
        "  !mkdir data\n",
        "  !cd data ; wget https://raw.githubusercontent.com/togethercomputer/together-cookbook/refs/heads/main/Agents/DataScienceAgent/data/train_and_test2.csv"
      ],
      "metadata": {
        "id": "IH40hVC5dmzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#intialize Necessary Functions and Configurations\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import textwrap\n",
        "import base64\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional , List , Any , Union\n",
        "\n",
        "from together import Together\n",
        "from IPython.display import Image , display"
      ],
      "metadata": {
        "id": "zKN5XDkIokQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agent configuration\n",
        "#==================\n",
        "\n",
        "#Model Selection\n",
        "reasoning_model = \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"  # primary model for reasoning and code generation\n",
        "#xecution setting\n",
        "max_iterations = 15 #number of max reasoning cycles\n",
        "temperature = 0.2 #lower temperature for more consistent code generation\n",
        "session_timeout = 3600 #session timeout in seconds\n",
        "\n",
        "#display settings\n",
        "max_output_length = 500\n",
        "show_images = True\n",
        "box_width = 80\n",
        "\n",
        "#Intialize Together client\n",
        "together = Together(api_key=os.environ[\"TOGETHER_API_KEY\"])\n",
        "code_interperter = together_client.code_interperter"
      ],
      "metadata": {
        "id": "ZuTlp0nRpqe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for running python code generated by the model - this uses CodeAct concept\n",
        "def run_python(code:str,session_id:Optional[str] = None, files:Optional[List[Dict[str,str]]] = None):\n",
        "  \"\"\" Executes Python Code\n",
        "    Args: python code\n",
        "    session_id :Optional session id to maintain state between execution\n",
        "    files : Optional list for files to upload to the code interperter\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  Returns : The output of the executed code as a JSON\n",
        "  \"\"\"\n",
        "  try:\n",
        "    kwargs = {\"code\":code , \"language\":\"python\"}\n",
        "    if session_id:\n",
        "      kwargs[\"session_id\"] = session_id\n",
        "    if files:\n",
        "      kwargs[\"files\"] = files\n",
        "    response = code_interperter.run(**kwargs)\n",
        "    result = {\"session_id\":response.data.session_id, \"status\":response.data.status,\"outputs\":[]}\n",
        "    for output in response.data.outputs:\n",
        "      result[\"outputs\"].append({\"type\":output.type, \"data\":output.data})\n",
        "\n",
        "    if response.data.errors:\n",
        "      result[\"errors\"] = response.data.errors\n",
        "    return result\n",
        "  except Exception as e:\n",
        "    error_result = {\"status\":\"error\",\"error_message\":str(e),\"session_id\":None}\n",
        "    return error_result"
      ],
      "metadata": {
        "id": "8HSGqPSbq4DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LLM output processing function\n",
        "def display_image(b64_image):\n",
        "  decoded_image = base64.b64decode(b64_image)\n",
        "  display(Image(data = decoded_image))\n",
        "#function to parse the output of the output generated by model\n",
        "def get_execution_summary(execution_result:Dict) ->str:\n",
        "  \"\"\" execution\"\"\"\n",
        "  if not execution_result:\n",
        "    return \"Execution failed - no result returned\"\n",
        "\n",
        "  #check execution status\n",
        "  status = execution_result.get(\"status\",\"unknown\")\n",
        "  summary_parts = [f\"Execution Status:{status}\"]\n",
        "  #process outputs\n",
        "  stdout_outputs = []\n",
        "  display_outputs = []\n",
        "  other_outputs = []\n",
        "\n",
        "  if \"outputs\" in execution_result:\n",
        "    for output in execution_result[\"outputs\"]:\n",
        "      output_type = output.get(\"type\",\"unknown\")\n",
        "      output_data = output.get(\"data\",\"\")\n",
        "\n",
        "      if output_type ==\"stdout\":\n",
        "        stdout_outputs.append(output_data)\n",
        "      elif output_type ==\"display_data\":\n",
        "        if isinstance(output_data,dict):\n",
        "          if \"image/png\" in output_data:\n",
        "            display_outputs.append(\"Generated plot/image\")\n",
        "          if \"text/plain\" in output_data:\n",
        "            display_outputs.append(f\"Display:{output_data[\"text/plain\"]}\")\n",
        "        else:\n",
        "          display_outputs.append(\"Generated display output\")\n",
        "      else:\n",
        "        other_outputs.append(f\"{output_type}:{str(output_data)[:100]}\")\n",
        "  #add stdout outputs\n",
        "  if stdout_outputs:\n",
        "    summary_parts.append(\"\\n\".join(stdout_outputs))\n",
        "  #add display outputs\n",
        "  if display_outputs:\n",
        "    summary_parts.append(\"\\n\".join(display_outputs))\n",
        "  #add other outputs\n",
        "  if other_outputs:\n",
        "    summary_parts.append(\"\\n\".join(other_outputs))\n",
        "  if \"errors\" in execution_result:\n",
        "    summary_parts.append(\"\\n\".join(execution_result[\"errors\"]))\n",
        "  if not stdout_outputs and not display_outputs and not other_outputs and status ==\"success\":\n",
        "    summary_parts.append(\"Code executed successfully\")\n",
        "  return \"\\n\".join(summary_parts)\n"
      ],
      "metadata": {
        "id": "OGUp08Dux1Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ReACT Agent\n",
        "class ReActDataScienceAgent():\n",
        "  def __init__(self, client, session_id:Optional[str] = None, model:str = reasoning_model , max_iterations:int = max_iterations):\n",
        "    self.client = client\n",
        "    self.session_id = session_id\n",
        "    self.model = model\n",
        "    self.max_iterations = max_iterations\n",
        "    self.system_prompt = \"\"\"\n",
        "    You are an expert Marketing Business Agent that follows a ReAct Framework(Reasoning + Acting)\n",
        "    CRITICAL RULES:\n",
        "    1.Execute ONLY ONE action at a time - this is non-negotiable\n",
        "    2.Be methodical and detailed and deliberate in your approach\n",
        "    3.For executing the action use the input as Media Channel , Campaign Name , Campaign Efficiency , Sales Generated by Campaign , ROI of Campaign , CAC of Campaign and Campaign Budget\n",
        "    4.Using the above data always do two simulations flat budget simulation and increase the spend by +-5% and do simulation\n",
        "    5.For flat budget simulation please use tool that is provided\n",
        "    6.For Increase in budget and then do simulation use tool that is provided\n",
        "    7.For Decrease in budget and then do simulation use tool that is provided\n",
        "    8.You will be given external data such as Retail Inflation , Consumer Sentiment Index and other macro economic data that impact sales\n",
        "    9.You will be given AGM reports and comp AGM reports as input use them pls and use the revenue section and forward looking section for strategy\n",
        "    10.Once simulation results are done based on lowest CAC generate the marketing plan\n",
        "    11.Never assume anything and ask before any other action\n",
        "\n",
        "    IMPORTANT GUIDELINES:\n",
        "    -Be explorative and creative , but cautious\n",
        "    -Try things incrementally and observe the results\n",
        "    -Never randomly guess\n",
        "    -if you don't have access to simulation tool use \"import os ; oslistdir() and check for Simulation.py\"\n",
        "    -When you see \"Marketing Plan Generated Successfully\" or \"User says good with marketing plan\" , it means your marketing plan worked\n",
        "    You must strictly adhere to below format\n",
        "\n",
        "    ##Format 1 - for taking action\n",
        "    Thought - Reflect on what to do next. Analyze results from previous steps. Be descriptive and detailed about your reasoning, what you expect to see,\n",
        "    how it builds on previous actions. Reference specific data points or patterns you have observed\n",
        "\n",
        "    Action Input:\n",
        "    '''python\n",
        "    <simulation.py to run>\n",
        "    '''\n",
        "    ##Format 2 - ONLY when you have completely finished the task:\n",
        "    Thought: Reflect and generate final marketing plan output\n",
        "    Final Answer:\n",
        "    [Provide a comprehensive marketing plan]\n",
        "\n",
        "    ##Example of marketing plan\n",
        "    Thought: I need to first get data for Media Channel , Campaign Name , Campaign Efficiency , Sales Generated by Campaign , ROI of Campaign ,\n",
        "     CAC of Campaign and Campaign Budget. I need to use the external macro economic data and I need to run simulation continous simulation so that I am able\n",
        "     to achieve lowest CAC and increase the sales and generate the plan\n",
        "\n",
        "     Action Input:\n",
        "     '''python\n",
        "     import pandas as pd\n",
        "     import Simulation as sim\n",
        "\n",
        "     sim.simulation()\n",
        "     '''\n",
        "\n",
        "    \"\"\"\n",
        "    self.history = [{\"role\":\"system\",\"content\":self.system_prompt}]\n",
        "\n",
        "  def llm_call(self):\n",
        "    response = self.client.chat.completions.create(\n",
        "        model = self.model,\n",
        "        messages = self.history,\n",
        "        temperature = temperature,\n",
        "        stream = False\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "  def parse_response(self):\n",
        "    response = self.llm_call()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oOggRK934o2b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}