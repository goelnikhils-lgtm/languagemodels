{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9A8rXj6h1GcbsUx5FnfDa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Autonomous_Agent_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF1VyHqMYI7l"
      },
      "outputs": [],
      "source": [
        "#Autonomus agent built using LangGraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uv\n",
        "!pip install langchain langgraph langchain_community\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "DDSTYVMKczmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict , List , Tuple , Any , Optional , TypedDict , Annotated\n",
        "#from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langgraph.graph import StateGraph, END\n",
        "from google.colab import userdata\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "from langchain_community.chat_models import ChatHuggingFace\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "# Initialize HuggingFaceHub with your Hugging Face API token and the model repo ID\n",
        "# Set the OpenAI API key from Colab secrets\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "llm = HuggingFaceHub(repo_id=\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "\n",
        "# Wrap it with ChatHuggingFace\n",
        "llm = ChatHuggingFace(llm=llm)\n",
        "\n",
        "\n",
        "\n",
        "#model_name = \"meta-llama/Llama-2-7b-chat-hf\" # Example: Replace with your desired Llama model\n",
        "#model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "#pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100)\n",
        "#llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "\n",
        "#llm = ChatOpenAI(model=\"meta-llama/Llama-3.1-8B-Instruct\", temperature=0)\n",
        "#define the agent state\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"State of the agent\"\"\"\n",
        "    question: str\n",
        "    sub__questions: List[str]\n",
        "    research_findings: List[str]\n",
        "    current_sub_question: Optional[str]\n",
        "    final_answer: Optional[str]\n",
        "    evaluation: Optional[str]\n",
        "    messages: List[Any]\n",
        "#intialize the state graph\n",
        "graph = StateGraph(AgentState)"
      ],
      "metadata": {
        "id": "YyfM1410m_kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict , List , Tuple , Any , Optional , TypedDict , Annotated\n",
        "#from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langgraph.graph import StateGraph, END\n",
        "from google.colab import userdata\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "from langchain_community.chat_models import ChatHuggingFace\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "# Initialize HuggingFaceHub with your Hugging Face API token and the model repo ID\n",
        "# Set the OpenAI API key from Colab secrets\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "# llm = HuggingFaceHub(repo_id=\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "\n",
        "# Wrap it with ChatHuggingFace\n",
        "#llm = ChatHuggingFace(llm=llm)\n",
        "\n",
        "\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\" # Example: Replace with your desired Llama model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100)\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "\n",
        "#llm = ChatOpenAI(model=\"meta-llama/Llama-3.1-8B-Instruct\", temperature=0)\n",
        "#define the agent state\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"State of the agent\"\"\"\n",
        "    question: str\n",
        "    sub_questions: List[str]\n",
        "    research_findings: Dict[str, str]\n",
        "    current_sub_question: Optional[str]\n",
        "    final_answer: Optional[str]\n",
        "    evaluation: Optional[str]\n",
        "    messages: List[Any]\n",
        "#intialize the state graph\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "#define the breakdown node - divides the main question into sub-questions\n",
        "def breakdown_question(state: AgentState) -> AgentState:\n",
        "  \"\"\"Breakdown the main question into sub-questions\"\"\"\n",
        "  messages = [\n",
        "      SystemMessage(content = \"You are a research expert skilled at breaking complex questions into specific sub-questions. Your job is to analyze provided research questions and break it down into clear 3-5 sub-questions.\"),\n",
        "      HumanMessage(content = f\"Please break down the following research question into 3-5 sub questions: \\n\\n{state['question']}\\n\\n. Format your response as a list of sub-questions one per line , withour numbering or bullets\")\n",
        "      ]\n",
        "  response = llm.invoke(messages)\n",
        "  sub_questions = [q.strip() for q in response.split('\\n') if q.strip()]\n",
        "  return {\n",
        "      ** state,\n",
        "      \"sub_questions\":sub_questions,\n",
        "      \"research_findings\":{},\n",
        "      \"messages\":state[\"messages\"] + [\n",
        "          HumanMessage(content = f\"Breaking down the question:{state['question']}\"),\n",
        "          AIMessage(content = f\"I have broken down your question into these sub-questions:\\n{response}\")\n",
        "          ]\n",
        "      }\n",
        "#define the research node - researches the current sub-question\n",
        "def research_sub_question(state:AgentState) ->AgentState:\n",
        "    \"\"\"Research the current sub-question\"\"\"\n",
        "    current_sub_question = state[\"sub_questions\"][0] if not state [\"current_sub_question\"] else state[\"current_sub_question\"]\n",
        "    messages = [\n",
        "      SystemMessage(content = \"You are a thorough research assistant. Your taks is to provide adetailed answer to a specific research question using your knowledge\"),\n",
        "      HumanMessage(content = f\"Please research the following question and provide a detailed answer: \\n\\n{current_sub_question}\")\n",
        "      ]\n",
        "    response = llm.invoke(messages)\n",
        "    research_findings = state[\"research_findings\"].copy()\n",
        "    research_findings[current_sub_question] = response\n",
        "\n",
        "    return {\n",
        "      ** state,\n",
        "      \"current_sub_question\":current_sub_question,\n",
        "      \"research_findings\":research_findings,\n",
        "      \"messages\":state[\"messages\"] + [\n",
        "          HumanMessage(content = f\"Researching:{current_sub_question}\"),\n",
        "          AIMessage(content = f\"Research Findings:\\n{response[:100]}...\")\n",
        "          ]\n",
        "      }\n",
        "#Define the next question selector - determines if we should research another question or synthesize\n",
        "def select_next_steps(state:AgentState) ->str:\n",
        "  \"\"\"Determine whether to research the next question or move to synthesis\"\"\"\n",
        "  researched_questions = set(state[\"research_findings\"].keys())\n",
        "  remanining_questions = [q for q in state[\"sub_questions\"] if q not in researched_questions]\n",
        "  if remanining_questions:\n",
        "    state[\"current_sub_question\"] = remanining_questions[0]\n",
        "    return \"research\"\n",
        "  else:\n",
        "    return \"synthesize\"\n",
        "\n",
        "#Define the synthesis node - combines research findings into a cohesive manner\n",
        "def synthesize_findings(state:AgentState) ->AgentState:\n",
        "  \"\"\"Synthesize all research findings into a comprehensive answer\"\"\"\n",
        "  findings_text = \"\\n\".join([f\"Sub-question: {q}:\\n Findings: {f}\" for q, f in state[\"research_findings\"].items()])\n",
        "  messages = [\n",
        "         SystemMessage(content = \"You are a research synthesis expert. Your task is to combine separate research findings into a cohesive,  well-structured answer to the original question.\"),\n",
        "         HumanMessage(content =f\"Original question:{state['question']}\\n\\n Research Findings:\\n{findings_text}\\n\\n Please synthesize these findings into a comprehensive answer to the original question.\")\n",
        "         ]\n",
        "  response = llm.invoke(messages)\n",
        "  return{\n",
        "         **state,\n",
        "         \"final_answer\":response,\n",
        "         \"messages\":state[\"messages\"] +[\n",
        "             HumanMessage(content=\"Synthesizing rsearch findings\"),\n",
        "             AIMessage(content=f\"I 've synthesized the findings into a comprehensive answer:\\n{response[:100]}...\")\n",
        "         ]\n",
        "     }\n",
        "\n",
        "#Define the evaluation node - assess the quality of the final answer\n",
        "def evaluate_answer(state:AgentState) ->AgentState:\n",
        "  \"\"\"Evaluate the quality of the synthesized answer\"\"\"\n",
        "  messages = [\n",
        "         SystemMessage(content = \"You are a critical evaluator of research answers. Your task is to identify any gaps , logical flows, or areas where the answer could be improved\"),\n",
        "         HumanMessage(content =f\"Original question:{state['question']}\\n\\n Synthesized answer:{state['final_answer']}\\n\\n Please evauuate this answer , identifying any weakness or areas of improvement.\")\n",
        "         ]\n",
        "  response = llm.invoke(messages)\n",
        "  return{\n",
        "         **state,\n",
        "         \"evaluation\":response,\n",
        "         \"messages\":state[\"messages\"] +[\n",
        "             HumanMessage(content=\"Evaluating the answer quality\"),\n",
        "             AIMessage(content=f\"Evaluation:\\n{response}\")\n",
        "         ]\n",
        "     }\n",
        "#define the refinement decision node - determines if the answer needs refinement\n",
        "def needs_refinement(state:AgentState) ->str:\n",
        "  \"\"\"Determine if the answer needs refinement based on evaluation\"\"\"\n",
        "  if not state[\"evaluation\"] or \"excellent\" in state[\"evaluation\"].lower() or \"adequate\" in state[\"evaluation\"].lower():\n",
        "    return \"complete\"\n",
        "  else:\n",
        "    return \"refine\"\n",
        "\n",
        "#Define the evaluation node - assess the quality of the final answer\n",
        "def refine_answer(state:AgentState) ->AgentState:\n",
        "  \"\"\"Refine the answer based on evaluation feedback\"\"\"\n",
        "  messages = [\n",
        "         SystemMessage(content = \"You are a expert at refining research answers. Your task is to improve an answer based on specific evaluation feedback\"),\n",
        "         HumanMessage(content =f\"Original question:{state['question']}\\n\\n Current answer:{state['final_answer']}\\n\\n Evaluation feedback:{state['evaluation']}\\n\\n Please provide an improved version of the answer that addresses the weakness identified in the evaluation.\")\n",
        "         ]\n",
        "  response = llm.invoke(messages)\n",
        "  return{\n",
        "         **state,\n",
        "         \"final_answer\":response,\n",
        "         \"messages\":state[\"messages\"] +[\n",
        "             HumanMessage(content=\"Refining the answer based on evaluation...\"),\n",
        "             AIMessage(content=f\"Refined answer :\\n{response[:100]}...\")\n",
        "         ]\n",
        "     }"
      ],
      "metadata": {
        "id": "kI3SvlILnZeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the state graph again to avoid \"Node already present\" errors on re-execution\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "#create the agent execution graph\n",
        "graph.add_node(\"breakdown\",breakdown_question)\n",
        "graph.add_node(\"research\",research_sub_question)\n",
        "graph.add_node(\"synthesize\",synthesize_findings)\n",
        "graph.add_node(\"evaluate\",evaluate_answer)\n",
        "graph.add_node(\"refine\",refine_answer)\n",
        "\n",
        "#define the edges - the flow of execution between nodes\n",
        "graph.add_edge(\"breakdown\",\"research\")\n",
        "graph.add_conditional_edges(\n",
        "    \"research\",\n",
        "    select_next_steps,\n",
        "    {\n",
        "        \"research\": \"research\", #continue researching\n",
        "        \"synthesize\":\"synthesize\" #move to Synthesis\n",
        "    }\n",
        ")\n",
        "graph.add_edge(\"synthesize\",\"evaluate\")\n",
        "graph.add_conditional_edges(\n",
        "    \"evaluate\",\n",
        "    needs_refinement,\n",
        "    {\n",
        "        \"refine\": \"refine\", #continue researching\n",
        "        \"complete\":END #Finish execution\n",
        "    }\n",
        ")\n",
        "graph.add_edge(\"refine\",\"evaluate\")\n",
        "#set the entry point\n",
        "graph.set_entry_point(\"breakdown\")\n",
        "#Compile the graph into an executable\n",
        "research_agent = graph.compile()\n",
        "\n",
        "#user interface\n",
        "def run_research_agent(question:str):\n",
        "  \"\"\"Run the research agent on a given question\"\"\"\n",
        "  initial_state={\n",
        "      \"question\":question,\n",
        "      \"sub_questions\":[],\n",
        "      \"research_findings\":{},\n",
        "      \"current_sub_question\":None,\n",
        "      \"final_answer\":None,\n",
        "      \"evaluation\":None,\n",
        "      \"messages\":[]\n",
        "  }\n",
        "  print(f\"Researching:{question}\\n\")\n",
        "  print(f\"Start the research process...\\n\")\n",
        "\n",
        "  final_state = research_agent.invoke(initial_state)\n",
        "\n",
        "  print(\"\\n=== Sub-Questions===\")\n",
        "  for i , q in enumerate(final_state[\"sub_questions\"],1):\n",
        "    print(f\"{i+1}.{q}\")\n",
        "\n",
        "  print(\"\\n=== Final Answer===\")\n",
        "  print(final_state[\"final_answer\"])\n",
        "\n",
        "  print(\"\\n=== Evaluation===\")\n",
        "  print(final_state[\"evaluation\"])\n",
        "  return final_state\n",
        "\n",
        "#Example usage\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  question = \"What are the enviornmental and economic impacts of switching to renewable energy sources , what are the main challenges to widespread adoption?\"\n",
        "  run_research_agent(question)"
      ],
      "metadata": {
        "id": "93tbXmvW3Ayi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}