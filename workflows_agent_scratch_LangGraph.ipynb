{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ6zbxHpWyL-"
      },
      "outputs": [],
      "source": [
        "#building agents from scratch .by god's grace. Jai Shri Ram . Jai Bajrangbali\n",
        "#https://www.youtube.com/watch?v=aHCDrAbH_go&t=120s - LangGraph\n",
        "!pip install langchain_core langchain-anthropic langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import os ,getpass\n",
        "def _set_env(var:str):\n",
        "    os.environ[var] = getpass.getpass(f\"Enter your {var}: \")\n",
        "_set_env(\"ANTHROPIC_API_KEY\")"
      ],
      "metadata": {
        "id": "MuLs5YP-XOMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#llm\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n"
      ],
      "metadata": {
        "id": "QqjNb4ycYRdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#schema for structured output\n",
        "from pydantic import BaseModel , Field\n",
        "class SearchQuery(BaseModel):\n",
        "    search_query: str = Field(None , description=\"Query that is optimized web search.\")\n",
        "    justification: str = Field(None , description=\"Why this query is relevant to user's request.\")\n",
        "\n",
        "#Augment the LLM with schema for structured output\n",
        "structured_llm = llm.with_structured_output(SearchQuery)\n",
        "\n",
        "#invoke the augmented LLM\n",
        "output = structured_llm.invoke(\"How does Calcium CT score relate to high chlorestrol levels?\")\n",
        "print(output.search_query)\n",
        "print(output.justification)"
      ],
      "metadata": {
        "id": "-k71NsC-Ykb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tool calling\n",
        "def multiply(a:int,b:int) ->int:\n",
        "  return a*b\n",
        "#augment the LLM with tools\n",
        "llm_with_tools = llm.bind_tools([multiply])\n",
        "#invoke the LLM with input that triggers the tool call\n",
        "msg = llm_with_tools.invoke(\"What is 2 times 3?\")\n",
        "print(msg)\n",
        "\n",
        "#get the tool call\n",
        "msg.tool_calls"
      ],
      "metadata": {
        "id": "7JOHCqIVa4R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt chaining\n",
        "#each llm processes the output of the other llm in a chain\n",
        "#when do you want to do this\n",
        "#use it in Conv AI .... in LLM-as-a-judge\n",
        "#!pip install typing_extensions\n",
        "from typing_extensions import TypedDict\n",
        "#graph state for passing thru one llm to another llm. this is KEY\n",
        "class State(TypedDict):\n",
        "  topic: str\n",
        "  joke: str\n",
        "  improved_joke: str\n",
        "  final_joke: str"
      ],
      "metadata": {
        "id": "eeiY2laLbr3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nodes\n",
        "def generate_joke(state:State):\n",
        "  \"\"\"First LLM call to generate initial joke\"\"\"\n",
        "  msg = llm.invoke(f\"Write a short joke {state('topic')}\")\n",
        "  return {\"joke\":msg.content}\n",
        "\n",
        "def improve_joke(state:State):\n",
        "  \"\"\"Second LLM call to improve the joke\"\"\"\n",
        "  msg = llm.invoke(f\"Make this joke funnier by adding wordplay{state('joke')}\")\n",
        "  return {\"improved_joke\":msg.content}\n",
        "\n",
        "def polish_joke(state:State):\n",
        "  \"\"\"Third LLM call for final polish of the joke content to make it better\"\"\"\n",
        "  msg = llm.invoke(f\"Add a surprising twist to this joke{state('joke')}\")\n",
        "  return {\"final_joke\": msg.content}\n",
        "\n",
        "#conditional edge function to check if the joke has a punchline\n",
        "def check_punchline(state:State): #conditional EDGE In Langgraph - what is the condition to move from one llm to another\n",
        "    \"\"\"Gate Function to check if the joke has a punchline\"\"\"\n",
        "    if \"?\" in state[\"improved_joke\"] or \"!\" in state[\"joke\"]:\n",
        "      return \"Pass\"\n",
        "    return \"Fail\""
      ],
      "metadata": {
        "id": "hOYkkSWIcm4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#langgraph simple workflow\n",
        "from langgraph.graph import StateGraph , START , END\n",
        "from IPython.display import Image, display\n",
        "\n",
        "#build workflow\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(generate_joke, generate_joke)\n",
        "workflow.add_node(improve_joke,improve_joke)\n",
        "workflow.add_node(polish_joke,polish_joke)\n",
        "\n",
        "#add edges to connect nodes\n",
        "workflow.add_edge(START,\"generate_joke\")\n",
        "workflow.add_conditional_edges(\"generate_joke\",check_punchline , {\"Pass\":\"improve_joke\", \"Fail\":END})\n",
        "workflow.add_edge(\"improve_joke\",\"polish_joke\")\n",
        "workflow.add_edge(\"polish_joke\",END)\n",
        "\n",
        "#compile workflow\n",
        "chain = workflow.compile()\n",
        "\n",
        "#show workflow\n",
        "display(Image(chain.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "qyyGyMPyf_U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "state = chain.invoke({\"topic\":\"cats\"})\n",
        "print(\"Intial Joke:\")\n",
        "print(state[\"joke\"])\n",
        "print(\"\\n=== === ===\\n\")\n",
        "if \"improved_joke\" in state:\n",
        "  print(\"Improved Joke:\")\n",
        "  print(state[\"improved_joke\"])\n",
        "  print(\"Final Joke:\")\n",
        "  print(state[\"final_joke\"])\n",
        "else:\n",
        "  print(\"joke failed quality gate = no punchline detected\")"
      ],
      "metadata": {
        "id": "y0zQE7VljCD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parallelization\n",
        "#one task and you fan out to sub task to separate LLMs to parallelize the task and do a final aggregration of the task\n",
        "#Graph State\n",
        "class State(TypedDict):\n",
        "  topic:str\n",
        "  joke: str\n",
        "  story: str\n",
        "  poem:str\n",
        "  combined_output: str"
      ],
      "metadata": {
        "id": "YbaHd2mHkHQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nodes\n",
        "def call_llm_1(state:State):\n",
        "  \"\"\" Fist LLM call to generate intial joke \"\"\"\n",
        "  msg = llm.invoke(f\"Write a short joke about {state('topic')}\")\n",
        "  return {\"joke\":msg.content}\n",
        "\n",
        "def call_llm_2(state:State):\n",
        "  \"\"\" Second LLM call to generate Story\"\"\"\n",
        "  msg = llm.invoke(f\"Write a short story about {state('topic')}\")\n",
        "  return {\"story\":msg.content}\n",
        "\n",
        "def call_llm_3(state:State):\n",
        "  \"\"\"Third LLM call to generate Poem\"\"\"\n",
        "  msg = llm.invoke(f\"Write a short poem about {state('topic')}\")\n",
        "  return {\"poem\":msg.content}\n",
        "\n",
        "def aggregator(state:State):\n",
        "  \"\"\" Combine the joke and story into a single output\"\"\"\n",
        "  combined = f\"Here's a story , joke and poem about {state['topic']}!\\n\\n\"\n",
        "  combined += f\"Joke: {state['joke']}\\n\\n\"\n",
        "  combined += f\"Story: {state['story']}\\n\\n\"\n",
        "  combined += f\"Poem: {state['poem']}\"\n",
        "  return {\"combined_output\":combined}"
      ],
      "metadata": {
        "id": "PVUaI_Tgk5D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build parallel workflow\n",
        "parallel_builder = StateGraph(State)\n",
        "\n",
        "#add nodes\n",
        "parallel_builder.add_node(\"call_llm_1\",call_llm_1)\n",
        "parallel_builder.add_node(\"call_llm_2\",call_llm_2)\n",
        "parallel_builder.add_node(\"call_llm_3\",call_llm_3)\n",
        "parallel_builder.add_node(\"aggregator\",aggregator)\n",
        "\n",
        "#add edges to connect nodes\n",
        "parallel_builder.add_edge(START,\"call_llm_1\")\n",
        "parallel_builder.add_edge(START,\"call_llm_2\")\n",
        "parallel_builder.add_edge(START,\"call_llm_3\")\n",
        "parallel_builder.add_edge(\"call_llm_1\",\"aggregator\")\n",
        "parallel_builder.add_edge(\"call_llm_2\",\"aggregator\")\n",
        "parallel_builder.add_edge(\"call_llm_3\",\"aggregator\")\n",
        "parallel_builder.add_edge(\"aggregator\",END)\n",
        "parallel_workflow = parallel_builder.compile()\n",
        "\n",
        "#show workflow\n",
        "display(Image(parallel_workflow.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "RT7yU3tMmVrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#routing\n",
        "#take a input and route the input to poem, story or joke generation based on user input and then aggregate\n",
        "from typing_extensions import Literal\n",
        "class Route(BaseModel):\n",
        "  step:Literal[\"poem\",\"story\",\"joke\"] = Field(None , description = \"The next step in the routing process\")\n",
        "#augment the LLM with schema for structured output\n",
        "router = llm.with_structured_output(Route)"
      ],
      "metadata": {
        "id": "Nwy86dxvnO8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#state for routing\n",
        "#one task and you fan out to sub task to separate LLMs to parallelize the task and do a final aggregration of the task\n",
        "#Graph State\n",
        "class State(TypedDict):\n",
        "  input:str\n",
        "  decision: str\n",
        "  output: str"
      ],
      "metadata": {
        "id": "JuAL-tGtpfiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from langchain_core.messages import HumanMessage , SystemMessage\n",
        "\n",
        "def call_llm_1(state:State):\n",
        "  \"\"\" Write a story \"\"\"\n",
        "  result = llm.invoke(f\"Write a short story about {state('input')}\")\n",
        "  return {\"output\":result.content}\n",
        "\n",
        "def call_llm_2(state:State):\n",
        "  \"\"\" Write a Joke \"\"\"\n",
        "  result = llm.invoke(f\"Write a short joke  about {state('input')}\")\n",
        "  return {\"output\":result.content}\n",
        "\n",
        "def call_llm_3(state:State):\n",
        "  \"\"\" Write a Poem  \"\"\"\n",
        "  result = llm.invoke(f\"Write a short poem  about {state('input')}\")\n",
        "  return {\"output\":result.content}\n",
        "\n",
        "def llm_call_router(state:State):\n",
        "  \"\"\" Route the input to the appropiate node \"\"\"\n",
        "  #run the augmented LLM with structured output to serve as routing logic\n",
        "  decision = router.invoke([SystemMessage(content = \"Route the input to Story , Joke or Poem based on the user's request.\"),\n",
        "                            HumanMessage(content = state(\"input\"))])\n",
        "  return {\"decision\": decision.step}\n",
        "\n",
        "#conditional edge function to route to the appropiate node #dotted line show conditional edge\n",
        "def route_decision(state:State):\n",
        "  #return the node name you want to visit next\n",
        "  if state[\"decision\"] == \"story\":\n",
        "    return call_llm_1\n",
        "  elif state[\"decision\"] == \"joke\":\n",
        "    return call_llm_2\n",
        "  elif state[\"decision\"] == \"poem\":\n",
        "    return call_llm_3\n",
        "  else:\n",
        "    return"
      ],
      "metadata": {
        "id": "xSgtnfSSpxNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build parallel workflow\n",
        "router_builder = StateGraph(State)\n",
        "\n",
        "#add nodes\n",
        "router_builder.add_node(\"call_llm_1\",call_llm_1)\n",
        "router_builder.add_node(\"call_llm_2\",call_llm_2)\n",
        "router_builder.add_node(\"call_llm_3\",call_llm_3)\n",
        "#router_builder.add_node(\"aggregator\",aggregator)\n",
        "\n",
        "#add edges to connect nodes\n",
        "router_builder.add_edge(\"call_llm_1\",END)\n",
        "router_builder.add_edge(\"call_llm_2\",END)\n",
        "router_builder.add_edge(\"call_llm_3\",END)\n",
        "\n",
        "router_workflow = router_builder.compile()\n",
        "#show workflow\n",
        "display(Image(router_builder.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "kRvsFMqzsLNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Orchestrator-Worker - LLM breaks down a task and delegate each task to a worker and synthensize to provide outcome\n",
        "from typing import Annotated , List\n",
        "import operator\n",
        "#schema for structure output to use in planning\n"
      ],
      "metadata": {
        "id": "UtAskB8mtJ22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluator-optimizer workflow\n",
        "#one LLM generates a response while another LLM evaluates and provide feedback in loop\n",
        "#schema for structured output to use in evaluation\n",
        "class Feedback(BaseModel):\n",
        "  grade:Literal[\"funny\",\"not funny\"] = Field(description = \"Decide if the joke is funny or not\",)\n",
        "  feedback :str = Field(description = \"Explain why the joke is funny or not\")\n",
        "\n",
        "#augment the llm with schema for structured output\n",
        "evaluator = llm.with_structured_output(Feedback)"
      ],
      "metadata": {
        "id": "AoYGl7oX5c9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Graph State\n",
        "class State(TypedDict):\n",
        "  joke:str\n",
        "  topic:str\n",
        "  feedback:str\n",
        "  funny_or_not:str"
      ],
      "metadata": {
        "id": "yefaGBr063fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nodes\n",
        "def llm_call_generator(state:State):\n",
        "  \"\"\"LLM generate a joke\"\"\"\n",
        "  if state.get(\"feedback\"):\n",
        "    msg = llm.invoke(f\"Write a joke about {state ['topic']} but take this feedback into account {state['feedback']}\")\n",
        "  else:\n",
        "    msg = llm.invoke(f\"Write a joke about {state ['topic']}\")\n",
        "  return {\"joke\":msg.content}\n",
        "\n",
        "def llm_call_evaluator(state:State):\n",
        "  \"\"\" LLM evaluates a joke \"\"\"\n",
        "  grade = evaluator.invoke(f\"Grade the joke{state['joke']}\")\n",
        "  return {\"funny_or_not\":grade.grade , \"feedback\":grade.feedback}\n",
        "\n",
        "#conditional edge function to route back to joke generator or end based upon feedback from the evaluator\n",
        "def route_joke():\n",
        "  \"\"\" Route the joke back to the joke generator or end based upon feedback from the evaluator\"\"\"\n",
        "  if state[\"funny_or_not\"] == \"funny\":\n",
        "    return \"Accepted\"\n",
        "  elif state[\"funny_or_not\"] == \"not funny\":\n",
        "    return \"Rejected + Feedback\""
      ],
      "metadata": {
        "id": "7VG5ZYEv7Qr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agent\n",
        "#remove scafolding and allow LLM to take actions\n",
        "#define tools using tool decorator\n",
        "#agent\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def multiply(a:int , b:int) ->int:\n",
        "  return a*b\n",
        "\n",
        "@tool\n",
        "def add(a:int , b:int) ->int:\n",
        "  return a+b\n",
        "\n",
        "@tool\n",
        "def divide(a:int , b:int) ->int:\n",
        "  return a/b\n",
        "\n",
        "@tool\n",
        "def subtract(a:int , b:int) ->int:\n",
        "  return a-b\n",
        "\n",
        "#augment the llm with tools\n",
        "tools = [add, multiply, divide, subtract]\n",
        "tools_by_name = {tool.name:tool for tool in tools}\n",
        "llm_with_tools = llm.bind_tools(tools)\n"
      ],
      "metadata": {
        "id": "7tT0nbOE9kak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import ToolMessage\n",
        "from IPython.display import Image, display\n",
        "\n",
        "#Nodes\n",
        "def llm_call(state:MessagesState):\n",
        "  \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
        "  return {\"messages\":[llm_with_tools.invoke([SystemMessage(content= \"You are a helpful Assistant tasked with performing arithmetic on a set of inputs\")]+ state[\"messages\"])]}\n",
        "\n",
        "def tool_node(state:dict):\n",
        "  \"\"\"Call a tool\"\"\"\n",
        "  result = []\n",
        "  for tool_call in state[\"messages\"][-1].tool_calls:\n",
        "    tool = tools_by_name[tool_call[\"name\"]]\n",
        "    observation = tool.invoke(tool_call[\"args\"])\n",
        "    result.append(ToolMessage(content = observation , tool_call_id = tool_call[\"id\"]))\n",
        "  return {\"messages\":result}\n",
        "\n",
        "def should_continue(state:MessagesState) -> Literal[\"enviornment\":\"END\"]:\n",
        "  \"\"\" decide if we should continue the loop or stop based on upon whether the LLM made a tool call\"\"\"\n",
        "  messages = state[\"messages\"]\n",
        "  last_message = messages[-1]\n",
        "  if last_message.tool_calls:\n",
        "        return \"Action\"\n",
        "  return \"END\""
      ],
      "metadata": {
        "id": "zlbibCL6_cLK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}