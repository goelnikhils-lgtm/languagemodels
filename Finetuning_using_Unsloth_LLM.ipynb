{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN9eocECSbsaaQs6F2bnQXz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Finetuning_using_Unsloth_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SOURCE CODE CREDIT - TRELLIS RESEARCH - https://www.youtube.com/watch?v=Ik6nbAjxLk4\n",
        "#This also has LLM-As-A-Judge\n",
        "#so the logic is evaluate your model before fine tuning using LLM-As-A-Judge and then evaluate post fine tuning\n",
        "#fine tune on response only - this is key .... mask the instruction"
      ],
      "metadata": {
        "id": "58H-xcpRa8L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R3WHvBwu-cQ"
      },
      "outputs": [],
      "source": [
        "#vLLM Evalaution + UnSloth Training MS PHI4-Instruct Model on Q&A for Virtual Assitant\n",
        "#latest version\n",
        "!pip install uv\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "from torch import __version__; from packaging.version import Version as V\n",
        "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
        "!pip install --no-deps {xformers}\n",
        "!uv pip install datasets tensorboard openai hf_transfer accelerate pillow -qU --system\n",
        "!uv pip install scikit-learn pymupdf -qU --system\n",
        "!uv pip install google.generativeai -qU --system\n",
        "!uv pip install flashinfer-python --system -qU\n",
        "!uv pip install vllm\n",
        "!pip install numpy\n",
        "!pip install scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfFolder , login\n",
        "if HfFolder.get_token() is None:\n",
        "  login()"
      ],
      "metadata": {
        "id": "KCaKlesupMvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_slug =\"microsoft/Phi-4-mini-instruct\"\n",
        "dataset_name = \"Trelis/touch-rugby-comprehensive-qa\"\n",
        "train_split_name = \"train\"\n",
        "eval_split_name = \"eval\"\n",
        "\n",
        "#dataset\n",
        "q_column =\"question\"\n",
        "c_coumn = \"evaluation_criteria\"\n",
        "a_column = \"answer\""
      ],
      "metadata": {
        "id": "DvTtIKygqNEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up LLM-As-a-Judge\n",
        "import os\n",
        "os.environ[\"PROVIDER\"] = \"gemini\"\n",
        "judge_model = \"gemini-2.0-flash\"\n",
        "max_seq_length = 8192 #long contect length for reasoning we need long context length\n",
        "temperature = 0.01\n",
        "os.environ[\"UNSLOTH_COMPILE_DISABLE\"] = \"1\"\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "os.environ[\"HF_HOME\"] = \"./\"\n",
        "\n",
        "#setup the client for the judge\n",
        "from __future__ import annotations\n",
        "import getpass\n",
        "from pathlib import Path\n",
        "from typing import List,Dict,Any\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata # Import userdata to access Colab secrets\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "api_key = userdata.get(\"GEMINI_API_KEY\")\n",
        "if api_key is None:\n",
        "  raise ValueError(\"GEMINI_API_KEY not found in Colab secrets. Please add it to the secrets manager.\")\n",
        "\n",
        "model_name = os.getenv(\"GEMINI_MODEL_NAME\",\"gemini-2.0-flash\")\n",
        "\n",
        "base_url = os.getenv(\n",
        "    \"GEMINI_BASE_URL\",\n",
        "    \"https://generativelanguage.googleapis.com/v1beta/openai\",\n",
        ")\n",
        "client = OpenAI(api_key=api_key,base_url=base_url) #use OpenAI client . this can call any LLM\n",
        "\n",
        "#unified chat wrapper\n",
        "def chat(messages: List[Dict[str,str]],**kwargs:Any) ->str:\n",
        "  response = client.chat.completions.create(\n",
        "      model=judge_model,\n",
        "      n=1,\n",
        "      messages=messages,\n",
        "      **kwargs\n",
        "  )\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "q6FbeWZ2rbYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the dataset\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "\n",
        "#load the dataset\n",
        "ds_dict = load_dataset(dataset_name)\n",
        "print(\"Splits found\",list[ds_dict.keys()])\n",
        "\n",
        "train_data = ds_dict[train_split_name]\n",
        "eval_data = ds_dict[eval_split_name]\n",
        "print(\"Train data size\",len(train_data))\n",
        "print(\"Eval data size\",len(eval_data))\n",
        "print(train_data)\n",
        "print(eval_data)"
      ],
      "metadata": {
        "id": "FHY98ZC01DkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!uv pip install transformers -qU --system\n",
        "#!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "id": "XjY_HrSYPI9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the model to evaluate\n",
        "#!uv pip install vllm\n",
        "import json\n",
        "import re\n",
        "from vllm import LLM, SamplingParams\n",
        "import torch\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = temperature, # at high temperature it is better to have min_p sampling to handle coherence and diversity. high temperature means exploration/diversity\n",
        "    top_k = 40,\n",
        "    top_p= 0.95, #token at very low probability are not accepted\n",
        "    min_p=0.1,  # use to balance text_coherence and creativity . mostly used when we specify high temperature\n",
        "    max_tokens = 6000 #max tokens generated\n",
        ")\n",
        "#max_model_len\n",
        "model = LLM(model=model_slug,gpu_memory_utilization=0.9,dtype=\"bfloat16\")"
      ],
      "metadata": {
        "id": "ktyGM4hg_z-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate loaded model . this model is not yet fine tuned\n",
        "import re\n",
        "from pydantic import BaseModel\n",
        "SYSTEM_MESSAGE =\"\"\"\n",
        "You are an expert evaluator tasked with determining if an answer satisfies specified evaluation criteria.\n",
        "\n",
        "You will receive\n",
        "1.A question\n",
        "2.The evaluation criteria\n",
        "3.The model's answer to evaluate\n",
        "\n",
        "First, explain how well the answer meets each of the specified criteria.\n",
        "Then provide your scores.\n",
        "-Score 1 - if the answer fully satisfies ALL the specified criteria\n",
        "-Score 0 - if it fails to meet ANY of the criteria\n",
        "\n",
        "Format of your response as:\n",
        "[Your detailed explanation of how the answer meets of fails the criteria]\n",
        "Score:[Just the number 1 or 0 , with no additional text]\n",
        "\"\"\".strip()\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Question:{question}\n",
        "Evaluation Criteria:{evaluation_criteria}\n",
        "Model Answer:{model_answer}\n",
        "\"\"\".strip()\n",
        "\n",
        "#helper function to call the judge parse it reply\n",
        "class EvalautionResult(BaseModel):\n",
        "  reason:str #give reason for evaluation\n",
        "  is_correct:bool #indication of evaluation  correct / not correct\n",
        "SCORE_RE = re.compile(r\"Score:\\s*([01])\\s*\",re.IGNORECASE + re.MULTILINE) #regular expression to extract score 1 or 0\n",
        "THINK_RE = re.compile(r\"<\\s*think\\s*\\(.*?\\)\\s*>\",re.IGNORECASE | re.DOTALL) # regular expression to strip any thinking as it will be hard to evaluate with thinking\n",
        "\n",
        "#evaluate function takes a generated answer from the model , ground truth and evaluatues the generated answer vs ground truth\n",
        "def evaluate_answer(question:str,evaluation_criteria:str,ground_truth:str | None , generated_answer:str , *,strip_reasoning: bool =True)-> tuple[bool,str]:\n",
        "  \"\"\" returns (is_correct,reason_from_judge)\n",
        "  if strip_reasoning = True , all <think> </think> blocks are removed from generated_answer before it is fed to judge model\n",
        "  \"\"\"\n",
        "  if strip_reasoning:\n",
        "    generated_answer = THINK_RE.sub(\"\",generated_answer).strip()\n",
        "  judge_prompt = PROMPT_TEMPLATE.format(question=question,evaluation_criteria=evaluation_criteria,model_answer=generated_answer)\n",
        "\n",
        "  #call your judge LLM\n",
        "  completion = client.chat.completions.create(\n",
        "      model=judge_model,\n",
        "      messages=[\n",
        "      {\"role\":\"system\", \"content\":SYSTEM_MESSAGE},\n",
        "      {\"role\":\"user\", \"content\":judge_prompt}\n",
        "      ],\n",
        "      temperature=0.0,\n",
        "      top_p=1.0,\n",
        "      max_tokens=1024,\n",
        "      )\n",
        "  judge_reply = completion.choices[0].message.content\n",
        "\n",
        "  # parse judge's reply\n",
        "  m = SCORE_RE.search(judge_reply)\n",
        "  if not m:\n",
        "    return False\n",
        "  score = int(m.group(1))\n",
        "  reason = judge_reply[:m.start()].strip()\n",
        "  return bool(score),reason # returns a tuple of score and reason\n",
        "\n",
        "  #evaluate a model by giving user role and providing content as problem message then generate anwer and provide this to evaluate model and this a full loop\n",
        "  #not coding here"
      ],
      "metadata": {
        "id": "uVlUS0L3SWDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we want to evaluate batch of questions using vLLM so we use Threadpool executor class and run parallel threads\n",
        "#SET UP EVALUATION WITH BATCHING OF ANSWER GENERATION AND THREADING FOR JUDGING\n",
        "#make parallel calls to Gemini API - LLM-As-a-Judge"
      ],
      "metadata": {
        "id": "AYcpLXAiawHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfFolder , login\n",
        "if HfFolder.get_token() is None:\n",
        "  login()\n",
        "\n",
        "model_slug =\"microsoft/Phi-4-mini-instruct\"\n",
        "max_seq_length = 8192\n",
        "dtype = None\n",
        "load_in_4bit = False\n",
        "load_in_8bit = False\n",
        "\n",
        "dataset_name = \"Trelis/touch-rugby-comprehensive-qa\"\n",
        "train_split_name = \"train\"\n",
        "eval_split_name = \"eval\"\n",
        "\n",
        "#dataset\n",
        "q_column =\"question\"\n",
        "c_coumn = \"evaluation_criteria\"\n",
        "a_column = \"answer\""
      ],
      "metadata": {
        "id": "OUuxURBu1iNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fine tune\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import gc, inspect , sys\n",
        "\n",
        "model , tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_slug,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    load_in_8bit = load_in_8bit,\n",
        ")\n",
        "print(tokenizer.padding_side) #pad on right hand side or what ever model deafault is"
      ],
      "metadata": {
        "id": "KUzDPD4r2NpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank = 32\n",
        "lora_alpha = 50\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = rank,\n",
        "    finetune_attention_modules = True,\n",
        "    lora_alpha = lora_alpha,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    full_finetuning = False,\n",
        "    random_state = 3407,\n",
        "    use_rslora= True,\n",
        "    )"
      ],
      "metadata": {
        "id": "ziLnb19N6CH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "Bq72Jhiz79c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ft_data = load_dataset(dataset_name)\n",
        "ft_train_data = ft_data[\"train\"]\n",
        "ft_eval_data = ft_data[\"eval\"]\n",
        "print(\"Train data size\",len(ft_train_data))\n",
        "print(\"Eval data size\",len(ft_eval_data))\n",
        "print(ft_train_data)\n",
        "print(ft_eval_data)\n",
        "print(ft_train_data[\"question\"][0])\n",
        "print(ft_eval_data[\"question\"][0])"
      ],
      "metadata": {
        "id": "dRGNpKvR8GZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#formatting the dataset\n",
        "def formatting_func(batch):\n",
        "  texts = []\n",
        "  for i in range(len(batch[q_column])):\n",
        "    user_content = batch[q_column][i]\n",
        "    print(\"user_content\",user_content)\n",
        "    assistant_content = batch[a_column][i]\n",
        "    messages =[\n",
        "        {\"role\":\"user\", \"content\":user_content},\n",
        "        {\"role\":\"assistant\", \"content\":assistant_content}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=False)\n",
        "    if text.startswith(\"<bos>\"):\n",
        "      text = text[len(\"<bos>\"):]\n",
        "    texts.append(text)\n",
        "  print(\"texts\",text)\n",
        "  return texts"
      ],
      "metadata": {
        "id": "M0y6LPC49DSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatting_func(ft_eval_data)[0])"
      ],
      "metadata": {
        "id": "gMkz7DooR_Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments , DataCollatorForSeq2Seq\n",
        "from transformers import get_scheduler\n",
        "from datetime import datetime\n",
        "\n",
        "per_device_train_batch_size = 4\n",
        "gradient_accumlation_steps = 8 #gradient accumlation steps\n",
        "#smaller model can fit larger batch size in memory\n",
        "#virtual batch size is 32\n",
        "epochs = 2 #one epoch at constant rate and one epoch decaying at constant rate (annealing)\n",
        "learning_rate = 1e-4\n",
        "\n",
        "#get current timestamp\n",
        "#Define training variables\n",
        "#steps\n",
        "total_steps = (len(ft_train_data) // (per_device_train_batch_size * gradient_accumlation_steps)) * epochs # batchsize *gradient accumlation steps\n",
        "warm_up_steps = int(0.01*total_steps) #1% of total steps .... warmup steps ....\n",
        "anneal_start_step = int(0.5*total_steps) #Annealing starts at 50% of total steps - this is key ....\n",
        "\n",
        "\n",
        "#intialize optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate, weight_decay = 0.01)\n",
        "\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "def custom_lr_scheduler(optimizer, num_warmup_steps, num_training_steps, anneal_start_step): #custom scheduler\n",
        "  def lr_lambda(current_step):\n",
        "    if current_step < num_warmup_steps:\n",
        "      return float(current_step) / float(max(1, num_warmup_steps))\n",
        "    elif current_step < anneal_start_step:\n",
        "      return 1.0\n",
        "    else:\n",
        "      return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - anneal_start_step)))\n",
        "  return LambdaLR(optimizer, lr_lambda)\n",
        "#create the learning rate scheduler\n",
        "scheduler = custom_lr_scheduler(optimizer,warm_up_steps,total_steps,anneal_start_step)"
      ],
      "metadata": {
        "id": "KfPKK2O3-rGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting training steps\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "common_args={\n",
        "    \"per_device_train_batch_size\" :per_device_train_batch_size,\n",
        "    \"per_device_eval_batch_size\": per_device_train_batch_size,\n",
        "    \"gradient_accumulation_steps\":gradient_accumlation_steps,\n",
        "    \"num_train_epochs\":epochs,\n",
        "    \"logging_strategy\":\"steps\",\n",
        "    \"eval_strategy\":\"steps\",\n",
        "    \"logging_steps\":min(max(int(0.5*total_steps),1),10),\n",
        "    \"eval_steps\":min(max(int(0.5*total_steps),1),10),\n",
        "    \"bf16\": is_bfloat16_supported(),\n",
        "    \"fp16\": not is_bfloat16_supported(),\n",
        "    \"report_to\": \"tensorboard\",\n",
        "    \"seed\":3407,\n",
        "    \"output_dir\":\"outputs\",\n",
        "    \"gradient_checkpointing\":True,\n",
        "    \"gradient_checkpointing_kwargs\":{\"use_reentrant\":True},\n",
        "    \"remove_unused_columns\":True\n",
        "}\n",
        "common_args[\"dataset_num_proc\"] = 1\n",
        "common_args[\"max_seq_length\"] = max_seq_length"
      ],
      "metadata": {
        "id": "dxpA2tkVyXf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fine tuning\n",
        "#run_name = f\"{run_name}-ft\"\n",
        "#print(f\"Setting up for run:{run_name}\")\n",
        "\n",
        "#common_args[\"run_name\"] = \"run_name\" #model name\n",
        "#common_args[\"logging_dir\"]*f\"./logs/{run_name}\" #logging directory\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer= tokenizer,\n",
        "    train_dataset = ft_train_data,\n",
        "    eval_dataset = ft_eval_data,\n",
        "    formatting_func = formatting_func,\n",
        "    args = SFTConfig(**common_args),\n",
        ")\n",
        "#must be done outside because of unsloth\n",
        "trainer.optimizer = optimizer\n",
        "trainer.lr_scheduler = scheduler\n",
        "\n",
        "#disable the internal builders so they don't override again\n",
        "trainer.create_optimizer = lambda *a , **k:trainer.optimizer\n",
        "trainer.create_scheduler = lambda *a , **k:trainer.lr_scheduler\n",
        "print(trainer.train_dataset)\n",
        "print(tokenizer.chat_template)"
      ],
      "metadata": {
        "id": "8wHWV0x64ji-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training on completions / responses only and don't use Instructions . This will help model to fine tune better on response and perform better\n",
        "#applicable for Conversational use cases\n",
        "\n",
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "TEMPLATES = {\n",
        "    \"llama\":{\n",
        "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        "    },\n",
        "    \"gemma\":{\n",
        "        \"<start_of_turn>user\\n\",\n",
        "        \"<start_of_turn>model\\n\",\n",
        "    },\n",
        "    \"qwen\":{\n",
        "        \"<|im_start|>user\\n\",\n",
        "        \"<|im_start|>assitant\\n\",\n",
        "    },\n",
        "    \"mistral\":{\n",
        "        \"[INST]\",\n",
        "        \"[/INST]\",\n",
        "    },\n",
        "    \"phi\":{\n",
        "        \"<|user|>\\n\",\n",
        "        \"<|end|><assistant|>\\n\",\n",
        "    }\n",
        "}\n",
        "instruction_tag , response_tag = TEMPLATES[\"phi\"] # Changed from \"mistral\" to \"phi\"\n",
        "#masks everything between instruction_part and response_part\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part= instruction_tag,\n",
        "    response_part = response_tag,\n",
        ")"
      ],
      "metadata": {
        "id": "xWVkgiUwA8vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#decoding\n",
        "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])\n",
        "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]]).replace(tokenizer.pad_token,\"\")"
      ],
      "metadata": {
        "id": "HmZh3zF3J6PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "ofColHvjM3IN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}