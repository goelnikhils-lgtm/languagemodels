{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBfcQl1QB7yXcD6kbCSwgf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/GatedAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GATED ATTENTION AND GATED DELTA MULTI ATTENTION\n",
        "USED IN QWEN3.0 AND KIMI MODELS\n",
        "THESE ATTENTION ENABLES MODELS TO\n",
        "HAVE EXTREMELY LONG CONTEXT WINDOW AS KV CACHE SIZES REDUCES IN  GATED DELTA MULTI ATTENTION\n",
        "\n",
        "https://sebastianraschka.com/llms-from-scratch/ch04/08_deltanet/\n",
        "\n"
      ],
      "metadata": {
        "id": "3RjY4KHl68JF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "lSaZxU_i0gYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfmg0qh3oIHj"
      },
      "outputs": [],
      "source": [
        "#GATED ATTENTION - THIS IS used bu Qwen 3.0\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.functional as F\n",
        "\n",
        "class GatedDeltaNet(nn.Module):\n",
        "  def __init__(self,d_in, d_out , dropout,num_heads, qkv_bias = False):\n",
        "    super().__init__()\n",
        "    assert d_out % num_heads == 0, 'd_out must be divisible by num_heads'\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    ######################################################\n",
        "    ### New: Gates for delta rule and output gating\n",
        "    self.W_gate = nn.Linear(d_in, d_out, bias=False)\n",
        "    self.W_beta = nn.Linear(d_in, d_out, bias=False)\n",
        "    #Note: The decay gate alpha corresponds to\n",
        "    #A_log + W_alpha(x) = dt_bias\n",
        "\n",
        "    self.W_alpha = nn.Linear(d_in, num_heads, bias=False)\n",
        "    self.dt_bias = nn.Parameter(torch.ones(num_heads))\n",
        "    A_init = torch.empty(num_heads).uniform(0,16)\n",
        "    self.A_log = nn.Parameter(torch.log(A_init))\n",
        "    self.norm = nn.RMSNorm(self.head_dim,eps=1e-6)  #RMS Normalization\n",
        "    ##################################################################\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, _ = x.shape\n",
        "    queries = self.W_query(x)\n",
        "    keys = self.W_key(x)\n",
        "    values = self.W_value(x)\n",
        "    ######################################################\n",
        "    #NEW COMPUTE delta rule gates\n",
        "    beta = torch.sigmoid(self.W_beta(x))\n",
        "    alpha =-self.A_log.exp().view(1,1,-1) * F.softplus(self.W_alpha(x)+self.dt_bias)\n",
        "    gate = self.W_gate(x)\n",
        "    ######################################################\n",
        "\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    beta = beta.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    gate = gate.view(b, num_tokens, self.num_heads, self.head_dim) #NEW\n",
        "\n",
        "    keys = keys.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "    beta = beta.transpose(1,2)\n",
        "    gate = gate.transpose(1,2) #NEW\n",
        "\n",
        "    #######################################################\n",
        "    ### NEW QKNorm-like normalization for delta rule\n",
        "    queries = l2norm(queries,dim>1)/(self.head_dim**0.5)\n",
        "    keys = l2norm(keys,dim=-1)\n",
        "    #######################################################\n",
        "    S = x.new_zeros(b, self.num_heads, self.head_dim, self.head_dim)\n",
        "    outs =[]\n",
        "    #######################################################\n",
        "    ### New: Gated  rule update\n",
        "    for t in range(num_tokens):\n",
        "      k_t = keys[:,:,t]\n",
        "      q_t = queries[:,:,t]\n",
        "      v_t = values[:,;,t]\n",
        "      beta_t = beta[:,:,t]\n",
        "      a_t = alpha[:,t].unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "      S = S*a_t.exp()\n",
        "      kv_mem = (S*k_t.unsqueeze(-1)).sum(dim=-2)\n",
        "      delta = (v_t-kv_mem) *beta_t\n",
        "      S = S + k_t.unsqueeze(-1) * delta.unsqueeze(-2)\n",
        "      y_t = (S*q_t.unsqueeze(-1)).sum(dim=-2)\n",
        "      outs.append(y_t)\n",
        "\n",
        "    context = torch.stack(outs,dim=2).transpose(1,2).contiguous()\n",
        "    context = context.view(b,num_tokens, self.num_heads, self.head_dim)\n",
        "    #############################################################\n",
        "    ### NEW: Apply RMSNorm and SiLU gate\n",
        "    context = self.norm(context)\n",
        "    context = context.F.silu(gate)\n",
        "    #############################################################\n",
        "    context = context.view(b, num_tokens, self.d_out)\n",
        "    context = self.dropout(context)\n",
        "    out = self.out_proj(context)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#code for gated self attention. the objective of using gated self attention is gate decides how much output of the token to be kept depending upon importance\n",
        "#this ALLOWS MODEL TO SCALE UP AND DOWN THE FEATURES DYNAMICALLY\n",
        "import torch\n",
        "from pytorh import nn\n",
        "\n",
        "class GatedMultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in, d_out , dropout,num_heads, qkv_bias = False):\n",
        "    super().__init__()\n",
        "    assert d_out % num_heads == 0, 'd_out must be divisible by num_heads'\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "\n",
        "    sel.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    ###################################################\n",
        "    ## NEW Add gate\n",
        "    self.W_gate = nn.Linear(d_in, d_out, bias=False)\n",
        "    ###################################################\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length,context_length),diagonal =1),persistent = False,)\n",
        "\n",
        "  def forward(self,x):\n",
        "    b, num_tokens, _ = x.shape\n",
        "    queries = self.W_query(x)\n",
        "    ###################################################\n",
        "    ## New Add gate\n",
        "    gate = self.W_gate(x)\n",
        "    ###################################################\n",
        "    keys = self.W_key(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    keys = keys.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    attn_scores = queries@keys.transpose(2,3)\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "    attn_scores.masked_fill_(mask_bool, torch.finfo(attn_scores.dtype).min)\n",
        "    attn_weights = torch.softmax(attn_scores/(self.head_dim**0.5),dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context = (attn_weights@values).transpose(1,2)\n",
        "    context = context.reshape(b,num_tokens,self.d_out)\n",
        "\n",
        "    ##################################################\n",
        "    ### New: Add Gate\n",
        "    context = context *torch.sigmoid(gate)\n",
        "    ##################################################\n",
        "    out  = self.out_proj(context)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "hPC8mgr50Wcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GATED DELTA NET ATTENTION - THIS IS used bu Qwen 3.0\n",
        "# NO PAIR WISE ATTENTION LKE IN SELF ATTENTION OR GATED ATTENTION\n",
        "#ADVATAGE IS THIS SCALES LINEARLY UNLIKE ATTENTION WHICH SCALES QUADRATICALLY\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.functional as F\n",
        "\n",
        "def l2norm(x,dim=-1, eps = 1e-6):\n",
        "  return x *torch.rsqrt((x*x).sum(dim=dim,keepdim = True) + eps)\n",
        "\n",
        "class GatedDeltaNet(nn.Module):\n",
        "  def __init__(self,d_in, d_out , dropout,num_heads, qkv_bias = False):\n",
        "    super().__init__()\n",
        "    assert d_out % num_heads == 0, 'd_out must be divisible by num_heads'\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    ######################################################\n",
        "    ### New: Gates for delta rule and output gating\n",
        "    self.W_gate = nn.Linear(d_in, d_out, bias=False)\n",
        "    self.W_beta = nn.Linear(d_in, d_out, bias=False) #update gate - CONTROLS HOW STRONGLY NEW INPUTS MODIFY THE STATE\n",
        "    #Note: The decay gate alpha corresponds to\n",
        "    #A_log + W_alpha(x) = dt_bias\n",
        "\n",
        "    self.W_alpha = nn.Linear(d_in, num_heads, bias=False) # decay gate  - CONTROLS HOW FAST THE MEMORY DECAYS OR RESETS OVER TIME\n",
        "    self.dt_bias = nn.Parameter(torch.ones(num_heads))\n",
        "    self.A_log = nn.Parameter(torch.zeros(num_heads))\n",
        "    ######################################################\n",
        "    self.norm = nn.RMSNorm(self.head_dim,eps=1e-6)\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, _ = x.shape\n",
        "    queries = self.W_query(x)\n",
        "    keys = self.W_key(x)\n",
        "    values = self.W_value(x)\n",
        "    ######################################################\n",
        "    #NEW COMPUTE delta rule gates\n",
        "    beta = torch.sigmoid(self.W_beta(x))\n",
        "    alpha =-self.A_log.exp().view(1,1,-1) * F.softplus(self.W_alpha(x)+self.dt_bias)\n",
        "    gate = self.W_gate(x)\n",
        "    ######################################################\n",
        "\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    beta = beta.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    gate = gate.view(b, num_tokens, self.num_heads, self.head_dim) #NEW\n",
        "\n",
        "    keys = keys.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "    beta = beta.transpose(1,2)\n",
        "    gate = gate.transpose(1,2) #NEW\n",
        "\n",
        "    #######################################################\n",
        "    ### NEW QKNorm-like normalization for delta rule\n",
        "    queries = l2norm(queries,dim>1)/(self.head_dim**0.5)\n",
        "    keys = l2norm(keys,dim=-1)\n",
        "    #######################################################\n",
        "    S = x.new_zeros(b, self.num_heads, self.head_dim, self.head_dim)\n",
        "    outs =[]\n",
        "    #######################################################\n",
        "    ### New: Gated delta rule update\n",
        "    for t in range(num_tokens):\n",
        "      k_t = keys[:,:,t]\n",
        "      q_t = queries[:,:,t]\n",
        "      v_t = values[:,;,t]\n",
        "      beta_t = beta[:,:,t]\n",
        "      a_t = alpha[:,t].unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "      S = S*a_t.exp() #S IS STATE/MEMORY OF FIXED SIZE ? AND  THAT GETS UPDATED\n",
        "      kv_mem = (S*k_t.unsqueeze(-1)).sum(dim=-2)\n",
        "      delta = (v_t-kv_mem) *beta_t\n",
        "      S = S + k_t.unsqueeze(-1) * delta.unsqueeze(-2)\n",
        "      y_t = (S*q_t.unsqueeze(-1)).sum(dim=-2)\n",
        "      outs.append(y_t)\n",
        "    context = torch.stack(outs,dim=2).transpose(1,2).contiguous()\n",
        "    context = context.view(b,num_tokens, self.num_heads, self.head_dim)\n",
        "    #############################################################\n",
        "    ### NEW: Apply RMSNorm and SiLU gate\n",
        "    context = self.norm(context)\n",
        "    context = context.F.silu(gate)\n",
        "    #############################################################\n",
        "    context = context.view(b, num_tokens, self.d_out)\n",
        "    context = self.dropout(context)\n",
        "    out = self.out_proj(context)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "y-z9HCNtz0nb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}