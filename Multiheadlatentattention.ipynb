{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnPTtZBpxH5BfTF9JeFGl8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Multiheadlatentattention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREDIT - >https://www.youtube.com/watch?v=mIaWmJVrMpc&list=PLPTV0NXA_ZSiOpKKlHCyOq9lnp-dLvlms&index=13"
      ],
      "metadata": {
        "id": "khBprqJnSRfl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZeKaZ1oQnJE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Multiheadlatentattention(nn.Module):\n",
        "  def __init__(self,d_model,n_heads,kv_latent_dim):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model #input embedding dimension\n",
        "    self.n_heads = n_heads #no of attention heads means no of query heads !!\n",
        "    self.dh = d_model // n_heads #dimension per head\n",
        "\n",
        "    #Projection layers\n",
        "    self.W_q = nn.Linear(d_model, d_model, bias=False) #Query Projection\n",
        "    self.W_dkv = nn.Linear(d_model, kv_latent_dim , bias = False) #Compress into latent KV space\n",
        "    self.W_uk = nn.Linear(kv_latent_dim, d_model , bias = False) # Decompress K\n",
        "    self.W_uv = nn.Linear(kv_latent_dim, d_model , bias = False) # Decompress V\n",
        "    self.W_o = nn.Linear(d_model, d_model, bias = False) #Final Output projection\n",
        "\n",
        "    self.ln = nn.LayerNorm(kv_latent_dim)\n",
        "    self.register_buffer('absorbed_k', None) #Holds W_q @ W_uk\n",
        "\n",
        "  def forward(self,x,kv_cache=None,past_length=0):\n",
        "      B,S,D = x.size() # batch size , #no of tokens and size of embedding of the token\n",
        "      #compute absorbed_k once: W_q @ W_uk , shape:(D,latent_dim)\n",
        "      if self.absorbed_k is None:\n",
        "        absorbed = torch.matmul(self.W_q.weight, self.W_uk.weight) #(D , latend_dim)\n",
        "        self.absorbed_k = absorbed.view(self.n_heads,self.dh,-1) #(n_heads,dh,latent_dim)\n",
        "\n",
        "      #compress x into latent KV space\n",
        "      new_c_kv = self.ln(self.W_dkv(x)) #(B,S,latent_dim). does two things multiplying and doing layer normalization\n",
        "      if kv_cache is None:\n",
        "        c_kv = new_c_kv\n",
        "      else:\n",
        "        c_kv = torch.cat([kv_cache,new_c_kv],dim=1) #(B,S_total,latent_dim)\n",
        "\n",
        "      S_full = c_kv.size(1)\n",
        "      #decompress V to full d_model and split into heads\n",
        "      v_full = self.W_uv(c_kv) #(B,S_full,D)\n",
        "      v = v_full.view(B,S_full,self.n_heads,self.dh).transpose(1,2) #(B,S_full,n_heads,dh)\n",
        "\n",
        "      #Use input \"x\" directly (since W_q is absorbed)\n",
        "      q = x.view(B,S, self.n_heads, self.dh) #(B,n_heads,S,dh)\n",
        "\n",
        "      #Compute attention scores\n",
        "      attn_scores = torch.zeros(B,self.n_heads,S,S_full,device=x.device) #(B,S,n_heads,dh)\n",
        "      for h in range(self.n_heads): # attention heads\n",
        "        tmp = torch.matmul(q[:,:,h],self.absorbed_k[h])\n",
        "        attn_scores[:,h] = torch.bmm(tmp,c_kv.transpose(1,2)) #batch matrix multiplication\n",
        "\n",
        "      #Scale and apply causal mask\n",
        "      attn_scores = attn_scores / (self.dh**0.5)\n",
        "      mask = torch.tril(torch.ones((S,S_full),device = x.device), diagonal = past_length)\n",
        "      attn_scores = attn_scores.masked_fill(mask.view(1,1,S,S_full) == 0, float('-inf'))\n",
        "\n",
        "      #Softmax to get attention weights\n",
        "      attn_weights = F.softmax(attn_scores,dim=-1) #(B, n_heads ,S,S_full)\n",
        "\n",
        "      #Apply attention weights to each head's V separately\n",
        "      out_heads = []\n",
        "      for h in range(self.n_heads):\n",
        "        context_h = torch.matmul(attn_weights[:,h],v[:,h]) #(B,S,dh)\n",
        "        out_heads.append(context_h)\n",
        "\n",
        "      #concatenate all head outputs along the feature dimension\n",
        "      out = torch.cat(out_heads,dim=-1) #(B,S,D)\n",
        "      return self.W_o(out), c_kv #Final output projection + updated latent cache"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2:Memory testing\n",
        "def demo():\n",
        "  model = Multiheadlatentattention(d_model=512, n_heads=8, kv_latent_dim=256)\n",
        "  x = torch.randn(1,5,512) #Batch = 2, Sequence  = 10 , d_model = 512\n",
        "  out , cache = model(x)\n",
        "  print(f\"Output shape: {out.shape}\")\n",
        "  print(f\"Cache shape: {cache.shape}\")\n",
        "  #memory comparison\n",
        "  std_size = 2*2*10*512*4/1024 #KB\n",
        "  latent_size = 1*2*10*256*4/1024 #KB\n",
        "  print(f\"Memory: Standard = {std_size:.1f}KB , Latent = {latent_size:.1f}KB , Reduction = {std_size/latent_size:.1f}x\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  demo()\n"
      ],
      "metadata": {
        "id": "gL0fqHQvf1-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#01/02 - MLA\n",
        "#I am coding again\n",
        "#my understanding of MLA is:\n",
        "#Q = X * Wq  -> project  input embedding X into lower dimension space(latent space) my multiplying with lower dimension matrix\n",
        "#CKV = X*Wdv -> This is called latent matrix and is cached t0 reduce the KV cache size to -> l*b*s*h*c\n",
        "#K = Ck*Wuk -> where Wuk is the up projection matrix for K and that is fixed at training time\n",
        "#V = Ck*Wuv -> where Wuv is the up projection matrix for V and that is fixed at training time\n",
        "#absorbtion trick -> results to attention scores ->\n",
        "#absorb Q*KT = (X*Wq) * (X*Wdv*Wuk) = [X*(Wq*Wukt)] -> merging (Wq with WukT) and this matrix is fixed at training time. WdvT*XT will be cached as this latent matrix CKV\n",
        "#attention score = Softmax(absorption/dmodel)\n",
        "#Attention Weights /context vector = Attention scores *VT\n",
        "\n"
      ],
      "metadata": {
        "id": "YyNZXIoPSIpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RopelessMLA(nn.Module):\n",
        "  def __init__(self,d_model,n_heads,kv_latent_dim):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.dh = d_model // n_heads #dimension per head\n",
        "\n",
        "    #projection layers\n",
        "    self.W_q = nn.Linear(d_model,d_model,bias=False) # query projection\n",
        "    self.W_dkv = nn.Linear(d_model,kv_latent_dim,bias=False) # compress embedding into lower latent space\n",
        "    self.W_uk = nn.Linear(kv_latent_dim,d_model,bias=False) # decompress uk matrix\n",
        "    self.W_uv = nn.Linear(kv_latent_dim,d_model,bias=False) #decompress uv matrix\n",
        "    self.W_o = nn.Linear(d_model,d_model,bias=False) # final output projection - context vector\n",
        "\n",
        "    self.ln = nn.LayerNorm(kv_latent_dim)\n",
        "    self.register_buffer('absorbed_k',None) # holds W_q @ W_uk - buffer is used for holding\n",
        "\n",
        "    def forward(self,x,kv_cache = None,past_length=0):\n",
        "      B,S,D = x.size() #batch , sequence , dimension\n",
        "      if self.absorbed_k is None:\n",
        "        absorbed = torch.matmul(self.W_q.weight,self.W_uk.weight)\n",
        "        self.absorbed_k = absorbed.view(self.n_heads,self.dh,-1)\n",
        "      c_kv = self.ln(self.W_dkv(x)) #latent matrix\n",
        "      if kv_cache is None:\n",
        "        c_kv = c_kv\n",
        "      else:\n",
        "        c_kv = torch.cat([kv_cache,c_kv],dim=1)\n",
        "      #calculate attention scores\n",
        "      S_full = c_kv.size(1)\n",
        "      v_full = self.W_uv(c_kv)\n",
        "      v = v_full.view(B,S_full,self.n_heads,self.dh).transpose(1,2)\n",
        "\n",
        "      q = x.view(B,S,self.n_heads,self.dh)\n",
        "      attn_scores = torch.zeros(B,self.n_heads,S,S_full,device=x.device)\n",
        "      for h in range(self.n_heads):\n",
        "        tmp = torch.matmul(q[:,:,h],self.absorbed_k[h])\n",
        "        attn_scores[:,h] = torch.bmm(tmp,c_kv.transpose(1,2))\n",
        "\n",
        "      attn_scores = attn_scores / (self.dh**0.5)\n",
        "      mask = torch.tril(torch.ones((S,S_full),device=x.device),diagonal=past_length) # causal mask\n",
        "      attn_scores = attn_scores.masked_fill(mask.view(1,1,S,S_full)==0,float('-inf'))\n",
        "      attn_weights = F.softmax(attn_scores,dim=-1)\n",
        "\n",
        "      #calculate final context vectors\n",
        "      out_heads = []\n",
        "      for h in range(self.n_heads):\n",
        "        context_h = torch.matmul(attn_weights[:,h],v[:,h])\n",
        "        out_heads.append(context_h)\n",
        "\n",
        "      #concatenate all vectors\n",
        "      out = torch.cat(out_heads,dim=-1)\n",
        "      return self.W_o(out),c_kv"
      ],
      "metadata": {
        "id": "MMRG93R3j9h1"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}