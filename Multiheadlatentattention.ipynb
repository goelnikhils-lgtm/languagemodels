{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcfOzziv5d8RCnh6NJEgtR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Multiheadlatentattention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gZeKaZ1oQnJE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Multiheadlatentattention(nn.Module):\n",
        "  def __init__(self,d_model,n_heads,kv_latent_dim):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model #input embedding dimension\n",
        "    self.n_heads = n_heads #no of attention heads\n",
        "    self.dh = d_model // n_heads #dimension per head\n",
        "\n",
        "    #Projection layers\n",
        "    self.W_q = nn.Linear(d_model, d_model, bias=False) #Query Projection\n",
        "    self.W_dkv = nn.Linear(d_model, kv_latent_dim , bias = False) #Compress into latent KV space\n",
        "    self.W_uk = nn.Linear(kv_latent_dim, d_model , bias = False) # Decompress K\n",
        "    self.W_uv = nn.Linear(kv_latent_dim, d_model , bias = False) # Decompress V\n",
        "    self.W_o = nn.Linear(d_model, d_model, bias = False) #Final Output projection\n",
        "\n",
        "    self.ln = nn.LayerNorm(kv_latent_dim)\n",
        "    self.register_buffer('absorbed_k', None) #Holds W_q @ W_uk\n",
        "\n",
        "  def forward(self,x,kv_cache=None,past_length=0):\n",
        "      B,S,D = x.size() # batch size , #no of tokens and size of embedding of the token\n",
        "      #compute absorbed_k once: W_q @ W_uk , shape:(D,latent_dim)\n",
        "      if self.absorbed_k is None:\n",
        "        absorbed = torch.matmul(self.W_q.weight, self.W_uk.weight) #(D , latend_dim)\n",
        "        self.absorbed_k = absorbed.view(self.n_heads,self.dh,-1) #(n_heads,dh,latent_dim)\n",
        "\n",
        "      #compress x into latent KV space\n",
        "      new_c_kv = self.ln(self.W_dkv(x)) #(B,S,latent_dim). does two things multiplying and doing layer normalization\n",
        "      if kv_cache is None:\n",
        "        c_kv = new_c_kv\n",
        "      else:\n",
        "        c_kv = torch.cat([kv_cache,new_c_kv],dim=1) #(B,S_total,latent_dim)\n",
        "\n",
        "      S_full = c_kv.size(1)\n",
        "      #decompress V to full d_model and split into heads\n",
        "      v_full = self.W_uv(c_kv) #(B,S_full,D)\n",
        "      v = v_full.view(B,S_full,self.n_heads,self.dh).transpose(1,2) #(B,S_full,n_heads,dh)\n",
        "\n",
        "      #Use input X directly (since W_q is absorbed)\n",
        "      q = x.view(B,S, self.n_heads, self.dh) #(B,n_heads,S,dh)\n",
        "\n",
        "      #Compute attention scores\n",
        "      attn_scores = torch.zeros(B,self.n_heads,S,S_full,device=x.device) #(B,S,n_heads,dh)\n",
        "      for h in range(self.n_heads): # attention heads\n",
        "        tmp = torch.matmul(q[:,:,h],self.absorbed_k[h])\n",
        "        attn_scores[:,h] = torch.bmm(tmp,c_kv.transpose(1,2)) #batch matrix multiplication\n",
        "\n",
        "      #Scale and apply causal mask\n",
        "      attn_scores = attn_scores / (self.dh**0.5)\n",
        "      mask = torch.tril(torch.ones((S,S_full),device = x.device), diagonal = past_length)\n",
        "      attn_scores = attn_scores.masked_fill(mask.view(1,1,S,S_full) == 0, float('-inf'))\n",
        "\n",
        "      #Softmax to get attention weights\n",
        "      attn_weights = F.softmax(attn_scores,dim=-1) #(B, n_heads ,S,S_full)\n",
        "\n",
        "      #Apply attention weights to each head's V separately\n",
        "      out_heads = []\n",
        "      for h in range(self.n_heads):\n",
        "        context_h = torch.matmul(attn_weights[:,h],v[:,h]) #(B,S,dh)\n",
        "        out_heads.append(context_h)\n",
        "\n",
        "      #concatenate all head outputs along the feature dimension\n",
        "      out = torch.cat(out_heads,dim=-1) #(B,S,D)\n",
        "      return self.W_o(out), c_kv #Final output projection + updated latent cache"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2:Memory testing\n",
        "def demo():\n",
        "  model = Multiheadlatentattention(d_model=512, n_heads=8, kv_latent_dim=256)\n",
        "  x = torch.randn(1,5,512) #Batch = 2, Sequence  = 10 , d_model = 512\n",
        "  out , cache = model(x)\n",
        "  print(f\"Output shape: {out.shape}\")\n",
        "  print(f\"Cache shape: {cache.shape}\")\n",
        "  #memory comparison\n",
        "  std_size = 2*2*10*512*4/1024 #KB\n",
        "  latent_size = 1*2*10*256*4/1024 #KB\n",
        "  print(f\"Memory: Standard = {std_size:.1f}KB , Latent = {latent_size:.1f}KB , Reduction = {std_size/latent_size:.1f}x\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  demo()\n"
      ],
      "metadata": {
        "id": "gL0fqHQvf1-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d22f6b-abf9-464d-b879-a83fbefb34e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 5, 512])\n",
            "Cache shape: torch.Size([1, 5, 256])\n",
            "Memory: Standard = 80.0KB , Latent = 20.0KB , Reduction = 4.0x\n"
          ]
        }
      ]
    }
  ]
}