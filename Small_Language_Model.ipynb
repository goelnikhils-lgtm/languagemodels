{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPFz/uFpcS5fqJ1LrzYuhlp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Small_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFmPnUAAoZ0J"
      },
      "outputs": [],
      "source": [
        "#developing Small Language Model from Scratch\n",
        "#scaling laws as the no of parameters of the model increase . Model performance increases as we increase model parameters\n",
        "#https://www.youtube.com/watch?v=pOFcwcwtv3k&t=24s"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1\n",
        "#Import the dataset\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "dy0zYaCwt5e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"roneneldan/TinyStories\")"
      ],
      "metadata": {
        "id": "-vMiguz7uH03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2\n",
        "#Data preprocessing for training a LLM\n",
        "#Tokenize the dataset\n",
        "#Tokenize the dataset into tokenIDs\n",
        "#Create a file called \"train.bin\" and \"validation.bin\" where we store all the tokenIDs from the entire dataset\n",
        "#Ensure that tokenIDs are stored on a disk , rather than on the RAM for efficient computations\n",
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\") #bpe tokenizer\n",
        "def process(example):\n",
        "  ids = enc.encode_ordinary(example[\"text\"])\n",
        "  out = {'ids':ids , 'len': len(ids)}\n",
        "  return out\n",
        "\n",
        "if not os.path.exists(\"train.bin\"):\n",
        "  tokenized = dataset.map(process, remove_columns= ['text'],desc=\"tokenizing the splits\",num_proc=8)\n",
        "#concatenate all the ids in each dataset into one large file for training\n",
        "for split , dset in tokenized.items():\n",
        "  arr_len = np.sum(dset['len'], dtype = np.uint64)\n",
        "  filename = f\"{split}.bin\"\n",
        "  dtype = np.uint16 #using 2 bytes\n",
        "  arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,)) #memory mapped array\n",
        "  total_batches = 1024\n",
        "  idx = 0\n",
        "  for batch_idx in tqdm(range(total_batches), desc=f\"writing {filename}\"):\n",
        "    batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True)\n",
        "    arr_batch = np.concatenate(batch['ids'])\n",
        "    #write into map\n",
        "    arr[idx : idx + len(arr_batch)] = arr_batch\n",
        "    idx += len(arr_batch)\n",
        "  arr.flush()\n",
        "  #one word is not one token"
      ],
      "metadata": {
        "id": "1MD4kwCiuwmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 3: create input-output batches from dataset\n",
        "def get_batch(split):\n",
        "  #create np.memap every batch to avoid a memory leak as per stackflow\n",
        "  if split == 'train':\n",
        "    data = np.memap('train.bin' , dtype = np.uint16 , mode = 'r')\n",
        "  else:\n",
        "    data = np.memap('validation.bin' , dtype = np.uint16 , mode = 'r')\n",
        "  ix = torch.randint(len(data) - block_size , (batch_size,)) #batch size = 4\n",
        "  x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix]) #block_size is the context size\n",
        "  y = torch.stack([torch.from_numpy((data[i+1:i+1+ block_size]).astype(np.int64)) for i in ix]) #target shifted by 1 token for prediction\n",
        "  if device_type  == 'cuda':\n",
        "    x,y = x.pin_memory().to(device , non_blocking=True) , y.pin_memory().to(device , non_blocking=True)\n",
        "  else:\n",
        "    x,y = x.to(device) , y.to(device)\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "XIVHYOC09YH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 4: define the SLM model architecture\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from contextlib import nullcontext\n",
        "import os\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, ndim, bias):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(torch.ones(ndim))\n",
        "    self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "  def forward(self, x):\n",
        "    return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "    self.c_attn = nn.Linear(config.n_embd , 3 * config.n_embd , bias = False)\n",
        "    self.c_proj = nn.Linear(config.n_embd , config.n_embd , bias = False)\n",
        "    self.attn_dropout = nn.Dropout(config.dropout)\n",
        "    self.resid_dropout = nn.Dropout(config.dropout)\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.flash = hasattr(F,'scaled_dot_product_attention')\n",
        "    if not self.flash:\n",
        "      self.register_buffer('bias',torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size()\n",
        "    q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    v = v.view(B, T, self.n_head, C // self.n_head).transpose\n",
        "    if self.flash:\n",
        "      y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout, is_causal=True)\n",
        "    else:\n",
        "      att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "      att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "      att = F.softmax(att, dim=-1)\n",
        "      att = self.attn_dropout(att)\n",
        "      y = att @ v\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "    y = self.resid_dropout(self.c_proj(y))\n",
        "    return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False) #expansion\n",
        "    self.gelu = nn.GELU()\n",
        "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=False) #contraction\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "  def forward(self,x):\n",
        "    return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
        "\n",
        "class Block(nn.Module): #transformer block\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = LayerNorm(config.n_embd, bias=config.bias) #layer normalization layer\n",
        "    self.attn = CausalSelfAttention(config) #casusal attention\n",
        "    self.ln_2 = LayerNorm(config.n_embd, bias=config.bias) #layer normalization layer\n",
        "    self.mlp = MLP(config) #config layer\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x)) #shortcut connection\n",
        "    x = x + self.mlp(self.ln_2(x)) #shortcut connection\n",
        "    return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "  block_size: int\n",
        "  vocab_size: int\n",
        "  n_layer: int\n",
        "  n_head: int\n",
        "  n_embd: int\n",
        "  dropout: float = 0.0\n",
        "  bias: bool = True\n",
        "\n",
        "class GPT(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.transformer = nn.ModuleDict(dict(\n",
        "      wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "      wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "      drop = nn.Dropout(config.dropout),\n",
        "      h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "      ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "    ))\n",
        "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) #output head logits....\n",
        "    self.transformer.wte.weight = self.lm_head.weight #weight tying\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "    for pn,p in self.named_parameters():\n",
        "      if pn.endswith('c_proj.weight'):\n",
        "        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer)) #parameter intialization using Normal Distribution\n",
        "\n",
        "  def __init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "    device = idx.device\n",
        "    b, t = idx.size()\n",
        "    assert t <= self.config.block_size, \"context length exceeded\"\n",
        "    pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "    tok_emb = self.transformer.wte(idx)\n",
        "    pos_embd = self.transformer.wpe(pos)\n",
        "    x = self.transformer.drop(tok_emb + pos_embd)\n",
        "    for block in self.transformer.h:\n",
        "      x = block(x)\n",
        "    x = self.transformer.ln_f(x)\n",
        "    if targets is not None:\n",
        "      logits = self.lm_head(x)\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "      return logits, loss\n",
        "    else:\n",
        "      logits = self.lm_head(x[:,[-1],:])\n",
        "      return logits, None\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None): #top_k - decoding strategy or top-p / nucleus sampling , temperature\n",
        "    \"\"\"\n",
        "    generate tokens given a conditioning sequence.\n",
        "    idx:Tensor of shape(B,T)\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "      logits, _ = self(idx_cond)\n",
        "      logits = logits[:, -1, :] / temperature\n",
        "      if top_k is not None:\n",
        "        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "        logits[logits < v[:, [-1]]]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "d93eG5no_AXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config =  GPTConfig(\n",
        "  vocab_size = 50257, #use the tokenizer's vocab size\n",
        "  block_size = 128,   # or whatever context size\n",
        "  n_layer= 6, #no of transformer blocks\n",
        "  n_head= 6, # no of attention heads - MHA\n",
        "  n_embd = 384,\n",
        "  dropout =  0.1,\n",
        "  bias = True\n",
        "  )\n",
        "gpt = GPT(config)"
      ],
      "metadata": {
        "id": "nwEQy0sSc-lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5: Define the loss function\n",
        "def estimate_loss(model):\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for split in ['train', 'val']:\n",
        "      losses = torch.zeros(eval_iters)\n",
        "      for k in range(eval_iters):\n",
        "        X, Y = get_batch(split)\n",
        "        with ctx:\n",
        "          logits, loss = model(X, Y)\n",
        "          losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "giNl9OMifL4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6 define the SLM training Loop\n",
        "#during training use Automate Mixed Precision - converts into float16 and fall back to float32  where needed - this helps in faster training and this is production grade\n",
        "#for matrix multiplication float 16 is fine to be used.... this is done during matrix multiplication\n",
        "max_iters = 20000\n",
        "warm_up_steps = 1000 #smoother intial train , earlier 100\n",
        "min_lr = 5e-4\n",
        "eval_iters = 500\n",
        "batch_size = 32\n",
        "block_size = 128\n",
        "gradient_accumulation_steps = 32 # accumlating gradients till step 32 and then back prop and then updating the parameters\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' #for later use in torch.autocast\n",
        "#note: float16 data type will automatically use a GradScaler\n",
        "\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "torch.set_default_device(device)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "rqsTKaOh6uVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 7: define SLM Training Configuration Part 2\n",
        "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
        "#put in weight decay , changed beta2 to 0.95\n",
        "#adam is adaptive learning rate .... learning rate is updated every time parameters are updated\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1) #use adamw so that we don't get stuck in local minima and training is better\n",
        "scheduler_warmup = LinearLR(optimizer, start_factor=0.0001, end_factor=1, total_iters=warm_up_steps)\n",
        "scheduler_decay = CosineAnnealingLR(optimizer, T_max=max_iters-warm_up_steps, eta_min=min_lr)\n",
        "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warm_up_steps]) # learning rate is a combination of warm up and decay like cosine function. Rationle is it leads to better training\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ],
      "metadata": {
        "id": "EziUMCjkBu7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 8: Pre-Train SLM\n",
        "best_val_loss = float('inf')\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "train_loss_list , validation_loss_list = [] , []\n",
        "\n",
        "#ensure model is on correct device\n",
        "model = model.to(device)\n",
        "\n",
        "#in your training loop\n",
        "for epoch in tqdm(range(max_iters)):\n",
        "  if epoch % eval_iters == 0 and epoch !=0:\n",
        "    losses = estimate_loss(model)\n",
        "    print(f\"step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    print(f\"the current learning rate:{optimizer.param_groups[0]['lr']:.5f}\")\n",
        "    train_loss_list+= [losses['train']]\n",
        "    validation_loss_list+= [losses['val']]\n",
        "    if losses['val'] < best_val_loss:\n",
        "      best_val_loss = losses['val']\n",
        "      torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "    #ensure X and y are on the correct device\n",
        "    X,y = get_batch(\"train\")\n",
        "    X,y = X.to(device), y.to(device)\n",
        "    with ctx:\n",
        "      logits, loss = model(X, y)\n",
        "      loss = loss/gradient_accumulation_steps\n",
        "      scaler.scale(loss).backward()\n",
        "    if ((epoch + 1) % gradient_accumulation_steps) == 0 or (epoch+1== max_iters):\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update() # update the parameters\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "PuRj2QBdFEZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 9: Plot the SLM Loss Function\n"
      ],
      "metadata": {
        "id": "WcNEdaE0NCXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 10 model inference\n",
        "#load the model\n",
        "model = GPT(config)\n",
        "device = \"cuda\" if torch.cuda is available() else \"cpu\"\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "model.load_state_dict(torch.load(best_ model_params_path, map_location=torch.device(device))) # load best model states\n",
        "\n",
        "sentence = \"Once upon a time there was a pumpkin.\"\n",
        "context = (torch.tensor (enc.encode_ordinary(sentence)).unsqueezze(dim=0))\n",
        "y = model.generate(context,200)\n",
        "print(enc.decode(y.squeeze().tolist()))"
      ],
      "metadata": {
        "id": "nh_gmnlANzbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.linear = nn.Linear(10, 1)\n",
        "        self.bn = nn.BatchNorm1d(10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(self.bn(x))\n",
        "\n",
        "model = MyModel()\n",
        "state_dict = model.state_dict()\n",
        "\n",
        "print(state_dict.keys())\n",
        "# Example output: odict_keys(['linear.weight', 'linear.bias', 'bn.weight', 'bn.bias', 'bn.running_mean', 'bn.running_var', 'bn.num_batches_tracked'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywlJj6PwTmgv",
        "outputId": "5cb5a58c-220f-4697-e583-73aefa1ac3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['linear.weight', 'linear.bias', 'bn.weight', 'bn.bias', 'bn.running_mean', 'bn.running_var', 'bn.num_batches_tracked'])\n"
          ]
        }
      ]
    }
  ]
}