{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOg1jXiOqAaqWfV+Gtb87w3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/master/llama3_2fromscratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2t-nv71cB7L4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from typing import Optional, Tuple, Union, Callable, Any, Dict\n",
        "\n",
        "# Configuration dictionary for LLaMA 3.2B model\n",
        "LLAMA32_CONFIG_1B = {\n",
        "    \"vocab_size\": 128_256, #vocab size of llama 3.2\n",
        "    \"context_length\": 131_072, #context length that was used to train the model\n",
        "    \"embedding_dim\": 2048, #embedding dimension\n",
        "    \"hidden_dim\": 8192, #size of intermediate dimension in feedforward layer\n",
        "    \"num_layers\": 16, #number of transformer layers\n",
        "    \"num_key_value_heads\": 8, #number of key-value heads / group for GQA\n",
        "    \"num_heads\": 16, #number of attention heads\n",
        "    \"rope_base\": 500_000.0, # the base in RoPE's \"theta\"\n",
        "    \"dtype\": torch.bfloat16, #data type\n",
        "    \"rope_freq\":{\n",
        "        \"factor\": 32.0,#frequency factor for RoPE computation\n",
        "        \"low_freq_factor\": 1.0, #low frequency for RoPE computation\n",
        "        \"high_freq_factor\": 4.0, #high frequency for RoPE computation\n",
        "        \"original_context_length\": 8192, #original context length used during training\n",
        "        }\n",
        "}\n",
        "LLAMA32_CONFIG_3B = {\n",
        "    \"vocab_size\": 128_256, #vocab size of llama 3.2\n",
        "    \"context_length\": 131_072, #context length that was used to train the model\n",
        "    \"embedding_dim\": 3072, #embedding dimension\n",
        "    \"hidden_dim\": 8192, #size of intermediate dimension in feedforward layer\n",
        "    \"num_layers\": 28, #number of transformer layers\n",
        "    \"num_key_value_heads\": 8, #number of key-value heads / group for GQA\n",
        "    \"num_heads\": 24, #number of attention heads\n",
        "    \"rope_base\": 500_000.0, # the base in RoPE's \"theta\"\n",
        "    \"dtype\": torch.bfloat16, #data type\n",
        "    \"rope_freq\":{\n",
        "        \"factor\": 32.0, #frequency factor for RoPE computation\n",
        "        \"low_freq_factor\": 1.0, #low frequency for RoPE computation\n",
        "        \"high_freq_factor\": 4.0, #high frequency for RoPE computation\n",
        "        \"original_context_length\": 8192, #original context length used during training\n",
        "    }\n",
        "}\n",
        "class Llama3Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Token embedding layer\n",
        "        self.token_embedding = nn.Embedding(config[\"vocab_size\"], config[\"embedding_dim\"], dtype=config[\"dtype\"])\n",
        "        self.trf_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(config) for _ in range(config[\"num_layers\"])]\n",
        "        )\n",
        "\n",
        "        # Final layer normalization\n",
        "        self.final_layer_norm = nn.RMSNorm(config[\"embedding_dim\"], eps=1e-6,dtype=config[\"dtype\"])\n",
        "\n",
        "        # Output projection layer\n",
        "        self.output_projection = nn.Linear(config[\"embedding_dim\"], config[\"vocab_size\"], bias=False,dtype=config[\"dtype\"])\n",
        "\n",
        "        #reusable utilities\n",
        "        cos,sin = compute_rope_parameters(\n",
        "            head_dim = config[\"embedding_dim\"] // config[\"num_heads\"],\n",
        "            theta_base = config[\"rope_base\"],\n",
        "            max_seq_len = config[\"context_length\"],\n",
        "            freq_config = config[\"rope_freq\"]\n",
        "            )\n",
        "        self.register_buffer(\"cos\", cos)\n",
        "        self.register_buffer(\"sin\", sin)\n",
        "        self.config = config\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        \"\"\"\n",
        "        Forward pass of the LLaMA 3.2 model.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Input token IDs of shape (batch_size, sequence_length).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Logits of shape (batch_size, sequence_length, vocab_size).\n",
        "        \"\"\"\n",
        "        token_embeds = self.token_embedding(in_idx)  # (batch_size, seq_length, embedding_dim)\n",
        "        x = token_embeds\n",
        "        num_tokens = x.shape[1]\n",
        "        mask = torch.triu(torch.ones((num_tokens, num_tokens), device=x.device), diagonal=1).bool()\n",
        "        for block in self.trf_blocks:\n",
        "            x = block(x, mask=mask, cos=self.cos, sin=self.sin)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.output_projection(x.to(self.config[\"dtype\"]))  # (batch_size, seq_length, vocab_size)\n",
        "        return logits\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = GroupedQueryAttention(\n",
        "            d_in=config[\"embedding_dim\"],\n",
        "            d_out=config[\"embedding_dim\"],\n",
        "            num_heads=config[\"num_heads\"],\n",
        "            num_key_value_heads=config[\"num_key_value_heads\"],\n",
        "            dtype=config[\"dtype\"]\n",
        "        )\n",
        "        self.feed_forward = FeedForward(config)\n",
        "        self.layer_norm1 = nn.RMSNorm(config[\"embedding_dim\"], eps=1e-6,dtype=config[\"dtype\"])\n",
        "        self.layer_norm2 = nn.RMSNorm(config[\"embedding_dim\"], eps=1e-6,dtype=config[\"dtype\"])\n",
        "\n",
        "    def forward(self, x, mask, cos, sin):\n",
        "        #shortcut connection for attention block / skip connection for gradients to flow\n",
        "        shortcut = x\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.attention(x,mask=mask, cos=cos, sin=sin)\n",
        "        x = x + shortcut #Add the input original input block\n",
        "\n",
        "        #shortcut connection for feedforward block / skip connection for gradients to flow\n",
        "        shortcut = x\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x = x + shortcut #Add the input original input block\n",
        "        return x\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(config[\"embedding_dim\"], config[\"hidden_dim\"],dtype=config[\"dtype\"], bias = False)\n",
        "        self.fc2 = nn.Linear(config[\"hidden_dim\"], config[\"embedding_dim\"],dtype=config[\"dtype\"], bias = False)\n",
        "        self.fc3 = nn.Linear(config[\"embedding_dim\"], config[\"hidden_dim\"],dtype=config[\"dtype\"], bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_fc1 = self.fc1(x)\n",
        "        x_fc2 = self.fc2(x)\n",
        "        x = nn.functional.silu(x_fc1) * x_fc2\n",
        "        return self.fc3(x)\n",
        "\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, num_heads, num_key_value_heads, dtype=None):\n",
        "        super().__init__()\n",
        "        assert d_in % num_heads == 0, \"d_in must be divisible by num_heads\"\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "        assert num_heads % num_key_value_heads == 0, \"num_heads must be divisible by num_key_value_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_key = nn.Linear(d_in,num_key_value_heads * self.head_dim, bias=False,dtype=dtype)\n",
        "        self.W_value = nn.Linear(d_in, num_key_value_heads * self.head_dim, bias=False,dtype=dtype)\n",
        "        self.num_key_value_heads = num_key_value_heads\n",
        "        self.group_size = num_heads // num_key_value_heads #Group size for GQA\n",
        "        # Query projection layer\n",
        "\n",
        "        self.W_Q = nn.Linear(d_in, d_out, bias=False,dtype=dtype)\n",
        "        # Output projection layer\n",
        "        self.o_proj = nn.Linear(d_out, d_out, bias=False,dtype=dtype)\n",
        "\n",
        "    def forward(self, x, mask=None, cos=None, sin=None):\n",
        "        b, num_tokens , d_in = x.shape\n",
        "\n",
        "        queries = self.W_Q(x) #shape (batch_size, num_tokens, d_out)\n",
        "        key = self.W_key(x) #shape (batch_size, num_tokens, num_key_value_heads * head_dim)\n",
        "        value = self.W_value(x) #shape (batch_size, num_tokens, num_key_value_heads * head_dim)\n",
        "\n",
        "        # Reshape queries , key and values\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, num_tokens, head_dim)\n",
        "        key = key.view(b, num_tokens, self.num_key_value_heads, self.head_dim ).transpose(1, 2)  # (batch_size, num_key_value_heads, num_tokens, head_dim)\n",
        "        value = value.view(b, num_tokens, self.num_key_value_heads, self.head_dim ).transpose(1, 2)  # (batch_size, num_key_value_heads, num_tokens, head_dim)\n",
        "\n",
        "        # Apply RoPE to queries and keys\n",
        "        keys = apply_rope(key, cos, sin)\n",
        "        queries = apply_rope(queries, cos, sin)\n",
        "\n",
        "        #expand keys and values to match the number of heads\n",
        "        #shape:(b,num_heads,num_tokens,head_dim)\n",
        "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
        "        values = value.repeat_interleave(self.group_size, dim=1)\n",
        "        # why interleave -> to repeat each key-value head group_size times to match the number of query heads\n",
        "        # This allows each query head to attend to all key-value pairs in its group\n",
        "        # without introducing additional parameters or complexity.\n",
        "        #for example if num_heads = 16 and num_key_value_heads = 8 then group_size = 2\n",
        "        # so each key-value head will be repeated twice to match the number of query heads\n",
        "\n",
        "        #compute scaled dot product attention(aka self attention) with a causal mask\n",
        "        #shape:(b,num_heads,num_tokens,head_dim) @ (b,num_heads,head_dim,num_tokens) -> (b,num_heads,num_tokens,num_tokens)\n",
        "        attn_scores = queries @ keys.transpose(2, 3) #dot product for each head\n",
        "\n",
        "        #use the mask to set the upper triangular part of the attention scores to -inf\n",
        "        attn_scores = attn_scores.masked_fill(mask[:num_tokens, :num_tokens], -torch.inf)\n",
        "        attn_weights = F.softmax(attn_scores / math.sqrt(self.head_dim), dim=-1) #softmax along the last dimension\n",
        "        assert keys.shape[-1] == self.head_dim\n",
        "\n",
        "        #shape:(b,num_heads,num_tokens,num_tokens) @ (b,num_heads,num_tokens,head_dim) -> (b,num_heads,num_tokens,head_dim)\n",
        "        context_vectors = (attn_weights @ values).transpose(1, 2) #weighted sum of values for each head\n",
        "        #combine heads , where self.d_out = num_heads * head_dim\n",
        "        context_vectors = context_vectors.reshape(b, num_tokens, self.d_out) # (batch_size, num_tokens, d_out)\n",
        "        context_vectors = self.o_proj(context_vectors) #final linear projection\n",
        "        return context_vectors\n",
        "def compute_rope_parameters(head_dim, theta_base=10_000, max_seq_len=4096, freq_config= None, dtype = torch.float32):\n",
        "    \"\"\" Compute the cosine and sine matrices for RoPE.\n",
        "\n",
        "    Args:\n",
        "        head_dim (int): Dimension of each attention head.\n",
        "        theta_base (float): Base value for computing the frequencies.\n",
        "        max_seq_len (int): Maximum sequence length.\n",
        "        freq_config (dict, optional): Configuration for frequency scaling.\n",
        "        dtype (torch.dtype, optional): Data type of the output tensors.\n",
        "        \"\"\"\n",
        "    assert head_dim % 2 == 0, \"head_dim must be even for RoPE\"\n",
        "    #compute the inverse frequencies\n",
        "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[:(head_dim // 2)].float() / head_dim))\n",
        "    #compute the frequencies for each position\n",
        "    if freq_config is not None:\n",
        "        low_freq_factor = freq_config[\"original_context_length\"] / freq_config[\"low_freq_factor\"]\n",
        "        high_freq_factor = freq_config[\"original_context_length\"] / freq_config[\"high_freq_factor\"]\n",
        "        wavelen = 2*torch.pi / inv_freq\n",
        "\n",
        "        inv_freq_llama = torch.where(wavelen>low_freq_factor,inv_freq/freq_config[\"factor\"],inv_freq)\n",
        "        smooth_factor = (freq_config[\"original_context_length\"] /wavelen- freq_config[\"low_freq_factor\"] ) / (freq_config[\"high_freq_factor\"] - freq_config[\"low_freq_factor\"])\n",
        "        smooth_inv_freq = ((1-smooth_factor)*(inv_freq/freq_config[\"factor\"]) + smooth_factor *inv_freq)\n",
        "\n",
        "        is_medium_freq = (wavelen <= low_freq_factor) & (wavelen >= high_freq_factor)\n",
        "        is_high_freq = torch.where(is_medium_freq,smooth_inv_freq,inv_freq_llama)\n",
        "        inv_freq =inv_freq_llama\n",
        "    #generate position indices\n",
        "    positions = torch.arange(max_seq_len, dtype=dtype)\n",
        "    #compute the angles\n",
        "    angles = positions[:, None] * inv_freq[None, :]  # (max_seq_len, head_dim // 2)\n",
        "    #expand the angles to match the head_dim\n",
        "    angles = torch.cat([angles, angles], dim=-1)  # (max_seq_len, head_dim)\n",
        "\n",
        "    #precompute the cosine and sine matrices\n",
        "    cos = torch.cos(angles)  # (max_seq_len, head_dim)\n",
        "    sin = torch.sin(angles)  # (max_seq_len, head_dim)\n",
        "    return cos, sin\n",
        "\n",
        "def apply_rope(x, cos, sin):\n",
        "    \"\"\" Apply RoPE to the input tensor.\"\"\"\n",
        "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
        "    assert head_dim%2 ==0 , \"head_dim must be even for RoPE\"\n",
        "\n",
        "    #split x into first half and second half\n",
        "    x1, x2 = x[..., :head_dim // 2], x[..., head_dim // 2:]\n",
        "\n",
        "    #adjust sin and cos shapes\n",
        "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, head_dim)\n",
        "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, head_dim)\n",
        "    #apply the rotatory transformation\n",
        "    rotated = torch.cat((-x2, x1), dim=-1)\n",
        "    x_rotated = (x * cos) + (rotated * sin)\n",
        "\n",
        "    #it's ok to use lower-precision after applying cos and sin rotation\n",
        "    return x_rotated.to(dtype = x.dtype)\n",
        "\n",
        "def generate(model , idx,max_new_tokens, context_size , temperature=0.0, top_k=None, eos_id = None):\n",
        "    \"\"\" Generate text using the LLaMA 3.2 model.\n",
        "\n",
        "    Args:\n",
        "        model (Llama3Model): The LLaMA 3.2 model.\n",
        "        idx (torch.Tensor): Input token IDs of shape (batch_size, sequence_length).\n",
        "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
        "        temperature (float): Sampling temperature.\n",
        "        top_k (int, optional): If specified, use top-k sampling.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Generated token IDs of shape (batch_size, sequence_length + max_new_tokens).\n",
        "    \"\"\"\n",
        "    #for loop is the same as before: Get logits , and only focus on the last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx if idx.size(1) <= context_size else idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        #filter logits for top-k sampling\n",
        "        if top_k is not None:\n",
        "            #keep only the top-k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
        "\n",
        "        #apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            #apply the softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (batch_size, context_length)\n",
        "            #sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "        #otherwise same as before: get idx of the vocab entry with the highest logit value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "        #stop generating early if eos token is encountered and eos id is specified\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "        #same as before : append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, sequence_length + 1)\n",
        "    return idx\n"
      ]
    }
  ]
}