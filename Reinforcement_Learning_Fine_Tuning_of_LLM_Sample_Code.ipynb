{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM84gtuffU4CEvjxMiPP3iB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Reinforcement_Learning_Fine_Tuning_of_LLM_Sample_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HCXBIyWwu-t"
      },
      "outputs": [],
      "source": [
        "#RLHF based fine tuning - sample code but not working code\n",
        "#credit - https://medium.com/@meeran03/fine-tuning-llms-with-human-feedback-rlhf-latest-techniques-and-best-practices-3ed534cf9828\n",
        "#credit - https://www.youtube.com/watch?v=R2paulc3P2M"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl\n",
        "\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from transformers import AutoTokenizer, pipeline , AutoModelForSequenceClassification\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "tZVFM9zKw_NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"lvwerra/gpt-imdb\" # base model\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#initiate a reward model\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\"lvwerra/distilbert-imdb\")\n",
        "\n",
        "#intialize PPO trainer with model , reward model and reference model for KL regularization\n",
        "ppo_trainer = PPOTrainer(model=model,\n",
        "                         ref_model=model,\n",
        "                         toekizer = tokenizer,\n",
        "                         reward_model = reward_model,\n",
        "                         **ppo_params)\n",
        "#prepare prompts for training\n",
        "prompts = [\"User: How can I improve my time management? Assistant:\",\n",
        "           \"User: Tell me about the benefits of meditation? Assistant:\"]\n",
        "#start training loop\n",
        "for epoch in enumerate(num_epochs):\n",
        "  for prompt in prompts:\n",
        "    input = tokenizer(prompt,return_tensor=\"pt\")\n",
        "    response_ids = model.generate(**input,max_new_tokens=100)\n",
        "    response = tokenizer.decode(response_ids[0,input['inputs_id'].shape[1]:])\n",
        "    reward_score = compute_reward_score(reward_model,prompt,response)\n",
        "    ppo_trainer.step([prompt],[response],reward_model)\n",
        "    print(response)"
      ],
      "metadata": {
        "id": "NDeoCe0OxJj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Second Code for fine tuning LLM with PPO - THE TASK IS SUMMARIZATION AND HENCE THE POLICY MODEL IS TRAINED ON THIS TASKS\n",
        "#to give better results\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import(\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    set_seed,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator\n",
        ")\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "  random.seed(seed_value)\n",
        "  np.random.seed(seed_value)\n",
        "  torch.manual_seed(seed_value)\n",
        "  torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "output_dir = \"supervised-summarize-checkpoint\"\n",
        "train_size = 16\n",
        "gradient_accumlation_steps = 1\n",
        "learning_rate = 1e-5\n",
        "eval_batch_size = 1\n",
        "eval_steps = 500\n",
        "max_input_length = 550\n",
        "save_steps = 1000\n",
        "num_train_epochs = 20\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "ADyCIOYADAnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create policy model\n",
        "df = pd.read_parquet(\"/content/sample_data/test_policy.parquet\")\n",
        "df.iloc[12]"
      ],
      "metadata": {
        "id": "9m_i1pCqKC-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import(\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    set_seed,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator\n",
        ")\n",
        "class TLDRDataset(Dataset):\n",
        "  def __init__(self,train_path,tokenizer,split,max_input_length = 256):\n",
        "    self.post_list=[]\n",
        "    dataset = pd.read_parquet(train_path)\n",
        "    self.labels = []\n",
        "    for sample in dataset.iterrows():\n",
        "      self.post_list.append(sample[1][\"prompt\"])\n",
        "      self.labels.append(sample[1][\"label\"])\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_input_length = max_input_length\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.post_list)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    text = self.post_list[idx]\n",
        "    label = self.labels[idx]\n",
        "\n",
        "    encodings_dict = self.tokenizer(\n",
        "        text,\n",
        "        max_length = self.max_input_length,\n",
        "        padding = \"max_length\",\n",
        "        truncation = True)\n",
        "\n",
        "    encodings_dict_label= self.tokenizer(\n",
        "        label,\n",
        "        max_length = self.max_input_length,\n",
        "        padding = \"max_length\",\n",
        "        truncation = True)\n",
        "    input_ids = torch.tensor(encodings_dict[\"input_ids\"])\n",
        "    attention_masks = torch.tensor(encodings_dict[\"attention_mask\"])\n",
        "    labels_ids = torch.tensor(encodings_dict_label[\"input_ids\"])\n",
        "    return {\"input_ids\":input_ids,\"attention_masks\":attention_masks,\"labels\":labels_ids}"
      ],
      "metadata": {
        "id": "g525Ow_2OCQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Intial Policy Model ---><--- - We are training on the dataset doing an SFT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/tiny_starcoder_py\")\n",
        "model  = AutoModelForCausalLM.from_pretrained(\"bigcode/tiny_starcoder_py\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" #padding as we want all sequences that go into LLM be of same size\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model.config.end_token_id = model.config.eos_token_id\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ],
      "metadata": {
        "id": "82Vi3B5cQgQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setup the dataset\n",
        "data_path = \"sample_data/test_policy.parquet\"\n",
        "train_dataset = TLDRDataset(data_path,tokenizer,\"train\",max_input_length=256)\n",
        "#eval_dataset = TLDRDataset(data_path,tokenizer,\"test\")"
      ],
      "metadata": {
        "id": "P-KDp_nORfx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_dataset:\n",
        "  print(i[\"input_ids\"],i[\"labels\"])\n",
        "  break"
      ],
      "metadata": {
        "id": "DWyVPl-4SsHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.set_device(1)"
      ],
      "metadata": {
        "id": "olqzfmLRT0Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = output_dir,\n",
        "    learning_rate = learning_rate,\n",
        "    per_device_train_batch_size = train_size,\n",
        "    #per_device_eval_batch_size = eval_batch_size,\n",
        "    fp16 = False,\n",
        "    gradient_accumulation_steps = gradient_accumlation_steps,\n",
        "    num_train_epochs = 2,\n",
        "    warmup_steps = 100,\n",
        "    logging_steps=10,\n",
        ")"
      ],
      "metadata": {
        "id": "XxcRVDwTToeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args.device.index"
      ],
      "metadata": {
        "id": "QUYIGCEUZ6bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    #eval_dataset = eval_dataset,\n",
        "    #data_collator = default_data_collator,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Lui_UA5zaAjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/models/summarization_policy_new/\")\n",
        "tokenizer.save_pretrained(\"/content/models/summarization_policy_new/\")"
      ],
      "metadata": {
        "id": "D9N73VVpatKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0][\"label\"]"
      ],
      "metadata": {
        "id": "MicTd6m0Co-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "import os\n",
        "\n",
        "model_path = \"/content/models/summarization_policy_new/\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Inspect the files in the directory to understand the tokenizer files\n",
        "print(\"Files in the saved model directory:\")\n",
        "print(os.listdir(model_path))\n",
        "\n",
        "# Load the tokenizer from the saved directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "text = df.iloc[2][\"prompt\"]\n",
        "tokenized_text = tokenizer(text,return_tensors=\"pt\",max_length=256)"
      ],
      "metadata": {
        "id": "yroKEdKkDUnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenized_text[\"input_ids\"][0])"
      ],
      "metadata": {
        "id": "1sUyD5-TKtG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the reward Model - the reward model acts as proxy to Human Feedback in RLHF - VERY IMP\n",
        "#SAME MODEL \"tiny_starcoder_py\" IS TRAINED ON SUMMARIZATION TASK AS POLICY MODEL . SAME MODEL IS TRAINED AS REWARD MODEL TO COMPARE THE SUMMARIES WHETHER SUMMARY\n",
        "#IS CHOSEN OR REJECTED\n",
        "\n",
        "!pip install --upgrade trl transformers"
      ],
      "metadata": {
        "id": "49GHmlItLWQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import pipeline , AutoTokenizer, AutoModelForCausalLM , DataCollatorForLanguageModeling , AutoModelForSequenceClassification\n",
        "from trl import RewardConfig,RewardTrainer ,SFTTrainer\n",
        "from datasets import Dataset\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import Trainer , TrainingArguments"
      ],
      "metadata": {
        "id": "s8sQwPHnM_3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path= \"bigcode/tiny_starcoder_py\" #reward model\n",
        "data_path = \"/content/sample_data/test.parquet\"\n",
        "#CarperAI/openai_summarize_comparisons - comparison dataset for REWARD MODEL"
      ],
      "metadata": {
        "id": "-xq8Uhm1NWxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(\"sample_data/test.parquet\")\n",
        "df = df[:10]\n",
        "raw_dataset = Dataset.from_pandas(df)\n",
        "raw_dataset"
      ],
      "metadata": {
        "id": "g7-eqYcROYMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer for the base model and set the padding token\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/tiny_starcoder_py\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the model with a sequence classification head for reward modeling\n",
        "reward_model = AutoModelForCausalLM.from_pretrained(model_path) # Using model_path which points to the saved policy model\n",
        "\n",
        "# Explicitly set the pad_token_id in the model's configuration\n",
        "reward_model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "e62gjJrZOumT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize the dataset\n",
        "tokenizer.add_special_tokens({'pad_token':'[PAD]'})\n",
        "def formatting_func(examples):\n",
        "  kwargs = {\n",
        "      \"padding\":\"max_length\",\n",
        "      \"truncation\": True,\n",
        "      \"max_length\":256,\n",
        "      \"return_tensors\":\"pt\"\n",
        "  }\n",
        "  #prepend the prompt\n",
        "  prompt_plus_chosen_response = examples[\"prompt\"] +\"\\n\"+examples[\"chosen\"]\n",
        "  prompt_plus_rejected_response = examples[\"prompt\"] +\"\\n\"+examples[\"rejected\"]\n",
        "\n",
        "  #tokenize the prompt\n",
        "  tokenized_prompt_plus_chosen_response = tokenizer(prompt_plus_chosen_response,**kwargs)\n",
        "  tokenized_prompt_plus_rejected_response = tokenizer(prompt_plus_rejected_response,**kwargs)\n",
        "  #tokenizer returns as Tensor with input_ids and attention masks - this is key\n",
        "\n",
        "  return {\n",
        "      \"input_ids_chosen\":tokenized_prompt_plus_chosen_response[\"input_ids\"][0],\n",
        "      \"attention_mask_chosen\":tokenized_prompt_plus_chosen_response[\"attention_mask\"][0], # Corrected key name\n",
        "      \"input_ids_rejected\":tokenized_prompt_plus_rejected_response[\"input_ids\"][0],\n",
        "      \"attention_mask_rejected\":tokenized_prompt_plus_rejected_response[\"attention_mask\"][0], # Corrected key name\n",
        "      }"
      ],
      "metadata": {
        "id": "XLKvMR15Skth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#map the function to data\n",
        "formatted_dataset = raw_dataset.map(formatting_func)\n",
        "formatted_dataset = formatted_dataset.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "6S95nnIJV5zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a training pipeline for training\n",
        "training_args = RewardConfig(\n",
        "    output_dir = \"/models/reward_model\",\n",
        "    num_train_epochs = 1,\n",
        "    logging_steps = 10,\n",
        "    gradient_accumulation_steps = 1,\n",
        "    per_device_train_batch_size = 2,\n",
        "    per_device_eval_batch_size = 1,\n",
        "    warmup_steps = 100,\n",
        "    learning_rate = 1e-5,\n",
        "    save_total_limit = 1,\n",
        ")"
      ],
      "metadata": {
        "id": "mrxVQBNiad_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl.trainer.utils import RewardDataCollatorWithPadding\n",
        "\n",
        "#reward trainer uses this loss function internally  -> loss = log(sigmoid(rchosen - rrejected))\n",
        "#training the model now\n",
        "trainer = RewardTrainer(\n",
        "    model = reward_model, # Use the reward_model here\n",
        "    train_dataset = formatted_dataset[\"train\"],\n",
        "    eval_dataset = formatted_dataset[\"test\"],\n",
        "    args = training_args,\n",
        "    data_collator = RewardDataCollatorWithPadding(tokenizer=tokenizer) # Explicitly provide the data collator\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "9hEJZF06cizL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "126ec74e"
      },
      "source": [
        "print(formatted_dataset[\"train\"].features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "trainer.save_model(\"/reward_model/\")"
      ],
      "metadata": {
        "id": "1rpJfonHl5YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade trl transformers accelerate"
      ],
      "metadata": {
        "id": "k1PhAOcA3Ih5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Policy Model - THIS POLICY MODEL IS A MODEL WITH VALUE HEAD - VALUE IS A SCALAR VALUE FOR expected cumulative reward from a given state.\n",
        "#MODEL LEARNS THE POLICY AND HOW GOOD THE POLICY IS AS WELL JOINTLY AT SAME TIME . THIS IS APPLICABLE IN CASE WHERE MODEL IS LEARNING BOTH POLICY BUT ALSO HOW..\n",
        "#GOOD THE STATE IS\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import pipeline , AutoTokenizer, AutoModelForCausalLM , DataCollatorForLanguageModeling\n",
        "from trl import RewardConfig,RewardTrainer ,SFTTrainer\n",
        "from datasets import Dataset\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import Trainer , TrainingArguments\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model"
      ],
      "metadata": {
        "id": "LwVqML7fnuHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path= \"bigcode/tiny_starcoder_py\"\n",
        "data_path = \"/content/sample_data/test.parquet\""
      ],
      "metadata": {
        "id": "hSVcutcboI0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(\"sample_data/test.parquet\")\n",
        "df = df[:1000]\n",
        "raw_dataset = Dataset.from_pandas(df)\n",
        "raw_dataset"
      ],
      "metadata": {
        "id": "SQPIi3cVoTyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_model_path = \"/reward_model/\"\n",
        "starcoder_model = AutoModelForCausalLMWithValueHead.from_pretrained(\"/content/models/summarization_policy_new/\") # policy model from Step 1\n",
        "starcoder_model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(reward_model_path) #reward model from Step2\n",
        "starcoder_tokenizer = AutoTokenizer.from_pretrained(\"bigcode/tiny_starcoder_py\")\n",
        "\n",
        "# Create and assign generation_config to the policy model\n",
        "#generation_config = GenerationConfig(pad_token_id=starcoder_tokenizer.pad_token_id)\n",
        "#starcoder_model.generation_config = generation_config\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_path,padding_side=\"left\")\n",
        "#tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "AlHylKLKogKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_in_len = 5\n",
        "txt_out_len = 32\n",
        "seed = 1\n",
        "\n",
        "dataset = raw_dataset.map(\n",
        "    lambda x: {\"input_ids\":tokenizer.encode(\" \"+ x[\"chosen\"], return_tensors=\"pt\", truncation = True, padding =\"max_length\",max_length= 32)[0]},batched = False)\n",
        "dataset = dataset.map(lambda x:{\"query\":tokenizer.decode(x[\"input_ids\"])} , batched= False)\n",
        "dataset = dataset[:20480]\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_dict(dataset)\n",
        "dataset.set_format(\"pytorch\")"
      ],
      "metadata": {
        "id": "fEm8wz30qDne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "PpryVdVgt72J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collator(data):\n",
        "  return dict((key,[d[key] for d in data]) for key in data[0])"
      ],
      "metadata": {
        "id": "zIfQ_g7GyooR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_kwargs = {\"top_k\":None , \"function_to_apply\":\"none\"}\n",
        "config = PPOConfig(learning_rate = 1.41e-5)\n",
        "txt_in_len = 5\n",
        "txt_out_len = 20\n",
        "seed = 1"
      ],
      "metadata": {
        "id": "GUhwcFanuf1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "ppo_trainer = PPOTrainer(\n",
        "    config,\n",
        "    starcoder_model, # model (policy)\n",
        "    starcoder_model_ref, # ref_model\n",
        "    starcoder_tokenizer, # tokenizer\n",
        "    dataset,  # train_dataset\n",
        "    collator, # data_collator\n",
        "    reward_model, # reward_model\n",
        "    starcoder_model # value_model\n",
        ")"
      ],
      "metadata": {
        "id": "sMrxOSFJvq2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GRPO TRAINING FOR RLHF\n",
        "#CREDIT - https://www.youtube.com/watch?v=yGkJj_4bjpE"
      ],
      "metadata": {
        "id": "Rcf6W7du-2wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate  utils"
      ],
      "metadata": {
        "id": "aDlYXUuBFpKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from accelerate import Accelerator\n",
        "from utils import*\n",
        "import grpo_utils\n",
        "\n",
        "model_name = \"models/sft_SmolLM-135M-Instruct\"\n",
        "batch_size = 2 #sample two questions from the dataloader at any time\n",
        "n_rollouts = 3\n",
        "buffer_size = 6\n",
        "max_new_tokens = 100\n",
        "\n",
        "#load essentials\n",
        "llm = load_model(model_name)\n",
        "for param in llm.parameters():\n",
        "  param.requires_grad = False\n",
        "tokenizer = load_tokenizer(model_name)\n",
        "dataloader = get_dataloader(\"syllogism\",tokenizer)\n",
        "optimizer = torch.optim.AdamW(llm.parameters(),lr=1e-5)\n",
        "\n",
        "#Intialize accelerator\n",
        "accelerator = Accelerator()\n",
        "llm, optimizer, dataloader = accelerator.prepare(llm,optimizer,dataloader,tokenizer)"
      ],
      "metadata": {
        "id": "pQ-raS3Z-_sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(dataloader))\n",
        "batch.keys()"
      ],
      "metadata": {
        "id": "wmamls_QLSB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = batch[\"inputs\"][\"input_ids\"]\n",
        "attention_mask = batch[\"inputs\"][\"attention_mask\"]\n",
        "validator = batch[\"validator\"]\n",
        "input_size = input_ids.shape[1]\n",
        "FORMAT_REWARD_WEIGHT = 0.1\n",
        "CORRECTNESS_REWARD_WEIGHT = 0.9\n",
        "\n",
        "#function to calculate reward\n",
        "def calculate_format_reward(response):\n",
        "    if (\n",
        "        \"<answer>\" not in response and\n",
        "         \"</answer>\" not in response and\n",
        "         \"<think>\" not in response and\n",
        "         \"</think>\" not in response\n",
        "    ):\n",
        "      return -1\n",
        "    format_reward = 0\n",
        "    if \"<think>\" in response:\n",
        "      format_reward += 0.15\n",
        "    if \"</think>\" in response:\n",
        "      format_reward += 0.15\n",
        "    if \"<answer>\" in response and \"</answer>\" in response:\n",
        "      return format_reward +0.7\n",
        "    else:\n",
        "      return format_reward\n",
        "\n",
        "def calculate_rewards(batch_responses,validation_objects):\n",
        "    format_rewards = np.array(\n",
        "        [calculate_format_reward(response) for response in batch_responses]\n",
        "    )\n",
        "    #calculate if answer is correct\n",
        "    correctness_rewards = np.array(\n",
        "        calculate_correctness_reward(extract_answer(response),val_obj)\n",
        "        for val_obj , response in zip(validation_objects,batch_responses)\n",
        "    )\n",
        "    #calculate final rewards\n",
        "    rewards = format_rewards + correctness_rewards\n",
        "    return rewards\n",
        "\n",
        "with torch.nograd(): # no gradient calculation\n",
        "  full_responses = llm.generate(\n",
        "      input_ids = input_ids,\n",
        "      attention_mask = attention_mask,\n",
        "      max_new_tokens = max_new_tokens, #100 only 100 tokens to be generated\n",
        "      do_sample = True,\n",
        "      top_p = 0.95, #probability of token\n",
        "      num_return_sequences = n_rollouts,\n",
        "      temperature=1,\n",
        "      eos_token_id = tokenizer.eos_token_id,\n",
        "      )\n",
        "  #get just the assitant response and hence below line of code\n",
        "  assistant_responses = full_responses[:,input_size:]\n",
        "  #raw logits from llm\n",
        "  logits = llm(input_ids = full_responses,attention_mask = attention_mask).logits\n",
        "  #get log probability of those logits by putting into a Softmax\n",
        "  log_probs = grpo_utils.calculate_logits(llm,full_responses,attention_mask)\n",
        "  #convert tokens into strings/text\n",
        "  decoded_responses = tokenizer.batch_decode(assistant_responses,skip_special_tokens = True)\n",
        "\n",
        "  #model_responses = [batch_size*n_rollouts, max_new_tokens]\n",
        "  #calculate rewards from the LLM response\n",
        "  rewards = grpo_utils.calculate_rewards(decoded_responses, np.repeat(validator,n_rollouts))\n",
        "  #how is reward calculated .... reward is calculated by analyzing the assitant response contains certain tags or not like <THINK> </THINK> etc...\n",
        "  #we asked the LLM for these tags .... so if these tags are present in the LLM response , accordingly reward is calculated and if LLM doesn't respond with tags\n",
        "  #accordingly reward is calculated and LLM is trained using GRPO\n",
        "\n",
        "  #calculate the advantage\n",
        "  rewards = np.reshape(rewards,(batch_size,n_rollouts))\n",
        "  advantages = (rewards - np.mean(rewards,axis=1,keepdims=True)) / np.std(rewards,axis=1,keepdims=True) +1e-8\n",
        "  #axis in numpy means:\n",
        "  #0 in 1D array - apply the function/formula to all elements\n",
        "  #0 in 2D array - apply the function/formula to row\n",
        "  #0 in 3D array - apply the function/formula to all 2D array - depth of array\n",
        "\n",
        "  #1 in 2D array - apply the function/formula to column\n",
        "  #1 in 3D array - apply the function / formula to row\n",
        "  #2 in 3D array - apply the function /formula to column\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "PQjg7xlxOhdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C3hVvpLPb5Ag"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}