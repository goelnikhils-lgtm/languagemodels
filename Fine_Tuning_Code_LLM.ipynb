{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWsQ5jN3RiPOgCs9CZxuEv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Fine_Tuning_Code_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBKfeuRiRrRg"
      },
      "outputs": [],
      "source": [
        "#fine tuning - Instruct Fine Tuning Code LLM  - SFT\n",
        "#https://milvus.io/ai-quick-reference/what-is-the-learning-rate-schedule-used-during-finetuning - learning rate for larger models can be range between 1e-5 to 5e-5..\n",
        "#.. this will help the model to not distrub too much of pre-trained weights\n",
        "#warm up steps are typically 10% of overall training steps however warm up steps can be more for bigger models\n",
        "#idea is that model should not get stuck in local minima\n",
        "#optimizer like AdamW can be used and annealing rate can be cosine etc .... and move to 0\n",
        "#FINE TUNING CODE CREDIT - https://www.youtube.com/watch?v=CUJexhbvBqM and BIGCODER https://github.com/bigcode-project/starcoder/blob/main/finetune/finetune.py\n",
        "#CREDIT - > https://huggingface.co/blog/personal-copilot\n",
        "#CREDIT ->https://github.com/pacman100/LLM-Workshop/blob/main/chat_assistant/dpo/utils.py\n",
        "\n",
        "#THERE IS CODE ALSO FOR FILL IN MIDDLE(FIM) .... WHICH IS MUST FOR PRE-TRAINING AND ALSO FOR FINE TUNING OF AN LLM\n",
        "#CREDIT - > FIM - PRE-TRAINING CODE - https://medium.com/@rajabmondal97/pretraining-extending-pretraining-of-autoregressive-model-using-the-fill-in-the-middle-objective-cc04c8f01d77 - SUPER ARTICLE\n",
        "#CREDIT -> FIM FINE TUNING CODE - >https://blog.gopenai.com/fine-tuning-language-models-with-fill-in-the-middle-a-comprehensive-guide-58a022b8f8df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers peft accelerate bitsandbytes datasets wandb"
      ],
      "metadata": {
        "id": "LGss9PQpSnnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
        "import torch\n",
        "import wandb"
      ],
      "metadata": {
        "id": "nWhLPztTTeLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"checkpoint = \"bigcode/starcoder\"\n",
        "device = \"cuda\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n",
        "\n",
        "inputs = tokenizer(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Qh7ZGoJsTp5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/bigcode-project/starcoder.git"
      ],
      "metadata": {
        "id": "eOUec3pIVm3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python /content/starcoder/finetune/finetune.py"
      ],
      "metadata": {
        "id": "r4ua6BWNWWIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fine tuning code from starcoder\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "from accelerate import Accelerator\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model,  prepare_model_for_kbit_training , set_peft_model_state_dict\n",
        "from torch.utils.data import IterableDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import AutoConfig , AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments , Trainer , logging , set_seed\n",
        "from transformers import TrainerCallback , TrainingArguments, TrainerState , TrainerControl\n",
        "\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "\n",
        "\"\"\"\n",
        "Fine tune Starcoder on Alpaca Dataset\n",
        "\"\"\"\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "        kwargs[\"model\"].save_pretrained(checkpoint_folder)\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, 'pytorch_model.bin')\n",
        "        torch.save({}, pytorch_model_path)\n",
        "        return control\n",
        "\n",
        "class LoadBestPeftModelCallback(TrainerCallback):\n",
        "    def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "      print(f\"Loading best model from {state.best_model_checkpoint} (score:{state.best_metric})\")\n",
        "      best_model_path = os.path.join(state.best_model_checkpoint, 'adapter_model.bin')\n",
        "      adapter_weights = torch.load(best_model_path)\n",
        "      model = kwargs['model']\n",
        "      set_peft_model_state_dict(model, adapter_weights)\n",
        "      return control\n",
        "\n",
        "def get_args():\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\"--model_name\", type=str, default=\"bigcode/starcoder\")\n",
        "  parser.add_argument(\"--dataset_name\", type=str, default=\"HuggingFaceH4/CodeAlpaca-20k\")\n",
        "  parser.add_argument(\"--subset\", type=str)\n",
        "  parser.add_argument(\"--split\",type = str)\n",
        "  parser.add_argument(\"--size_valid_set\" , type = int,default = 10000)\n",
        "  parser.add_argument(\"--size_train_set\" , type = int, default = 10000)\n",
        "  parser.add_argument(\"--streaming\", action=\"store_true\")\n",
        "  parser.add_argument(\"--shuffle_buffer\",type = int , default = 5000)\n",
        "  parser.add_argument(\"--input_column\", type = str, default = \"prompt\")\n",
        "  parser.add_argument(\"--output_column\", type = str, default = \"completion\")\n",
        "\n",
        "  parser.add_argument(\"--seq_length\",type = int , default = 2048)\n",
        "  parser.add_argument(\"--max_steps\",type=int,default = 10000)\n",
        "  parser.add_argument(\"--batch_size\",type = int , default = 4)\n",
        "  parser.add_argument(\"--gradient_accumulation_steps\",type = int , default = 16)\n",
        "  parser.add_argument(\"--eos_token_id\",type = int , default = 41952)\n",
        "\n",
        "  parser.add_argument(\"--lora_r\",type = int , default = 16)\n",
        "  parser.add_argument(\"--lora_alpha\",type = int , default = 32)\n",
        "  parser.add_argument(\"--lora_dropout\",type = float , default = 0.05)\n",
        "  parser.add_argument(\"--lora_target_modules\",type = str , default = \"query_key_value\") #this is key\n",
        "\n",
        "\n",
        "  parser.add_argument(\"--learning_rate\",type = float , default = 5e-6) # this is key start with lower\n",
        "  parser.add_argument(\"--lr_scheduler_type\",type = str , default = \"cosine\") # this is key cosine\n",
        "  parser.add_argument(\"--num_warmup_steps\",type = int , default = 100)\n",
        "  parser.add_argument(\"--weight_decay\",type = float , default = 0.05)\n",
        "\n",
        "  parser.add_argument(\"--local_rank\",type = int , default = 0)\n",
        "  parser.add_argument(\"--no_fp16\",action = \"store_false\")\n",
        "  parser.add_argument(\"--bf16\",action = \"store_true\", default = True)\n",
        "  parser.add_argument(\"--no_gradient_checkpointing\",action = \"store_false\",default = False)\n",
        "  parser.add_argument(\"--seed\",type = int , default = 0)\n",
        "  parser.add_argument(\"--num_workers\",type = int , default = None)\n",
        "  parser.add_argument(\"--output_dir\", type=str, default=\"./checkpoints\")\n",
        "  parser.add_argument(\"--log_freq\",type = int , default = 100)\n",
        "  parser.add_argument(\"--eval_freq\",type = int , default = 100)\n",
        "  parser.add_argument(\"--save_freq\",type = int , default = 1000)\n",
        "  args, unknown = parser.parse_known_args()\n",
        "  return args\n",
        "\n",
        "def chars_token_ratio(dataset,tokenizer,input_column_name = \"prompt\",output_column_name = \"completion\",nb_examples = 400):\n",
        "  \"\"\"\n",
        "  Estimate the average number of characters per token in the dataset.\n",
        "  \"\"\"\n",
        "  total_characters, total_tokens = 0,0\n",
        "  for _ , example in tqdm(zip(range(nb_examples),iter(dataset)),total = nb_examples):\n",
        "    text = prepare_sample_text(example,input_column_name, output_column_name)\n",
        "    total_characters += len(text)\n",
        "    if tokenizer.is_fast:\n",
        "      total_tokens += len(tokenizer(text).tokens())\n",
        "    else:\n",
        "      total_tokens += len(tokenizer.tokenize(text))\n",
        "  return total_characters / total_tokens\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "  \"\"\"\n",
        "  Prints the number of trainable parameters in the model.\n",
        "  \"\"\"\n",
        "  trainable_params = 0\n",
        "  all_param = 0\n",
        "  for _, param in model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "      trainable_params += param.numel()\n",
        "  print(\n",
        "      f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "  )\n",
        "def prepare_sample_text(example ,input_column_name = \"prompt\",output_column_name = \"completion\"):\n",
        "  \"\"\"\n",
        "  Prepare the text from a sample of the dataset.\n",
        "  \"\"\"\n",
        "  text = f\"{example[input_column_name]}\\n\\nAnswer {example[output_column_name]}\"\n",
        "  return text\n",
        "\n",
        "class ConstantLengthDataset(IterableDataset):\n",
        "  \"\"\"\n",
        "  Iterable dataset that returns constant length chunks of tokens from stream of text files\n",
        "  Args:\n",
        "  tokenizer\n",
        "  dataset\n",
        "  infinite\n",
        "  seq_length - length  of token sequences to return\n",
        "  num_of_sequences - number of sequences to keep in buffer\n",
        "  chars_per_token  - no of characters per token used to estimate number of token in text buffer\n",
        "  \"\"\"\n",
        "  def __init__(self,tokenizer,dataset,infinite = False ,seq_length = 1024,num_of_sequences = 1024,chars_per_token = 3.6 ,\n",
        "               input_column_name = \"prompt\",output_column_name = \"completion\"):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.concat_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else args.eos_token_id\n",
        "    self.dataset = dataset\n",
        "    self.infinite = infinite\n",
        "    self.seq_length = seq_length\n",
        "    self.current_size = 0\n",
        "    self.max_buffer_size = seq_length * num_of_sequences * chars_per_token\n",
        "    self.input_column_name = input_column_name\n",
        "    self.output_column_name = output_column_name\n",
        "\n",
        "  def __iter__(self):\n",
        "    iterator = iter(self.dataset)\n",
        "    more_examples = True\n",
        "    while more_examples:\n",
        "      buffer,buffer_len = [],0\n",
        "      while True:\n",
        "        if buffer_len >= self.max_buffer_size:\n",
        "          break\n",
        "        try:\n",
        "          buffer.append(prepare_sample_text(next(iterator),self.input_column_name,self.output_column_name))\n",
        "          buffer_len += len(buffer[-1])\n",
        "        except StopIteration:\n",
        "          if self.infinite:\n",
        "            iterator = iter(self.dataset)\n",
        "          else:\n",
        "            more_examples = False\n",
        "            break\n",
        "      tokenized_inputs = self.tokenizer(buffer,truncation = False)[\"input_ids\"]\n",
        "      all_token_ids =[]\n",
        "      for tokenized_input in tokenized_inputs:\n",
        "        all_token_ids.extend(tokenized_input + [self.concat_token_id]) #checking for eos token and appending that\n",
        "      for i in range(0,len(all_token_ids),self.seq_length): # do till sequence length\n",
        "        input_ids = all_token_ids[i:i+self.seq_length]\n",
        "        if len(input_ids) == self.seq_length:\n",
        "          self.current_size += 1\n",
        "          yield {\n",
        "            \"input_ids\":torch.LongTensor(input_ids),\n",
        "            \"labels\":torch.LongTensor(input_ids)\n",
        "            }\n",
        "def create_dataset(tokenizer,args):\n",
        "  dataset = load_dataset(args.dataset_name,\n",
        "                         data_dir = args.subset,\n",
        "                         split = args.split,\n",
        "                         use_auth_token = True,\n",
        "                         num_proc = args.num_workers if not args.streaming else None,\n",
        "                         streaming = args.streaming\n",
        "                         )\n",
        "  if args.streaming:\n",
        "    valid_data = dataset.take(args.size_valid_set)\n",
        "    train_data = dataset.skip(args.size_valid_set)\n",
        "    train_data = train_data.shuffle(buffer_size = args.shuffle_buffer , seed = args.seed)\n",
        "  else:\n",
        "    train_data = dataset[\"train\"]\n",
        "    valid_data = dataset[\"valid\"]\n",
        "  chars_per_token = chars_token_ratio(train_data,tokenizer,args.input_column,args.output_column)\n",
        "  train_dataset = ConstantLengthDataset(tokenizer,\n",
        "                                     train_data,\n",
        "                                     infinite = True,\n",
        "                                     seq_length = args.seq_length,\n",
        "                                     chars_per_token = chars_per_token,\n",
        "                                     input_column_name = args.input_column,\n",
        "                                     output_column_name = args.output_column)\n",
        "\n",
        "  valid_dataset = ConstantLengthDataset(tokenizer,\n",
        "                                     valid_data,\n",
        "                                     infinite = True,\n",
        "                                     seq_length = args.seq_length,\n",
        "                                     chars_per_token = chars_per_token,\n",
        "                                     input_column_name = args.input_column,\n",
        "                                     output_column_name = args.output_column)\n",
        "  return train_dataset,valid_dataset\n",
        "def run_training(args,train_dataset,valid_dataset):\n",
        "  model = AutoModelForCausalLM.from_pretrained(args.model_name,use_auth_token = True,\n",
        "                                               use_cache = not args.no_gradient_checkpointing,load_in_8bit = True,device_map = {\"\": Accelerator().process_index})\n",
        "\n",
        "  model = prepare_model_for_kbit_training(model)\n",
        "  lora_config = LoraConfig(\n",
        "      r = args.lora_r,\n",
        "      lora_alpha = args.lora_alpha,\n",
        "      lora_dropout = args.lora_dropout,\n",
        "      bias = \"none\",\n",
        "      task_type = \"CAUSAL_LM\",\n",
        "      target_modules = [\"c_proj\",\"c_attn\",\"q_attn\"])\n",
        "  model = get_peft_model(model,lora_config)\n",
        "  train_dataset.start_iteration = 0\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=args.output_dir,\n",
        "          dataloader_drop_last=True,\n",
        "          evaluation_strategy=\"steps\",\n",
        "          save_strategy=\"steps\",\n",
        "          load_best_model_at_end=True,\n",
        "          max_steps=args.max_steps,\n",
        "          eval_steps=args.eval_freq,\n",
        "          save_steps=args.save_freq,\n",
        "          logging_steps=args.log_freq,\n",
        "          per_device_train_batch_size=args.batch_size,\n",
        "          per_device_eval_batch_size=args.batch_size,\n",
        "          learning_rate=args.learning_rate,\n",
        "          lr_scheduler_type=args.lr_scheduler_type,\n",
        "          warmup_steps=args.num_warmup_steps,\n",
        "          gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "          gradient_checkpointing=not args.no_gradient_checkpointing,\n",
        "          fp16=not args.no_fp16,\n",
        "          bf16=args.bf16,\n",
        "          weight_decay=args.weight_decay,\n",
        "          run_name=\"StarCoder-finetuned\",\n",
        "          report_to=\"wandb\",\n",
        "          ddp_find_unused_parameters=False,\n",
        "      )\n",
        "  trainer = Trainer(model = model , args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "      eval_dataset = valid_dataset,\n",
        "      callbacks = [SavePeftModelCallback,LoadBestPeftModelCallback])\n",
        "  trainer.train()\n",
        "  model.save_pretrained(args.output_dir)\n",
        "  tokenizer.save_pretrained(args.output_dir)\n",
        "def main(args):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(args.model_name,use_auth_token = True)\n",
        "  train_dataset,eval_dataset = create_dataset(tokenizer,args)\n",
        "  run_training(args,train_dataset,eval_dataset)\n",
        "if __name__ == \"__main__\":\n",
        "  args = get_args()\n",
        "  set_seed(args.seed)\n",
        "  os.makedirs(args.output_dir,exist_ok = True)\n",
        "  main(args)"
      ],
      "metadata": {
        "id": "EWp6fj29WcGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qSsJhQqfiy6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FILL IN MIDDLE FINE TUNING TRAINING CODE FOR LLM"
      ],
      "metadata": {
        "id": "aCJT6fJLEdK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FIM - PSM ->PRE + SUF + MIDDLE + EOT\n",
        "#Original: \"Hello world from Python\"\n",
        "#FIM - >Transformed: <PRE>Hello<SUF>from Python<MID>world<EOT>\n",
        "\n",
        "#FIM - SPM ->(Suffix-Prefix-Middle)\n",
        "#Original: \"Hello world from Python\"\n",
        "#Transformed: <SUF>from Python<PRE>Hello<MID>world<EOT>\n",
        "\n",
        "#Joint training of the model on PSM + SPM helps better code generation\n",
        "#FIM models use bi-directional context unlike AR models which use left to right context"
      ],
      "metadata": {
        "id": "TJogFPu8MGr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth"
      ],
      "metadata": {
        "id": "YJGbGuMHQ9YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_training_mix(texts, fim_rate = 0.7):\n",
        "  \"\"\" Create mix training data with specified FIM rate\"\"\"\n",
        "  mixed_data = []\n",
        "  for text in texts:\n",
        "    if random.random() < fim_rate:\n",
        "      #apply FIM transformation\n",
        "      mixed_data.append(make_fim_mix(text, mode=\"joint\"))\n",
        "    else:\n",
        "      #keep standard AR format\n",
        "      mixed_data.append(tokenize_standard(text))\n",
        "  return mixed_data\n"
      ],
      "metadata": {
        "id": "ddAkkanxMuou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FIM transformation function\n",
        "def char_level_split(text):\n",
        "  \"\"\" Character level splitting for robustness against subtoken boundaries\"\"\"\n",
        "  if len(text) <3:\n",
        "    return text ,\"\",\"\"\n",
        "  i = random.randint(0,len(text)-2)\n",
        "  j = random.randint(i+1,len(text)-1)\n",
        "  return text[:i],text[i:j],text[j:]\n",
        "\n",
        "def make_fim_mix(text,mode = \"joint\"):\n",
        "  \"\"\" Convert test to FIM format with special tokens \"\"\"\n",
        "  prefix , middle , suffix = char_level_split(text)\n",
        "  #tokenize components\n",
        "  prefix = tokenizer.encode(prefix , add_special_tokens = False)\n",
        "  middle = tokenizer.encode(middle , add_special_tokens = False)\n",
        "  suffix = tokenizer.encode(suffix , add_special_tokens = False)\n",
        "\n",
        "  #get special token IDs\n",
        "  PRE = tokenizer.convert_tokens_to_ids(\"<PRE>\")\n",
        "  MID = tokenizer.convert_tokens_to_ids(\"<MID>\")\n",
        "  SUF = tokenizer.convert_tokens_to_ids(\"<SUF>\")\n",
        "  EOT = tokenizer.convert_tokens_to_ids(\"<EOT>\")\n",
        "\n",
        "  #Random format selection for joint training\n",
        "  if mode == \"joint\":\n",
        "    mode = \"PSM\"  if random.random() < 0.5 else \"SPM\"\n",
        "\n",
        "  # Format according to chosen mode\n",
        "  if mode == \"PSM\":\n",
        "    return [PRE] + prefix + [SUF] + suffix +  [MID] + middle + [EOT]\n",
        "  elif mode == \"SPM\":\n",
        "    return [SUF] + suffix + [PRE] + prefix + [MID] + middle + [EOT]\n",
        "  else:\n",
        "    raise ValueError(f\"Unknown mode {mode}\")"
      ],
      "metadata": {
        "id": "9BYynjZjNu3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "import random\n",
        "\n",
        "base_model = \"facebook/opt-350m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "model = FastLanguageModel.from_pretrained(base_model , load_in_4bit = True ,use_cache = False)\n",
        "\n",
        "#Add FIM special tokens\n",
        "special_tokens = [\"<PRE>\",\"<SUF>\",\"<EOT>\",\"<MID>\"]\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\":special_tokens})\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "Zt66_6vKQ2d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DATASET PROCESSING AND TRAINING\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"wikitest\",\"wikitext-2-raw-v1\")\n",
        "\n",
        "def process(batch):\n",
        "  \"\"\" Apply FIM transformation to batch\"\"\"\n",
        "  toks = []\n",
        "  for txt in batch[\"text\"]:\n",
        "    if len(txt.strip()) ==0:\n",
        "      continue\n",
        "    toks.append(make_fim_mix(txt))\n",
        "  return {\"input_ids\":toks}\n",
        "\n",
        "#tranform datasets\n",
        "train_ds = dataset[\"train\"].map(process, batched = True, remove_columns = [\"text\"])\n",
        "eval_ds = dataset[\"test\"].map(process, batched = True, remove_columns = [\"text\"])\n",
        "\n",
        "#Training Configuration\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"/content/model\",\n",
        "    per_device_train_batch_size = 4,\n",
        "    gradient_accumlation_steps = 4,\n",
        "    learning_rate = 2e-4,\n",
        "    num_train_epochs =1,\n",
        "    logging_steps = 10,\n",
        "    save_steps = 100,\n",
        "    save_total_limit = 2\n",
        ")\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_ds,\n",
        "    eval_dataset = eval_ds,\n",
        "    args = training_args,\n",
        "    dataset_text_field = \"input_ids\",\n",
        "    max_seq_length = tokenizer.model_max_length\n",
        ")\n"
      ],
      "metadata": {
        "id": "8A6IcTqmSdsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRE-TRAINING FIM CODE"
      ],
      "metadata": {
        "id": "lWrnSrQaWQGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "4o1c2i8vW6k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset , DatasetDict , load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import Trainer , TrainingArguments, DataCollatorForLanguageModeling\n"
      ],
      "metadata": {
        "id": "j-YCBIRyWdXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"bigcode/starcoderbase-1b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "1YebNQAoXcFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix_token = \"<fim_prefix>\"\n",
        "suffix_token = \"<fim_suffix>\"\n",
        "middle_token = \"<fim_middle>\"\n",
        "eos_token = \"<|eot|>\"\n",
        "\n",
        "context_length = 512\n",
        "chunk_length = context_length\n",
        "max_len = context_length\n",
        "\n",
        "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
        "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
        "raw_datasets = DatasetDict(\n",
        "    {\n",
        "        \"train\": ds_train.shuffle().select(range(50000)),\n",
        "        \"valid\": ds_valid.shuffle().select(range(500))\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "RXYUJWWcYIJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chunking function for FIM - THIS IS  KEY FUNCTION\n",
        "def chunk_documents(doc_list,chunk_length):\n",
        "  joined_documents = eos_token.join(doc_list)\n",
        "  chunk_list = []\n",
        "  for i in range(0,len(joined_documents),chunk_length):\n",
        "    chunk_list.append(joined_documents[i:i+chunk_length])\n",
        "  return chunk_list\n",
        "\n",
        "def chunk_dataset(raw_datasets,chunk_length):\n",
        "  train_data = [d[\"content\"] for d in raw_datasets[\"train\"]]\n",
        "  valid_data = [d[\"content\"] for d in raw_datasets[\"valid\"]]\n",
        "\n",
        "  train_data = chunk_documents(train_data,chunk_length)\n",
        "  valid_data = chunk_documents(valid_data,chunk_length)\n",
        "\n",
        "  train_data = [{\"content_chunk\":ch} for ch in train_data]\n",
        "  valid_data = [{\"content_chunk\":ch} for ch in valid_data]\n",
        "\n",
        "  chunk_train_ds = Dataset.from_pandas(pd.DataFrame(train_data))\n",
        "  chunk_valid_ds = Dataset.from_pandas(pd.DataFrame(valid_data))\n",
        "\n",
        "  chunk_datasets = DatasetDict(\n",
        "      {\n",
        "          \"train\":chunk_train_ds,\n",
        "          \"valid\":chunk_valid_ds\n",
        "      }\n",
        "  )\n",
        "  return chunk_datasets\n",
        "chunk_ds = chunk_dataset(raw_datasets,chunk_length)"
      ],
      "metadata": {
        "id": "KE70PQ-_ZUyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build FIM Dataset\n",
        "def split_document(doc, fim_rate = 0.5):\n",
        "  if random.random() < fim_rate:\n",
        "      #apply FIM transformation\n",
        "      length = len(doc)\n",
        "      prefix_len = random.randint(0,length)\n",
        "      suffix_len = random.randint(0,length-prefix_len)\n",
        "      middle_len = length - prefix_len - suffix_len\n",
        "\n",
        "      prefix = doc[:prefix_len]\n",
        "      middle = doc[prefix_len:prefix_len+middle_len]\n",
        "      suffix = doc[prefix_len + middle_len:]\n",
        "\n",
        "      return prefix , middle , suffix\n",
        "  else:\n",
        "      return doc, None , None\n",
        "\n",
        "def format_psm(prefix,middle,suffix,tokenizer):\n",
        "    return f\"{prefix_token}{prefix}{middle_token}{middle}{suffix_token}{suffix}{eos_token}\"\n",
        "def format_spm(prefix,middle,suffix,tokenizer):\n",
        "    return f\"{suffix_token}{suffix}{prefix_token}{prefix}{middle_token}{middle}{eos_token}\"\n"
      ],
      "metadata": {
        "id": "bPXhwUECcLc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply FIM\n",
        "def apply_fim(chunk_list , p_psm = 0.5):\n",
        "  transformed_docs = []\n",
        "  for chunk in chunk_list:\n",
        "    prefix , middle , suffix = split_document(chunk)\n",
        "    if middle is not None:\n",
        "      if random.random() < p_psm:\n",
        "        transformed_docs.append(format_psm(prefix,middle,suffix, tokenizer))\n",
        "      else:\n",
        "        transformed_docs.append(format_spm(prefix,middle,suffix,tokenizer))\n",
        "  return transformed_docs\n",
        "def join_tranformed_chunk_docs(transformed_chunk_docs):\n",
        "  merged_docs = transformed_chunk_docs[0]\n",
        "  if len(transformed_chunk_docs) == 1:\n",
        "    return merged_docs\n",
        "  for i in range(2,len(transformed_chunk_docs)):\n",
        "    merged_docs += eos_token + transformed_chunk_docs[i]\n",
        "    print(\"merged_document\",merged_docs)\n",
        "  return merged_docs"
      ],
      "metadata": {
        "id": "9Yw_0t0xeydC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build FIM Dataset\n",
        "def split_document(doc, fim_rate = 0.5):\n",
        "  if random.random() < fim_rate:\n",
        "      #apply FIM transformation\n",
        "      length = len(doc)\n",
        "      prefix_len = random.randint(0,length)\n",
        "      suffix_len = random.randint(0,length-prefix_len)\n",
        "      middle_len = length - prefix_len - suffix_len\n",
        "\n",
        "      prefix = doc[:prefix_len]\n",
        "      middle = doc[prefix_len:prefix_len+middle_len]\n",
        "      suffix = doc[prefix_len + middle_len:]\n",
        "\n",
        "      return prefix , middle , suffix\n",
        "  else:\n",
        "      return doc, None , None\n",
        "\n",
        "def format_psm(prefix,middle,suffix,tokenizer):\n",
        "    return f\"{prefix_token}{prefix}{middle_token}{middle}{suffix_token}{suffix}{eos_token}\"\n",
        "def format_spm(prefix,middle,suffix,tokenizer):\n",
        "    return f\"{suffix_token}{suffix}{prefix_token}{prefix}{middle_token}{middle}{eos_token}\"\n",
        "\n",
        "def apply_fim(chunk_list , p_psm = 0.5):\n",
        "  transformed_docs = []\n",
        "  for chunk in chunk_list:\n",
        "    prefix , middle , suffix = split_document(chunk)\n",
        "    if middle is not None:\n",
        "      if random.random() < p_psm:\n",
        "        transformed_docs.append(format_psm(prefix,middle,suffix, tokenizer))\n",
        "      else:\n",
        "        transformed_docs.append(format_spm(prefix,middle,suffix,tokenizer))\n",
        "  return transformed_docs\n",
        "def join_tranformed_chunk_docs(transformed_chunk_docs):\n",
        "  if not transformed_chunk_docs:  # Add this check\n",
        "    return \"\"\n",
        "  merged_docs = transformed_chunk_docs[0]\n",
        "  if len(transformed_chunk_docs) == 1:\n",
        "    return merged_docs\n",
        "  for i in range(1,len(transformed_chunk_docs)): # Start from index 1\n",
        "    merged_docs += eos_token + transformed_chunk_docs[i]\n",
        "  return merged_docs\n",
        "\n",
        "def apply_context_level_fim(chunk):\n",
        "  chunk_documents = chunk.split(eos_token) #split \"huggingface-course/codeparrot-ds-train/valid and in that \"content\" column data  into prefix , suffix and middle\n",
        "  transformed_chunk_docs = apply_fim(chunk_documents) # apply FIM - either PSM or SPM\n",
        "  joined_transformed_chunk_docs = join_tranformed_chunk_docs(transformed_chunk_docs) # appending EOS to the\n",
        "  formatted_example = tokenizer(joined_transformed_chunk_docs,truncation = False, padding =\"max_length\",max_length = max_len)\n",
        "  return {\"input_ids\":torch.LongTensor(formatted_example[\"input_ids\"]).squeeze(0),\"attention_mask\":torch.LongTensor(formatted_example[\"attention_mask\"]).squeeze(0)}\n",
        "\n",
        "# this is the main function that maps the dataset to the format\n",
        "train_dataset = chunk_ds[\"train\"].map(lambda x: apply_context_level_fim(x[\"content_chunk\"]))\n",
        "valid_dataset = chunk_ds[\"valid\"].map(lambda x: apply_context_level_fim(x[\"content_chunk\"]))"
      ],
      "metadata": {
        "id": "dLhyWSP5hKRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78edc716"
      },
      "source": [
        "!pip install --upgrade transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def custom trainer\n",
        "class CustomTrainer(Trainer):\n",
        "  def get_train_dataloader(self):\n",
        "    if self.train_dataset is None:\n",
        "      raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
        "    # Use the default sampler provided by the base Trainer class\n",
        "    train_sampler = super().get_train_sampler()\n",
        "    data_collator = self.data_collator\n",
        "    return DataLoader(\n",
        "        self.train_dataset,\n",
        "        batch_size = self.args.train_batch_size,\n",
        "        sampler = train_sampler,\n",
        "        collate_fn = data_collator,\n",
        "        drop_last = self.args.dataloader_drop_last,\n",
        "        num_workers = self.args.dataloader_num_workers,\n",
        "        pin_memory = self.args.dataloader_pin_memory,\n",
        "    )\n",
        "  def get_eval_dataloader(self, eval_dataset = None):\n",
        "    if eval_dataset is None and self.eval_dataset is None:\n",
        "      raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
        "    if eval_dataset is not None and not isinstance(eval_dataset, Dataset):\n",
        "      raise ValueError(\"Trainer: eval_dataset must be a torch.utils.data.Dataset.\")\n",
        "\n",
        "    eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
        "    # Use the default sampler provided by the base Trainer class\n",
        "    eval_sampler = super().get_eval_sampler(eval_dataset)\n",
        "    data_collator = self.data_collator\n",
        "    return DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size = self.args.eval_batch_size,\n",
        "        sampler = eval_sampler,\n",
        "        collate_fn = data_collator,\n",
        "        drop_last = self.args.dataloader_drop_last,\n",
        "        num_workers = self.args.dataloader_num_workers,\n",
        "        pin_memory = self.args.dataloader_pin_memory,\n",
        "    )\n",
        "#this is for preparing data to be given to batches for training models. mlm = False means for Causal Models AR models .... mlm = True means for models where\n",
        "#training objective is MLM like BERT\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer,mlm = False)\n",
        "\n",
        "num_train_epochs = 3 # Define num_train_epochs\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"/content/model\",\n",
        "    per_device_train_batch_size =1,\n",
        "    per_device_eval_batch_size = 1,\n",
        "    evaluation_strategy =\"steps\",\n",
        "    eval_steps = 100,\n",
        "    logging_steps = 100,\n",
        "    gradient_accumulation_steps = 2,\n",
        "    num_train_epochs = num_train_epochs,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.95,\n",
        "    adam_epsilon = 1e-8,\n",
        "    max_grad_norm = 1.0,\n",
        "    weight_decay = 0.01,\n",
        "    learning_rate = 5e-5,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    save_strategy = \"steps\",\n",
        "    save_steps = 100,\n",
        "    save_total_limit = 10,\n",
        "    load_best_model_at_end = True\n",
        "    )\n",
        "trainer = CustomTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    args = training_args,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = valid_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "gB0b0Ekuf-SQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}