{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEuCeWF7x/z1CMDcdFWBnW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Fine_Tuning_Code_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBKfeuRiRrRg"
      },
      "outputs": [],
      "source": [
        "#fine tuning - Instruct Fine Tuning Code LLM  - SFT\n",
        "#CREDIT - >https://milvus.io/ai-quick-reference/what-is-the-learning-rate-schedule-used-during-finetuning - learning rate for larger models can be range between 1e-5 to 5e-5..\n",
        "#.. this will help the model to not distrub too much of pre-trained weights\n",
        "#warm up steps are typically 10% of overall training steps however warm up steps can be more for bigger models\n",
        "#idea is that model should not get stuck in local minima\n",
        "#optimizer like AdamW can be used and annealing rate can be cosine etc .... and move to 0\n",
        "#FINE TUNING CODE CREDIT - https://www.youtube.com/watch?v=CUJexhbvBqM and BIGCODER https://github.com/bigcode-project/starcoder/blob/main/finetune/finetune.py\n",
        "#CREDIT - >https://github.com/pacman100/LLM-Workshop/blob/main/chat_assistant/dpo/utils.py\n",
        "#CREDIT - >https://huggingface.co/blog/personal-copilot\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers peft accelerate bitsandbytes datasets wandb"
      ],
      "metadata": {
        "id": "LGss9PQpSnnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
        "import torch\n",
        "import wandb"
      ],
      "metadata": {
        "id": "nWhLPztTTeLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"checkpoint = \"bigcode/starcoder\"\n",
        "device = \"cuda\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n",
        "\n",
        "inputs = tokenizer(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Qh7ZGoJsTp5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/bigcode-project/starcoder.git"
      ],
      "metadata": {
        "id": "eOUec3pIVm3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python /content/starcoder/finetune/finetune.py"
      ],
      "metadata": {
        "id": "r4ua6BWNWWIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fine tuning code from starcoder\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "from accelerate import Accelerator\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model,  prepare_model_for_kbit_training , set_peft_model_state_dict\n",
        "from torch.utils.data import IterableDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import AutoConfig , AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments , Trainer , logging , set_seed\n",
        "from transformers import TrainerCallback , TrainingArguments, TrainerState , TrainerControl\n",
        "\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "\n",
        "\"\"\"\n",
        "Fine tune Starcoder on Alpaca Dataset\n",
        "\"\"\"\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "        kwargs[\"model\"].save_pretrained(checkpoint_folder)\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, 'pytorch_model.bin')\n",
        "        torch.save({}, pytorch_model_path)\n",
        "        return control\n",
        "\n",
        "class LoadBestPeftModelCallback(TrainerCallback):\n",
        "    def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "      print(f\"Loading best model from {state.best_model_checkpoint} (score:{state.best_metric})\")\n",
        "      best_model_path = os.path.join(state.best_model_checkpoint, 'adapter_model.bin')\n",
        "      adapter_weights = torch.load(best_model_path)\n",
        "      model = kwargs['model']\n",
        "      set_peft_model_state_dict(model, adapter_weights)\n",
        "      return control\n",
        "\n",
        "def get_args():\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\"--model_name\", type=str, default=\"bigcode/starcoder\")\n",
        "  parser.add_argument(\"--dataset_name\", type=str, default=\"HuggingFaceH4/CodeAlpaca-20k\")\n",
        "  parser.add_argument(\"--subset\", type=str)\n",
        "  parser.add_argument(\"--split\",type = str)\n",
        "  parser.add_argument(\"--size_valid_set\" , type = int,default = 10000)\n",
        "  parser.add_argument(\"--size_train_set\" , type = int, default = 10000)\n",
        "  parser.add_argument(\"--streaming\", action=\"store_true\")\n",
        "  parser.add_argument(\"--shuffle_buffer\",type = int , default = 5000)\n",
        "  parser.add_argument(\"--input_column\", type = str, default = \"prompt\")\n",
        "  parser.add_argument(\"--output_column\", type = str, default = \"completion\")\n",
        "\n",
        "  parser.add_argument(\"--seq_length\",type = int , default = 2048)\n",
        "  parser.add_argument(\"--max_steps\",type=int,default = 10000)\n",
        "  parser.add_argument(\"--batch_size\",type = int , default = 4)\n",
        "  parser.add_argument(\"--gradient_accumulation_steps\",type = int , default = 16)\n",
        "  parser.add_argument(\"--eos_token_id\",type = int , default = 41952)\n",
        "\n",
        "  parser.add_argument(\"--lora_r\",type = int , default = 16)\n",
        "  parser.add_argument(\"--lora_alpha\",type = int , default = 32)\n",
        "  parser.add_argument(\"--lora_dropout\",type = float , default = 0.05)\n",
        "  parser.add_argument(\"--lora_target_modules\",type = str , default = \"query_key_value\") #this is key\n",
        "\n",
        "\n",
        "  parser.add_argument(\"--learning_rate\",type = float , default = 5e-6) # this is key start with lower\n",
        "  parser.add_argument(\"--lr_scheduler_type\",type = str , default = \"cosine\") # this is key cosine\n",
        "  parser.add_argument(\"--num_warmup_steps\",type = int , default = 100)\n",
        "  parser.add_argument(\"--weight_decay\",type = float , default = 0.05)\n",
        "\n",
        "  parser.add_argument(\"--local_rank\",type = int , default = 0)\n",
        "  parser.add_argument(\"--no_fp16\",action = \"store_false\")\n",
        "  parser.add_argument(\"--bf16\",action = \"store_true\", default = True)\n",
        "  parser.add_argument(\"--no_gradient_checkpointing\",action = \"store_false\",default = False)\n",
        "  parser.add_argument(\"--seed\",type = int , default = 0)\n",
        "  parser.add_argument(\"--num_workers\",type = int , default = None)\n",
        "  parser.add_argument(\"--output_dir\", type=str, default=\"./checkpoints\")\n",
        "  parser.add_argument(\"--log_freq\",type = int , default = 100)\n",
        "  parser.add_argument(\"--eval_freq\",type = int , default = 100)\n",
        "  parser.add_argument(\"--save_freq\",type = int , default = 1000)\n",
        "  args, unknown = parser.parse_known_args()\n",
        "  return args\n",
        "\n",
        "def chars_token_ratio(dataset,tokenizer,input_column_name = \"prompt\",output_column_name = \"completion\",nb_examples = 400):\n",
        "  \"\"\"\n",
        "  Estimate the average number of characters per token in the dataset.\n",
        "  \"\"\"\n",
        "  total_characters, total_tokens = 0,0\n",
        "  for _ , example in tqdm(zip(range(nb_examples),iter(dataset)),total = nb_examples):\n",
        "    text = prepare_sample_text(example,input_column_name, output_column_name)\n",
        "    total_characters += len(text)\n",
        "    if tokenizer.is_fast:\n",
        "      total_tokens += len(tokenizer(text).tokens())\n",
        "    else:\n",
        "      total_tokens += len(tokenizer.tokenize(text))\n",
        "  return total_characters / total_tokens\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "  \"\"\"\n",
        "  Prints the number of trainable parameters in the model.\n",
        "  \"\"\"\n",
        "  trainable_params = 0\n",
        "  all_param = 0\n",
        "  for _, param in model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "      trainable_params += param.numel()\n",
        "  print(\n",
        "      f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "  )\n",
        "def prepare_sample_text(example ,input_column_name = \"prompt\",output_column_name = \"completion\"):\n",
        "  \"\"\"\n",
        "  Prepare the text from a sample of the dataset.\n",
        "  \"\"\"\n",
        "  text = f\"{example[input_column_name]}\\n\\nAnswer {example[output_column_name]}\"\n",
        "  return text\n",
        "\n",
        "class ConstantLengthDataset(IterableDataset):\n",
        "  \"\"\"\n",
        "  Iterable dataset that returns constant length chunks of tokens from stream of text files\n",
        "  Args:\n",
        "  tokenizer\n",
        "  dataset\n",
        "  infinite\n",
        "  seq_length - length  of token sequences to return\n",
        "  num_of_sequences - number of sequences to keep in buffer\n",
        "  chars_per_token  - no of characters per token used to estimate number of token in text buffer\n",
        "  \"\"\"\n",
        "  def __init__(self,tokenizer,dataset,infinite = False ,seq_length = 1024,num_of_sequences = 1024,chars_per_token = 3.6 ,\n",
        "               input_column_name = \"prompt\",output_column_name = \"completion\"):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.concat_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else args.eos_token_id\n",
        "    self.dataset = dataset\n",
        "    self.infinite = infinite\n",
        "    self.seq_length = seq_length\n",
        "    self.current_size = 0\n",
        "    self.max_buffer_size = seq_length * num_of_sequences * chars_per_token\n",
        "    self.input_column_name = input_column_name\n",
        "    self.output_column_name = output_column_name\n",
        "\n",
        "  def __iter__(self):\n",
        "    iterator = iter(self.dataset)\n",
        "    more_examples = True\n",
        "    while more_examples:\n",
        "      buffer,buffer_len = [],0\n",
        "      while True:\n",
        "        if buffer_len >= self.max_buffer_size:\n",
        "          break\n",
        "        try:\n",
        "          buffer.append(prepare_sample_text(next(iterator),self.input_column_name,self.output_column_name))\n",
        "          buffer_len += len(buffer[-1])\n",
        "        except StopIteration:\n",
        "          if self.infinite:\n",
        "            iterator = iter(self.dataset)\n",
        "          else:\n",
        "            more_examples = False\n",
        "            break\n",
        "      tokenized_inputs = self.tokenizer(buffer,truncation = False)[\"input_ids\"]\n",
        "      all_token_ids =[]\n",
        "      for tokenized_input in tokenized_inputs:\n",
        "        all_token_ids.extend(tokenized_input + [self.concat_token_id]) #checking for eos token and appending that\n",
        "      for i in range(0,len(all_token_ids),self.seq_length): # do till sequence length\n",
        "        input_ids = all_token_ids[i:i+self.seq_length]\n",
        "        if len(input_ids) == self.seq_length:\n",
        "          self.current_size += 1\n",
        "          yield {\n",
        "            \"input_ids\":torch.LongTensor(input_ids),\n",
        "            \"labels\":torch.LongTensor(input_ids)\n",
        "            }\n",
        "def create_dataset(tokenizer,args):\n",
        "  dataset = load_dataset(args.dataset_name,\n",
        "                         data_dir = args.subset,\n",
        "                         split = args.split,\n",
        "                         use_auth_token = True,\n",
        "                         num_proc = args.num_workers if not args.streaming else None,\n",
        "                         streaming = args.streaming\n",
        "                         )\n",
        "  if args.streaming:\n",
        "    valid_data = dataset.take(args.size_valid_set)\n",
        "    train_data = dataset.skip(args.size_valid_set)\n",
        "    train_data = train_data.shuffle(buffer_size = args.shuffle_buffer , seed = args.seed)\n",
        "  else:\n",
        "    train_data = dataset[\"train\"]\n",
        "    valid_data = dataset[\"valid\"]\n",
        "  chars_per_token = chars_token_ratio(train_data,tokenizer,args.input_column,args.output_column)\n",
        "  train_dataset = ConstantLengthDataset(tokenizer,\n",
        "                                     train_data,\n",
        "                                     infinite = True,\n",
        "                                     seq_length = args.seq_length,\n",
        "                                     chars_per_token = chars_per_token,\n",
        "                                     input_column_name = args.input_column,\n",
        "                                     output_column_name = args.output_column)\n",
        "\n",
        "  valid_dataset = ConstantLengthDataset(tokenizer,\n",
        "                                     valid_data,\n",
        "                                     infinite = True,\n",
        "                                     seq_length = args.seq_length,\n",
        "                                     chars_per_token = chars_per_token,\n",
        "                                     input_column_name = args.input_column,\n",
        "                                     output_column_name = args.output_column)\n",
        "  return train_dataset,valid_dataset\n",
        "def run_training(args,train_dataset,valid_dataset):\n",
        "  model = AutoModelForCausalLM.from_pretrained(args.model_name,use_auth_token = True,\n",
        "                                               use_cache = not args.no_gradient_checkpointing,load_in_8bit = True,device_map = {\"\": Accelerator().process_index})\n",
        "\n",
        "  model = prepare_model_for_kbit_training(model)\n",
        "  lora_config = LoraConfig(\n",
        "      r = args.lora_r,\n",
        "      lora_alpha = args.lora_alpha,\n",
        "      lora_dropout = args.lora_dropout,\n",
        "      bias = \"none\",\n",
        "      task_type = \"CAUSAL_LM\",\n",
        "      target_modules = [\"c_proj\",\"c_attn\",\"q_attn\"])\n",
        "  model = get_peft_model(model,lora_config)\n",
        "  train_dataset.start_iteration = 0\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=args.output_dir,\n",
        "          dataloader_drop_last=True,\n",
        "          evaluation_strategy=\"steps\",\n",
        "          save_strategy=\"steps\",\n",
        "          load_best_model_at_end=True,\n",
        "          max_steps=args.max_steps,\n",
        "          eval_steps=args.eval_freq,\n",
        "          save_steps=args.save_freq,\n",
        "          logging_steps=args.log_freq,\n",
        "          per_device_train_batch_size=args.batch_size,\n",
        "          per_device_eval_batch_size=args.batch_size,\n",
        "          learning_rate=args.learning_rate,\n",
        "          lr_scheduler_type=args.lr_scheduler_type,\n",
        "          warmup_steps=args.num_warmup_steps,\n",
        "          gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "          gradient_checkpointing=not args.no_gradient_checkpointing,\n",
        "          fp16=not args.no_fp16,\n",
        "          bf16=args.bf16,\n",
        "          weight_decay=args.weight_decay,\n",
        "          run_name=\"StarCoder-finetuned\",\n",
        "          report_to=\"wandb\",\n",
        "          ddp_find_unused_parameters=False,\n",
        "      )\n",
        "  trainer = Trainer(model = model , args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "      eval_dataset = valid_dataset,\n",
        "      callbacks = [SavePeftModelCallback,LoadBestPeftModelCallback])\n",
        "  trainer.train()\n",
        "  model.save_pretrained(args.output_dir)\n",
        "  tokenizer.save_pretrained(args.output_dir)\n",
        "def main(args):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(args.model_name,use_auth_token = True)\n",
        "  train_dataset,eval_dataset = create_dataset(tokenizer,args)\n",
        "  run_training(args,train_dataset,eval_dataset)\n",
        "if __name__ == \"__main__\":\n",
        "  args = get_args()\n",
        "  set_seed(args.seed)\n",
        "  os.makedirs(args.output_dir,exist_ok = True)\n",
        "  main(args)"
      ],
      "metadata": {
        "id": "EWp6fj29WcGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qSsJhQqfiy6N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}