{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMcQ06o/fuLEjk3dSSzhv/W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Code_Generation_Fine_Tuning_LLama3_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecIiO15iVg5d"
      },
      "outputs": [],
      "source": [
        "#Fine Tuning LLama3.2 on Code Generation\n",
        "!pip install  datasets\n",
        "!pip install --upgrade peft trl\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login(\"HF_TOKEN\")"
      ],
      "metadata": {
        "id": "OzaWkMQVI9do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import data set - DATA\n",
        "from datasets import load_dataset\n",
        "from datasets.arrow_dataset import Dataset\n",
        "\n",
        "def format_sample(sample):\n",
        "    \"\"\"helper function to format as single input sample \"\"\"\n",
        "    instruction = sample['instruction']\n",
        "    input_text = sample['input']\n",
        "    output_text = sample['output']\n",
        "\n",
        "    if input_text is None or input_text ==\"\":\n",
        "      formatted_prompt = ( # case when there is no input from user\n",
        "          f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "          f\"Below is an instruction that describes a task.Write a response that appropriately completes the task.\\n\\n\"\n",
        "          f\"### Instruction:\\n{instruction}\\n\\n\"\n",
        "          f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" #eot indicates to assitant that end of turn from user and now assitant needs to generate the response\n",
        "          f\"{output_text}<|eot_id|>\"\n",
        "      )\n",
        "    else:\n",
        "          formatted_prompt = (\n",
        "          f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "          f\"Below is an instruction that describes a task. Write a response that appropriately completes the task.\\n\\n \"\n",
        "          f\"### Instruction:\\n{instruction}\\n\\n ### Input:\\n{input_text}\\n\\n\" # input from user\n",
        "          f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" # assitant start generating the response\n",
        "          f\"{output_text}<|eot_id|>\"\n",
        "          )\n",
        "    formatted_prompt = \"\".join(formatted_prompt)\n",
        "    return formatted_prompt\n",
        "\n",
        "  #function for generating training data for model\n",
        "def gen_train_input():\n",
        "    \"\"\"format all data input in alpaca style\n",
        "      Return: A generator on train_data \"train_gen\"\n",
        "    \"\"\"\n",
        "    #load_data\n",
        "    ds = load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\",streaming=True, split=\"train\")\n",
        "    #dataset has 18.6k samples , we use 16.8k (90% of training +1.8 k for validation)\n",
        "    num_samples = 16800\n",
        "    counter = 0\n",
        "    for sample in iter(ds):\n",
        "      if counter >= num_samples:\n",
        "        break\n",
        "      formatted_prompt = format_sample(sample)\n",
        "      yield {\"text\": formatted_prompt}\n",
        "      counter += 1\n",
        "\n",
        "#function for generating validation data for model\n",
        "def gen_val_input():\n",
        "    \"\"\"format all data input in alpaca style\n",
        "      Return: A generator on train_data \"train_gen\"\n",
        "    \"\"\"\n",
        "    ds = load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\",streaming=True, split=\"train\")\n",
        "    #dataset has 18.6k samples , we use 16.8 K\n",
        "    num_samples = 16800\n",
        "    counter = 0\n",
        "    for sample in iter(ds):\n",
        "      if counter <  num_samples:\n",
        "        counter += 1\n",
        "        continue\n",
        "      if counter >= num_samples + 1800:\n",
        "        break\n",
        "      formatted_prompt = format_sample(sample)\n",
        "      yield {\"text\": formatted_prompt}\n",
        "\n",
        "#train datatset\n",
        "dataset_train = Dataset.from_generator(gen_train_input)\n",
        "#validate dataset\n",
        "dataset_val = Dataset.from_generator(gen_val_input)\n",
        "\n"
      ],
      "metadata": {
        "id": "SXckqeo8N_48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train dataset size: {len(dataset_train)}\")\n",
        "print(f\"Validation dataset size: {len(dataset_val)}\")\n",
        "print(f\"Sample train : \\n{dataset_train[0]}\")\n"
      ],
      "metadata": {
        "id": "VZAb5NYtYWNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model and Tokenizer\n",
        "import torch\n",
        "from peft import LoraConfig,AutoPeftModelForCausalLM\n",
        "from transformers import AutoModelForCausalLM , AutoTokenizer , TrainingArguments , BitsAndBytesConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "from google.colab import userdata\n",
        "access_token = userdata.get('HF_TOKEN')\n",
        "print(\"access_token\",access_token)\n",
        "\n",
        "def create_and_prepare_model():\n",
        "  #QLoRA - load the model in 4 bit quantization as the model is having 1BN parameters\n",
        "  bnb_config = BitsAndBytesConfig( #Model Quantization\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_quant_type=\"nf4\", #reduce model size\n",
        "      bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      quantization_config=bnb_config,\n",
        "      device_map=\"auto\",\n",
        "      token= access_token\n",
        "  )\n",
        "  peft_config = LoraConfig(\n",
        "      lora_alpha=16,\n",
        "      lora_dropout=0.05, #drop out for regularization to prevent overfitting. each neuron has a chance of 5%\n",
        "      r=8,\n",
        "      bias=\"none\",\n",
        "      task_type=\"CAUSAL_LM\",\n",
        "      target_modules=[\"q_proj\",\"k_proj\",\"v_proj\"]\n",
        "  )\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
        "  tokenizer.padding_side = \"right\"\n",
        "  return model , peft_config , tokenizer\n",
        "model , peft_config , tokenizer = create_and_prepare_model()"
      ],
      "metadata": {
        "id": "FMIC29nsYtGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fine tuning\n",
        "from trl import SFTConfig , SFTTrainer\n",
        "args = SFTConfig(\n",
        "    output_dir = \"./llama32_finetuned_code_generation-python\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True, # to save memory\n",
        "    optim = \"adamw_torch_fused\",\n",
        "    logging_steps=50,\n",
        "    learning_rate=2e-4,\n",
        "    bf16=True, # better for training stability in comparsion to fp16 . can't use tf32 as we don't have ampere GPU\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to = \"tensorboard\",\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    dataset_text_field=\"text\",\n",
        "    eval_strategy = \"steps\",\n",
        "    eval_steps = 50,\n",
        "    save_strategy = \"epoch\"\n",
        ")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=dataset_train,\n",
        "    eval_dataset=dataset_val,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=tokenizer\n",
        "    )\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "PuOwnD7ypGY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save model\n",
        "model_file_name = \"llama32_finetuned_code_generation-python.pth\"\n",
        "torch.save(model.state.dict(),model_file_name)\n",
        "print(f\"Model saved to {model_file_name}\")\n",
        "\n",
        "#free the memory\n",
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8IKJ0VkhtikK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load fine tuned model and run inference\n",
        "from peft import PeftModel , LoraConfig\n",
        "from transformers import AutoModelForCasualLM , AutoTokenizer\n",
        "import torch\n",
        "\n",
        "def load_fine_tune_model(base_model_id,saved_weights):\n",
        "  #load tokenizer and base model\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "  base_model = AutoModelForCasualLM.from_pretrained(\n",
        "      base_model_id,\n",
        "      load_in_4bit=True,\n",
        "      device_map=\"auto\",\n",
        "      torch_dtype=torch.bfloat16\n",
        "  )\n",
        "  base_model.to(device)\n",
        "  #create LoRA Config - make sure these parameters match your training configuration\n",
        "  peft_config = LoraConfig(\n",
        "      lora_alpha=16,\n",
        "      lora_dropout=0.05,\n",
        "      r=8,\n",
        "      bias=\"none\",\n",
        "      task_type=\"CAUSAL_LM\",\n",
        "      target_modules=[\"q_proj\",\"k_proj\",\"v_proj\"]\n",
        "  )\n",
        "  #Initialize PeftModel\n",
        "  lora_model = PeftModel(base_model,peft_config)\n",
        "  #load the saved_weights\n",
        "  state_dict = torch.load(saved_weights , map_location=device)\n",
        "\n",
        "  #create new dict\n",
        "  new_state_dict = {}\n",
        "  for k,v in state_dict.items():\n",
        "    if k.startswith(\"base_model.\"):\n",
        "      new_key = f\"base_{key}\"\n",
        "      new_state_dict[new_key] = v\n",
        "\n",
        "  #load the weights with strict = false to allow partial loading\n",
        "  lora_model.load_state_dict(new_state_dict,strict=False)\n",
        "  lora_model.eval()\n",
        "  return lora_model , tokenizer\n",
        "\n",
        "#Original model and saved_weight\n",
        "base_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "lora_weights = \"llama32_finetuned_code_generation-python.pth\"\n",
        "\n",
        "#load model\n",
        "print(\"Loading model\")\n",
        "model_ft , tokenizer = load_fine_tune_model(base_model_id,lora_weights)\n",
        "total_params = sum(p.numel() for p in model_ft.parameters())\n",
        "trainable_params = sum(p.numel() for p in model_ft.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters in the model: {total_params}\")\n",
        "print(f\"Trainable parameters in the model: {trainable_params}\")"
      ],
      "metadata": {
        "id": "svnRI2QRvF0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(model,prompt,tokenizer,max_new_tokens, context_size=512,temperature = 0.0 , top_k=1, eos_id=[128001,128009]):\n",
        "  \"\"\"Generate text using a language model with proper dtype handling and improved sampling\"\"\"\n",
        "  #Get Model's expected dtype\n",
        "  model_dtype = next(model.parameters()).dtype\n",
        "  model_device = next(model.parameters()).device\n",
        "\n",
        "  #user driven prompt can be very raw and hence we need to format the prompt\n",
        "  formatted_prompt = (\n",
        "      f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "      f\"Below is an instruction that describes a task.Write a response that appropriately completes the task.\\n\\n\"\n",
        "      f\"### Instruction:\\n{prompt}\\n\\n\"\n",
        "      f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" # model generate the response\n",
        "  )\n",
        "  formatted_prompt = \"\".join(formatted_prompt)\n",
        "  #encode and prepare input\n",
        "  idx = tokenizer.encode(formatted_prompt)\n",
        "  idx = torch.tensor(idx,dtype=torch.long, device = model_device).unsqueeze(0) #convert the prompt to tensor to give it to model\n",
        "  num_tokens = idx.shape[1]\n",
        "\n",
        "  #generation loop\n",
        "  for _ in range(max_new_tokens):\n",
        "    idx_cond = idx if idx.size(1) <= context_size else idx[:,-context_size:]\n",
        "    with torch.no_grad():\n",
        "      outputs = model(input_ids=idx_cond,use_cache = False)\n",
        "      logits = outputs.logits\n",
        "    logits = logits[:, -1, :] # focus on last time time step to get logits\n",
        "\n",
        "    #apply top-k filtering\n",
        "    if not top_k and top_k > 0:\n",
        "      top_logits, _ = torch.topk(logits,top_k) #pick top logits\n",
        "      min_val = top_logits[:,[-1]]\n",
        "      #make rest all logits to -inf as we don't need them\n",
        "      #we can constraint the logits that we want to produce....\n",
        "      #use case for constrainting the logits is - structuring the output , constrainting the tool/function list\n",
        "      logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\"), device = model_device , dtype = model_dtype) , logits)\n",
        "\n",
        "\n",
        "    #apply temperature and sample\n",
        "    if temperature > 0.0: # if temp is 1 keep logits as same . temp is way to magnify to pick high logits\n",
        "      logits = logits / temperature\n",
        "    #apply softmax\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "    else:\n",
        "      idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "    #check for EOS\n",
        "    if idx_next.item() in eos_id:\n",
        "      break\n",
        "    #append new token\n",
        "    idx = torch.cat((idx,idx_next), dim=1) #append new tokens every single step untill we reach max_new_tokens\n",
        "\n",
        "#decode generated text\n",
        "  generated_ids = idx.squeeze(0)[num_tokens:] #exclude the intial tokens of the prompt. num_tokens indicate the number of intial prompt tokens\n",
        "  generated_text = tokenizer.decode(generated_ids)\n",
        "  return generated_text"
      ],
      "metadata": {
        "id": "ZuING0rx-cM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\"Write a function to compute preorder traversal of a tree\")\n",
        "print(generate_prompt(model_ft,prompt,tokenizer,max_new_tokens=512))"
      ],
      "metadata": {
        "id": "k9FnVvroCy2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kuOk-oRb98JM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}