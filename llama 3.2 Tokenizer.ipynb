{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "766ed80f",
      "metadata": {
        "id": "766ed80f"
      },
      "outputs": [],
      "source": [
        "#implementing tolenizer for LLama 3.2\n",
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "# https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/standalone-llama32.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2a57a2d7",
      "metadata": {
        "id": "2a57a2d7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import tiktoken\n",
        "from tiktoken.load import load_tiktoken_bpe\n",
        "\n",
        "class Llama3Tokenizer:\n",
        "  \"\"\" Thin wrapper around tiktoken that keeps track of Llama -3.2 special IDs. \"\"\"\n",
        "  def __init__(self, model_path):\n",
        "    if not os.path.isfile(model_path):\n",
        "        raise FileNotFoundError(f\"Model path {model_path} does not exist.\")\n",
        "    mergeable =  load_tiktoken_bpe(model_path)\n",
        "\n",
        "    #hard-coded path from Meta's Tokenizer.json\n",
        "    self.special = {\n",
        "        \"<|begin_of_text|>\": 128000,\n",
        "        \"<|end_of_text|>\":128001,\n",
        "        \"<|start_header_id|>\": 128006,\n",
        "        \"<|end_header_id|>\": 128007,\n",
        "        \"<|eot_id|>\": 128009,\n",
        "        }\n",
        "    self.special.update({\n",
        "       f\"<|reserved_{i}|>\": 128002 + i for i in range(256) if 128002 + i not in self.special.values()})\n",
        "\n",
        "    self.model = tiktoken.Encoding(\n",
        "        name=Path(model_path).name,\n",
        "        path_str= r\"(?i:'s|'t|'re|'ve|'m|'ll|'d|)\"\n",
        "         r\"|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+\"\n",
        "         r\"|\\p{N}{1,3}\"\n",
        "         r\"|?[^s\\n\\p{L}\\p{N}]+[\\r\\n]*\"\n",
        "         r\"|\\s*[\\r\\n]\"\n",
        "         r\"|\\s+(?!S)\"\n",
        "         r\"|\\s+\",\n",
        "        mergeable_ranks=mergeable,\n",
        "        special_tokens=self.special,)\n",
        "\n",
        "    def encode(self, text , bos=False, eos=False , allowed_special = set()):\n",
        "        \"\"\" Encode text to a list of token IDs. \"\"\"\n",
        "        ids: list[int] = []\n",
        "\n",
        "        if bos:\n",
        "            ids.append(self.special_tokens[\"<|begin_of_text|>\"])\n",
        "        ids.extend(self.model.encode(text,allowed_special=allowed_special))\n",
        "        if eos:\n",
        "            ids.append(self.special_tokens[\"<|end_of_text|>\"])\n",
        "        return ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        \"\"\" Decode a list of token IDs to text. \"\"\"\n",
        "        return self.model.decode(token_ids)\n",
        "\n",
        "    class ChatFormat:\n",
        "       def __init__(self, tokenizer = Llama3Tokenizer, *,default_system = \"You are a helpful assistant.\"):\n",
        "              self.tokenizer = tokenizer\n",
        "              self.default_system = default_system\n",
        "\n",
        "       def _header(self,role):\n",
        "           \"\"\" Encode <|start_header_id|> role <|end_header_id|> \"\"\"\n",
        "           return (\n",
        "               [self.tokenizer.special_tokens[\"<|start_header_id|>\"]] +\n",
        "               self.tokenizer.encode(role) +\n",
        "               [self.tokenizer.special_tokens[\"<|end_header_id|>\"]] +\n",
        "               self.tokenizer.encode(\"\\n\\n\")\n",
        "           )\n",
        "       def encode(self,user_message,system_message = None,allowed_special = None):\n",
        "            \"\"\" Encode a user message with optional system message. \"\"\"\n",
        "            sys_msg = system_message if system_message is not None else self.default_system\n",
        "            ids = [self.tokenizer.special_tokens[\"<|begin_of_text|>\"]]\n",
        "            #system\n",
        "            ids+= self._header(\"system\")\n",
        "            ids+= self.tokenizer.encode(sys_msg,allowed_special=allowed_special)\n",
        "            ids+= self.tokenizer.special_tokens[\"<|eot_id|>\"]\n",
        "\n",
        "            #user\n",
        "            ids+= self._header(\"user\")\n",
        "            ids+= self.tokenizer.encode(user_message)\n",
        "            ids+= self.tokenizer.special_tokens[\"<|eot_id|>\"]\n",
        "\n",
        "            #assitant header (no content yet)\n",
        "            ids+= self._header(\"assistant\")\n",
        "            return ids\n",
        "    def decode(self,token_ids):\n",
        "            \"\"\" Decode token IDs to a user message, removing special tokens. \"\"\"\n",
        "            return self.tokenizer.decode(token_ids)\n",
        "def clean_text(text, header_end = \"assistant<|end_header_id|>\\n\\n\"):\n",
        "    \"\"\" Find the index of the first occurence of header_end and remove all special tokens. \"\"\"\n",
        "    index = text.find(header_end)\n",
        "    if index != -1:\n",
        "        text = text[index + len(header_end):].strip() #strip removes leading/trailing whitespace\n",
        "    else:\n",
        "        return text"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}