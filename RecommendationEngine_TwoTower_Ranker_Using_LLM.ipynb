{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMBS415IEUusW3fK3DsYrz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/RecommendationEngine_TwoTower_Ranker_Using_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9AiZcmsP_yG"
      },
      "outputs": [],
      "source": [
        "#feature computing\n",
        "#credit - >https://github.com/decodingml/personalized-recommender-course/blob/main/notebooks/1_fp_computing_features.ipynb\n",
        "#https://medium.com/data-science-collective/4-deploy-scalable-tiktok-like-recommenders-bdf117c55648\n",
        "#https://medium.com/data-science-collective/using-llms-to-build-tiktok-like-recommenders-bd001c1329d2\n",
        "#in two tower networks important point is that each of that the two tower neural nets can have any data set to either make then use Collaborative Filtering or Content Filtering\n",
        "#if we add interaction features in item or user tower then we orient TTF to content based filtering aspect\n",
        "#the two towers that have been trained are used at online inference time\n",
        "#the retrieval towers are trained so that user and candidate items are in nearby embedding space. THAT IS THE OBJECTIVE OF TWO TOWER\n",
        "#once embeddings trained thru tower then candidate items are found by using these embeddings and these candidate items are stored in a Vector DB\n",
        "#when new user comes at real time / online - then user embeddings are generated then a similarity search candidate Item embeddings and a list of item is generated\n",
        "#this list of items is given to online ranker model to rank items for user. the ranker model can use extra features as well"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "notebook_start_time = time.time()"
      ],
      "metadata": {
        "id": "QsPViSstYgpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setup enviornment\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def is_google_colab() ->bool:\n",
        "  if \"google.colab\" in str(get_ipython()):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def clone_repository() ->None:\n",
        "  !git clone https://github.com/decodingml/hands-on-recommender-system.git\n",
        "  %cd hands-on-recommender-system\n",
        "\n",
        "def install_dependencies() ->None:\n",
        "  !pip install --upgrade uv\n",
        "  !uv pip install --all-extras --system --requirement pyproject.toml\n",
        "\n",
        "if is_google_colab():\n",
        "  clone_repository()\n",
        "  install_dependencies()\n",
        "\n",
        "  root_dir = str(Path().absolute())\n",
        "  print(\"Colab Enviornment\")\n",
        "else:\n",
        "  root_dir = str(Path().absolute().parent)\n",
        "  print(\"Local Enviornment\")\n",
        "\n",
        "#add the root directory to the 'PYPYTHONPATH' to use the 'recys' Python module from the notebook.\n",
        "\n",
        "if root_dir not in sys.path:\n",
        "  print(f\"adding the following directory to the PYTHONPATH:{root_dir}\")\n",
        "  sys.path.append(root_dir)\n"
      ],
      "metadata": {
        "id": "DSlUvkNVYm21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.19.0"
      ],
      "metadata": {
        "id": "1ukn9ZBQlTgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install a compatible tensorflow version to resolve dependency conflict\n",
        "!pip install loguru\n",
        "!pip install hopsworks\n",
        "!pip install langchain\n",
        "!pip install langchain-openai\n",
        "!pip install catboost\n",
        "!pip install tensorflow_recommenders"
      ],
      "metadata": {
        "id": "4beK4Xg2e-Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade recsys"
      ],
      "metadata": {
        "id": "xkEzZguzzmnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  pip show recsys"
      ],
      "metadata": {
        "id": "ycS72BP51dsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature pipeline computing features\n",
        "#imports\n",
        "#%load_ext autoreload\n",
        "#%autoreload 2\n",
        "\n",
        "# Workaround for imp module error in autoreload\n",
        "try:\n",
        "    from IPython.extensions import autoreload\n",
        "    get_ipython().run_line_magic('load_ext', 'IPython.extensions.autoreload')\n",
        "    get_ipython().run_line_magic('autoreload', '2')\n",
        "except ImportError:\n",
        "    print(\"Could not load autoreload extension.\")\n",
        "\n",
        "import warnings\n",
        "from pprint import pprint\n",
        "\n",
        "import polars as pl\n",
        "import torch\n",
        "from loguru import logger\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from recsys import hopsworks_integration\n",
        "from recsys.config import settings\n",
        "from recsys.features.articles import(\n",
        "    compute_features_articles,\n",
        "    generate_embeddings_for_dataframe\n",
        ")\n",
        "\n",
        "\n",
        "from recsys.features.customers import DatasetSampler , compute_features_customers\n",
        "from recsys.features.interaction import generate_interaction_data\n",
        "from recsys.features.ranking import compute_ranking_dataset\n",
        "from recsys.features.transactions import compute_features_transactions\n",
        "from recsys.hopsworks_integration import feature_store\n",
        "from recsys.raw_data_sources import h_and_m as h_and_m_raw_data\n"
      ],
      "metadata": {
        "id": "tnY6c3-5adfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5077390e"
      },
      "source": [
        "pprint(dict(settings))\n",
        "DatasetSampler.get_supported_sizes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f2eb01c"
      },
      "source": [
        "#connect to Hopsworks feature store\n",
        "project , fs = hopsworks_integration.get_feature_store()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24660e0a"
      },
      "source": [
        "#H&M dataset\n",
        "articles_df = h_and_m_raw_data.extract_articles_df()\n",
        "articles_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79b81406"
      },
      "source": [
        "articles_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_df.null_count()\n",
        "#only one null record"
      ],
      "metadata": {
        "id": "FiOw92p9pXLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_df = compute_features_articles(articles_df)\n",
        "articles_df.shape"
      ],
      "metadata": {
        "id": "xZgmKh7xpf8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_df.head(3)"
      ],
      "metadata": {
        "id": "Qv9LS07epqjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create embeddings from the articles description\n",
        "for i , desc in enumerate(articles_df['article_description'].head(n=3)):\n",
        "  logger.info(f\"Item{i+1}:\\n{desc}\")"
      ],
      "metadata": {
        "id": "E-8acMUsq0Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate embeddings for articles\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "logger.info(f\"loading '{settings.FEATURES_EMBEDDING_MODEL_ID}' embedding model to {device=}\")\n",
        "embedding_model = SentenceTransformer(settings.FEATURES_EMBEDDING_MODEL_ID, device=device)\n",
        "#generate embeddings for articles\n",
        "articles_df = generate_embeddings_for_dataframe(articles_df,\"article_description\",embedding_model,batch_size = 128)"
      ],
      "metadata": {
        "id": "eKPHEc94sYeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_df[[\"article_description\",\"embeddings\"]].head(3)"
      ],
      "metadata": {
        "id": "mea_SFtuuSnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Customer Data\n",
        "customers_df = h_and_m_raw_data.extract_customers_df()\n",
        "customers_df.shape"
      ],
      "metadata": {
        "id": "v6z0rl2Xukht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customers_df.head(3)\n",
        "customers_df.null_count()"
      ],
      "metadata": {
        "id": "ppM9m88Y3whM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customers_df = compute_features_customers(customers_df,drop_null_age=True)\n",
        "customers_df.shape"
      ],
      "metadata": {
        "id": "PP-7CUvy3656"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transactions Data\n",
        "transactions_df = h_and_m_raw_data.extract_transactions_df()\n",
        "transactions_df.shape"
      ],
      "metadata": {
        "id": "rUQWCNE94HrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df.head(3)"
      ],
      "metadata": {
        "id": "8XNQgMZJ4M6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transactions feature engineering\n",
        "transactions_df = compute_features_transactions(transactions_df)\n",
        "transactions_df.shape"
      ],
      "metadata": {
        "id": "9A-lXGVp5_mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sampling smaller data set as Transactions has 30M transactions\n",
        "sampler = DatasetSampler(size=settings.CUSTOMER_DATA_SIZE)\n",
        "dataset_subset = sampler.sample(customers_df = customers_df ,transations_df = transactions_df)\n",
        "customers_df = dataset_subset[\"customers_df\"]\n",
        "transactions_df = dataset_subset[\"transactions_df\"]"
      ],
      "metadata": {
        "id": "7XaYVCzP6OAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#interaction data as we need interaction of customer with transactions\n",
        "interaction_df = generate_interaction_data(transactions_df)\n",
        "interaction_df.shape"
      ],
      "metadata": {
        "id": "dKP27HEC6xaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interaction_df.head(3)"
      ],
      "metadata": {
        "id": "8UVfEUPo64Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#look at interaction score distribution\n",
        "interaction_df.groupby(\"interaction_score\").agg(pl.count(\"interaction_score\").alias(\"total_interactions\"))\n",
        "#0 - not interaction\n",
        "#1 - A customer clicked on item\n",
        "#2 - A customer bought an item"
      ],
      "metadata": {
        "id": "7_gy-lfs65x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create hopworks feature groups\n",
        "logger.info(\"Uploading 'customers' Feature Group to Hopsworks.\")\n",
        "customers_fg = feature_store.create_customers_feature_group(\n",
        "    fs,\n",
        "    df=customers_df,\n",
        "    customers_age_embedding_dim=model.get_sentence_embedding_dimension(),\n",
        "    online_enabled=True,\n",
        ")"
      ],
      "metadata": {
        "id": "lptkS2Zk7vpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for Articles\n",
        "logger.info(\"Uploading 'articles' Feature Group to Hopsworks.\")\n",
        "articles_fg = feature_store.create_articles_feature_group(\n",
        "    fs,\n",
        "    df=articles_df,\n",
        "    articles_embedding_dim=embedding_model.get_sentence_embedding_dimension(),\n",
        "    online_enabled=True,\n",
        ")"
      ],
      "metadata": {
        "id": "cycYbyEa7-1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transactions\n",
        "logger.info(\"Uploading 'transactions' Feature Group to Hopsworks.\")\n",
        "transactions_fg = feature_store.create_transactions_feature_group(\n",
        "    fs,\n",
        "    df=transactions_df,\n",
        "    online_enabled=True,\n",
        ")"
      ],
      "metadata": {
        "id": "JC9t4MtQ8O6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#interactions\n",
        "logger.info(\"Uploading 'interactions' Feature Group to Hopsworks.\")\n",
        "interactions_fg = feature_store.create_interactions_feature_group(\n",
        "    fs,\n",
        "    df=interaction_df,\n",
        "    online_enabled=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Rcpr9u_A8rRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute Ranking dataset\n",
        "ranking_df = compute_ranking_dataset(transactions_fg,customers_fg,articles_fg)\n",
        "ranking_df.shape"
      ],
      "metadata": {
        "id": "IXUacDhK8wiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranking_df.head(3)"
      ],
      "metadata": {
        "id": "8LeTuRVe88rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranking_df.get_column_names(\"label\").value_counts()"
      ],
      "metadata": {
        "id": "esOPuisW9BHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading this dataset to hopworks\n",
        "logger.info(\"Uploading 'ranking' Dataset to Hopsworks.\")\n",
        "ranking_ds = feature_store.create_ranking_dataset(\n",
        "    fs,\n",
        "    df=ranking_df,\n",
        "    parents =[articles_fg,customers_fg,transactions_fg],\n",
        "    online_enabled=True,\n",
        ")"
      ],
      "metadata": {
        "id": "dBSGbsa19Ohd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MOVING TO NOW CODE RETRIEVAL PIPELINE\n",
        "from recsys import hopworks_integration , training\n",
        "from recsys.config import settings\n",
        "from pprint import pprint\n",
        "\n",
        "pprint(dict(settings))"
      ],
      "metadata": {
        "id": "5T8kHVv1DfkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to hopworks feature store\n",
        "project , fs = hopsworks_integration.get_feature_store()\n",
        "#"
      ],
      "metadata": {
        "id": "qMKs_1MrD3nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create hopworks feature views\n",
        "feature_view = hopsworks_integration.feature_store.create_retrieval_feature_view(fs)"
      ],
      "metadata": {
        "id": "FgPcstmbD75K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a training data set for training retrieval two tower neural network\n",
        "#there are two towers\n",
        "#customer tower independent of articles tower\n",
        "#item tower\n",
        "\n",
        "#for training customer tower we will use following data set\n",
        "#ID , age , month_sin and month_cos(indicating purchase month)\n",
        "\n",
        "#for article tower we will use\n",
        "#article_id,garment_group_name , index_group_name\n",
        "\n",
        "dataset = training.two_tower.TwoTowerDataset(\n",
        "    feature_view = feature_view, batch_size = settings.TWO_TOWER_MODEL_BATCH_SIZE\n",
        ")\n",
        "train_ds , val_ds = dataset.get_train_val_split()\n",
        "\n",
        "#let's look at the dataset\n",
        "dataset.properties[\"train_df\"].head()\n"
      ],
      "metadata": {
        "id": "Si6qqfzSEYIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating two towers 1 - customer 2 articles - we will be using hopworks for this\n",
        "\n",
        "query_model_factory = training.two_tower.QueryTowerFactory(dataset=dataset)\n",
        "query_model = query_model_factory.build()\n",
        "\n",
        "item_model_factory = training.two_tower.ItemTowerFactory(dataset=dataset)\n",
        "item_model = item_model_factory.build()\n",
        "\n",
        "model_factory = training.two_tower.TwoTowerModelFactory(dataset = dataset)\n",
        "model = model_factory.build(query_model,item_model)\n",
        "\n",
        "#training the model\n",
        "trainer = training.two_tower.TwoTowerTrainer(dataset = dataset , model=model)\n",
        "history = trainer.train(train_ds,val_ds)\n"
      ],
      "metadata": {
        "id": "LPaivAINIBBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the training loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#creating figure with two subplots\n",
        "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(10,6))\n",
        "\n",
        "#Training Loss Subplot\n",
        "ax1.plot(history.history[\"loss\"] , label = \"Training Loss\", color = \"blue\")\n",
        "ax1.set_title(\"Training Loss over time\")\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Loss\")\n",
        "ax1.legend()\n",
        "as1.grid(True)\n",
        "\n",
        "#Validation Loss Subplot\n",
        "ax2.plot(history.history[\"val_loss\"] , label = \"Validation Loss\", color = \"red\")\n",
        "ax2.set_title(\"Validation Loss over time\")\n",
        "ax2.set_xlabel(\"Epoch\")\n",
        "ax2.set_ylabel(\"Loss\")\n",
        "ax2.legend()\n",
        "as2.grid(True)\n",
        "\n",
        "#\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nRf3U6F1I6gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#push the models to hopsworks registry\n",
        "m = project.get_model_registry()\n",
        "\n",
        "query_model = hopsworks_integration.two_tower_serving.HopsworksQueryModel(model = model.query_model)\n",
        "query_model.register(\n",
        "    mr= mr,\n",
        "    feature_view = feature_view,\n",
        "    query_df = dataset.properties[\"query_df\"]\n",
        ")\n",
        "\n",
        "\n",
        "item_model = hopsworks_integration.two_tower_serving.HopsworksCandidate(model = model.item_model)\n",
        "item_model.register(\n",
        "    mr= mr,\n",
        "    feature_view = feature_view,\n",
        "    item_df = dataset.properties[\"query_df\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "iMgBlf4DKX3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ranking model\n",
        "#get the training data for ranking model\n",
        "feature_view_ranking = hopsworks_integration.feature_store.create_ranking_feature_view(fs)\n",
        "\n",
        "X_train, X_val, y_train, y_val = feature_view_ranking.train_test_split(\n",
        "    test_size = settings.RANKING_DATASET_VALIDATION_SPLIT_SIZE,\n",
        "    description = \"Ranking Training Dataset\"\n",
        ")\n",
        "X_train.head(3)\n",
        "y_train.head(3)"
      ],
      "metadata": {
        "id": "VXL0lY8LMRb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the ranking model - Catboost is the ranker model\n",
        "model = training.ranking.RankingModelFactory.build()\n",
        "trainer = training.ranking.RankingTrainer(model = model , train_dataset = (X_train,y_train) , eval_dataset = (X_val,y_val))\n",
        "trainer.fit()"
      ],
      "metadata": {
        "id": "hwnAcN9VNEsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the ranking model\n",
        "metrics = trainer.evaluate(log=True)\n",
        "trainer.get_feature_importance()\n",
        "mr = project.get_model_registry()\n",
        "\n",
        "ranking_model = hopsworks_integration.ranking_serving.HopsworksRankingModel(model = model)\n",
        "ranking_model.register(mr, feature_view_ranking, X_train,metrics)"
      ],
      "metadata": {
        "id": "mdIvH6tRNo6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using LLM for ranking\n",
        "import logging\n",
        "import hopsworks\n",
        "from langchain import PromptTemplate , LLMChain\n",
        "from langchain_core.output_parsers import BaseOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "class ScoreOutputParser(BaseOutputParser[float]):\n",
        "\n",
        "  def parse(self,output) ->float:\n",
        "      text = output['text']\n",
        "      if \"Probability:\" not in text:\n",
        "        raise ValueError(\"Text doesn't not contain probability:'  label\")\n",
        "      probability_str = text.split(\"Probability:\")[1].strip()\n",
        "      probability = float(probability_str)\n",
        "      #ensure the probability is in the valid range[0,1]\n",
        "      if not(0.0<=probability<=1.0):\n",
        "        raise ValueError(f\"Invalid probability:{probability}\")\n",
        "      return probability\n",
        "\n",
        "  PROMPT_TEMPLATE: str = \"\"\"\n",
        "      You are a helpful assistant specialized in predicting customer behavior . Your task is to analyze the features of product and predict the probability of it being purchased by customer\n",
        "\n",
        "      ###Instructions:\n",
        "      1.Use the provided features of the product to make your prediction.\n",
        "      2.Consider the following numeric and categorical features:\n",
        "      -Numeric features: These are quantitaive attributes , such as numerical identifiers or measurements.\n",
        "      -Categorical features: These describe qualitative aspects , like product category , color, and material\n",
        "      3.Your response should only include the probability of purchase for the positive class(e.g likelihood of being purchased) , as a value between 0 and 1.\n",
        "\n",
        "       ### Product and User Features:\n",
        "            Numeric features:\n",
        "            - Age: {age}\n",
        "            - Month Sin: {month_sin}\n",
        "            - Month Cos: {month_cos}\n",
        "\n",
        "            Categorical features:\n",
        "            - Product Type: {product_type_name}\n",
        "            - Product Group: {product_group_name}\n",
        "            - Graphical Appearance: {graphical_appearance_name}\n",
        "            - Colour Group: {colour_group_name}\n",
        "            - Perceived Colour Value: {perceived_colour_value_name}\n",
        "            - Perceived Colour Master Value: {perceived_colour_master_name}\n",
        "            - Department Name: {department_name}\n",
        "            - Index Name: {index_name}\n",
        "            - Department: {index_group_name}\n",
        "            - Sub-Department: {section_name}\n",
        "            - Group: {garment_group_name}\n",
        "\n",
        "            ### Your Task:\n",
        "            Based on the features provided, predict the probability that the customer will purchase this product to 4-decimals precision. Provide the output in the following format:\n",
        "            Probability:\n",
        "        \"\"\"\n",
        "  class Predict(object):\n",
        "    def __init__(self):\n",
        "        self.input_features = [\"age\",\"month_sin\",\"month_cos\",\"product_type_name\",\"product_group_name\",\"graphical_appearance_name\",\"colour_group_name\",\n",
        "                              \"perceived_colour_value_name\",\"perceived_colour_master_name\",\"department_name\",\"index_name\",\"index_group_name\",\n",
        "                              \"section_name\",\"garment_group_name\"]\n",
        "        self._retrieve_secrets()\n",
        "        self.LLM = self._build_lang_chain()\n",
        "        self.parser = ScoreOutputParser()\n",
        "\n",
        "    def _retrieve_secrets(self):\n",
        "        project = hopsworks.login()\n",
        "        secrets_api = hopsworks.get_secrets_api()\n",
        "        self.openai_api_key = secrets_api.get_secret(\"OPENAI_API_KEY\").value\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        #extract rank features\n",
        "        features = inputs[0].pop(\"ranking_features\")[:20]\n",
        "        articles_ids = inputs[0].pop(\"article_ids\")[:20]\n",
        "\n",
        "        #preprocess features for LLM model\n",
        "        preprocessed_feature_candidates = self._preprocess_features(features)\n",
        "\n",
        "        scores =[]\n",
        "        for candidates in preprocessed_feature_candidates:\n",
        "          try:\n",
        "            text = self.llm_invoke(candidate)\n",
        "            score = self.parser.parse(text)\n",
        "          except Exception as exception:\n",
        "            score = 0\n",
        "          scores.append(score)\n",
        "        return {\n",
        "            \"article_ids\":articles_ids,\n",
        "            \"scores\":scores\n",
        "        }\n",
        "\n",
        "    def _preprocess_features(self,features):\n",
        "      \"\"\" Convert ranking features into natural language \"\"\"\n",
        "      preprocessed = []\n",
        "      for feature_set in features:\n",
        "          query_parameters = {}\n",
        "          for key,value in zip(self.input_features,feature_set):\n",
        "            query_parameters[key] = value\n",
        "          preprocessed.append(query_parameters)\n",
        "      return preprocessed\n",
        "\n",
        "    def _build_lang_chain(self):\n",
        "            model = ChatOpenAI(\n",
        "            model_name='gpt-4o-mini-2024-07-18',\n",
        "            temperature=0.7,\n",
        "            openai_api_key=self.openai_api_key,)\n",
        "\n",
        "            prompt = PromptTemplate(\n",
        "                input_variables=self.input_features,\n",
        "                template=self.PROMPT_TEMPLATE,\n",
        "            )\n",
        "            langchain = LLMChain(\n",
        "                llm= model,\n",
        "                prompt=prompt,\n",
        "                verbose = True\n",
        "            )\n",
        "            return langchain"
      ],
      "metadata": {
        "id": "MOch3kBRiFjo"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a214297f",
        "outputId": "54fe917f-112c-4746-bde5-dc6d14be1736"
      },
      "source": [
        "# Example usage of the Predict class for LLM ranking\n",
        "\n",
        "# Instantiate the Predict class\n",
        "# This will attempt to retrieve the OpenAI API key and build the LangChain model\n",
        "try:\n",
        "    llm_ranker = Predict()\n",
        "    print(\"Predict class instantiated successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error instantiating Predict class: {e}\")\n",
        "    llm_ranker = None # Set to None if instantiation fails\n",
        "\n",
        "if llm_ranker:\n",
        "    # Prepare some sample input data\n",
        "    # Replace this with your actual ranking features and article IDs\n",
        "    sample_inputs = [\n",
        "        {\n",
        "            \"ranking_features\": [\n",
        "                [49.0, 0.707, 0.707, \"Tops\", \"Garment Upper Body\", \"Solid\", \"Black\", \"Dark\", \"Black\", \"Ladieswear\", \"Ladieswear\", \"Ladieswear\", \"Blouses\", \"Woven Tops\"],\n",
        "                [25.0, -0.707, 0.707, \"Shoes\", \"Shoes\", \"Metallic\", \"Silver\", \"Light\", \"Silver\", \"Divided\", \"Divided\", \"Divided\", \"Outdoor\", \"Outdoor\"],\n",
        "                # Add more sample feature sets as needed\n",
        "            ],\n",
        "            \"article_ids\": [\n",
        "                \"663713001\",\n",
        "                \"541518023\",\n",
        "                # Add corresponding article IDs\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Get predictions from the LLM ranker\n",
        "    try:\n",
        "        predictions = llm_ranker.predict(sample_inputs)\n",
        "        print(\"\\nPredictions:\")\n",
        "        print(predictions)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during prediction: {e}\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error instantiating Predict class: name 'Predict' is not defined\n"
          ]
        }
      ]
    }
  ]
}