{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPX5NoyNq59ZthK2nHdFGeL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Load_pre_trained_weights_from_GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FBh9ibLT8XEj"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow>=2.15.0 tqdm>=4.66"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tqdm as tqdm\n",
        "print(f\"Tensorflow version:\", tf.__version__)\n",
        "print(f\"tqdm version:\",tqdm.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TANOG0w8pM6",
        "outputId": "e6471211-cfd1-4b2f-e13c-bb34c8a22bef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version: 2.19.0\n",
            "tqdm version: 4.67.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download3 import download_and_load_gpt2\n",
        "settings , params = download_and_load_gpt2(model_size = \"124M\", model_dirs=\"gpt2\")\n",
        "print(\"Settings:\",settings)\n",
        "print(\"Parameter Disctionary Keys:\",param.keys()) #blocks , b,g,wpe,wte\n",
        "print(params[\"wte\"])\n",
        "print(\"Token embedding weight tensor dimesions:\", param[\"wte\"].shape)"
      ],
      "metadata": {
        "id": "T-1Lkyzr8_wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define model configurations in a dictionary for compactness\n",
        "model_confings ={\n",
        "    \"gpt-small (124M)\":{\"emb_dim\":768,\"n_layers\":12,\"n_heads\":12},\n",
        "    \"gpt-medium (355M)\":{\"emb_dim\":1024,\"n_layers\":24,\"n_heads\":16},\n",
        "    \"gpt-medium (774M)\":{\"emb_dim\":1280,\"n_layers\":36,\"n_heads\":20},\n",
        "    \"gpt-medium (1558M)\":{\"emb_dim\":1600,\"n_layers\":48,\"n_heads\":25},\n",
        "}\n",
        "#copy the base configuration and update with specific model settings\n",
        "model_name = \"gpt-small (124M)\" #example model name\n",
        "NEW_CONFIG = GPT_CONFIG_[model_name].copy()\n",
        "NEW_CONFIG.update(model_confings[model_name])\n",
        "print(NEW_CONFIG)"
      ],
      "metadata": {
        "id": "VZiqJxWKIar5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEW_CONFIG.update({\"context_length\":1024 , \"qkv_bias\": True})\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval();"
      ],
      "metadata": {
        "id": "Ue609AGfKVc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check whether two tensor values left and right have the same dimenions . if shape is matching return the tensor\n",
        "def assign(left , right):\n",
        "  if left.shape != right.shape:\n",
        "    raise ValueError(f\"Shape mismatch: {left.shape} != {right.shape}\")\n",
        "  return torch.nn.Parameter(torch.tensor(right)) # Class for holding parameters in tensor that are trainable ...."
      ],
      "metadata": {
        "id": "WGQ-13hOndxc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def load_weights_into_gpt(gpt,params):\n",
        "  gpt.pos_emb.weight = assign(gpt.pos_emb.weight,params['wpe'])\n",
        "  gpt.tok_emb.weight = assign(gpt.pos_emb.weight,params['wte'])\n",
        "\n",
        "  for b in range(len([\"blocks\"])):\n",
        "    q_w,q_k,q_v = np.split((params['blocks'][b]['attn']['c_attn'])[\"w\"],3,axis=-1) #weights are fusion of q,k and v hence we need to split\n",
        "    gpt.trf_blocks[b].attn.W_query.weight = assign(gpt.trf_blocks[b].attn.W_query.weight,q_w.T)\n",
        "    gpt.trf_blocks[b].attn.W_key.weight = assign(gpt.trf_blocks[b].attn.W_key.weight,k_w.T)\n",
        "    gpt.trf_blocks[b].attn.W_value.weight = assign(gpt.trf_blocks[b].attn.W_value.weight,v_w.T)\n",
        "\n",
        "    #bias\n",
        "    q_b,k_b,v_b = np.split((params['blocks'][b]['attn']['c_attn'])[\"b\"],3,axis=-1)\n",
        "    gpt.trf_blocks[b].attn.W_query.bias = assign(gpt.trf_blocks[b].attn.W_query.bias,q_b)\n",
        "    gpt.trf_blocks[b].attn.W_key.bias = assign(gpt.trf_blocks[b].attn.W_key.bias,k_b)\n",
        "    gpt.trf_blocks[b].attn.W_value.bias = assign(gpt.trf_blocks[b].attn.W_value.bias,v_b)\n",
        "\n",
        "    #projection\n",
        "    gpt.trf_blocks[b].attn.out_proj.weight = assign(gpt.trf_blocks[b].attn.out_proj.weight,params['blocks'][b]['attn']['c_proj'][\"w\"].T)\n",
        "    gpt.trf_blocks[b].attn.out_proj.bias = assign(gpt.trf_blocks[b].attn.out_proj.bias,params['blocks'][b]['attn']['c_proj'][\"b\"])\n",
        "\n",
        "    #layers\n",
        "    gpt.trf_blocks[b].ff.layers[0].weight = assign(gpt.trf_blocks[b].ff.layers[0].weight,params['blocks'][b]['mlp']['c_fc'][\"w\"].T)\n",
        "    gpt.trf_blocks[b].ff.layers[0].bias = assign(gpt.trf_blocks[b].ff.layers[0].bias,params['blocks'][b]['mlp']['c_fc'][\"b\"])\n",
        "    gpt.trf_blocks[b].ff.layers[2].weight = assign(gpt.trf_blocks[b].ff.layers[2].weight,params['blocks'][b]['mlp']['c_proj'][\"w\"].T)\n",
        "    gpt.trf_blocks[b].ff.layers[2].bias = assign(gpt.trf_blocks[b].ff.layers[2].bias,params['blocks'][b]['mlp']['c_proj'][\"b\"])\n",
        "\n",
        "    #norm 1\n",
        "    gpt.trf_blocks[b].norm1.scale = assign(gpt.trf_blocks[b].norm1.scale,params['blocks'][b]['ln_1']['g'])\n",
        "    gpt.trf_blocks[b].norm1.bias = assign(gpt.trf_blocks[b].norm1.bias,params['blocks'][b]['ln_1']['b'])\n",
        "\n",
        "    #norm 2\n",
        "    gpt.trf_blocks[b].norm2.scale = assign(gpt.trf_blocks[b].norm2.scale,params['blocks'][b]['ln_2']['g'])\n",
        "    gpt.trf_blocks[b].norm2.bias = assign(gpt.trf_blocks[b].norm2.bias,params['blocks'][b]['ln_2']['b'])\n",
        "\n",
        "    #final scale and norm outside of transformer block\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale,params['g'])\n",
        "    gpt.final_norm.shift = assign(gpt.final_shift.shift,params['b'])\n",
        "    #linear output later is also a NN\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight,params['wte']) #weight tying\n"
      ],
      "metadata": {
        "id": "MQa9AkCfGxta"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_weights_into_gpt(gpt,params) #load params into the GPT model that we created\n",
        "gpt.to(device);"
      ],
      "metadata": {
        "id": "mgyl7ojEsoWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "token_ids = generate(\n",
        "    model = gpt\n",
        "    idx = text_to_token_ids(\"Every Effort Moves you\",tokenizer).to(device)\n",
        "    max_new_tokens = 25\n",
        "    context_size = NEW_CONFIG[\"context_length\"],\n",
        "    top_k = 50, # 50 tokens have opportunity for maximum new generated tokens\n",
        "    temperature = 1.5\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\",token_ids_to_text(token_ids,tokenizer))\n"
      ],
      "metadata": {
        "id": "vuHln-I1s-eb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}