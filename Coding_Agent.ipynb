{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiRjX/QGFZ70s920zwe2ZR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Coding_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "id": "U3b4RPt9WcuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code for FIM\n",
        "import numpy as np\n",
        "import functools\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PhfP8aGbX0TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkpT0yIiChf0"
      },
      "outputs": [],
      "source": [
        "#THIS CODE IS FOR CODING Generation\n",
        "import gc\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from typing import Optional\n",
        "from dataclasses import dataclass , field\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import IterableDataset\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        "    Trainer,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model,prepare_model_for_kbit_training,replace_lora_weights_loftq\n",
        "from trl import SFTTrainer\n",
        "import fim\n",
        "\n",
        "#define and parse arguments\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "  \"\"\"\n",
        "  Arguments pertaining to which model /config/tokenizer we will be using for fine tuning\n",
        "  \"\"\"\n",
        "  model_name_or_path: str = field(\n",
        "      metadata = {\n",
        "          \"help\":\"Path to pretrained model or model identifier from huggingface.co/models\"\n",
        "      }\n",
        "\n",
        "  )\n",
        "  lora_alpha:Optional[int] = field(default = 16)\n",
        "  lora_dropout:Optional[float] = field(default = 0.1)\n",
        "  lora_r:Optional[int] = field(default = 64)\n",
        "  lora_target_modules:Optional[str] = field(\n",
        "      default = \"q_proj,v_proj,k_proj,o_proj,down_proj,up_proj,gate_proj\",\n",
        "      metadata = {\n",
        "          \"help\":\"Comma separated list of modules to apply Lora to\"\n",
        "      }\n",
        "      )\n",
        "  use_nested_quant:Optional[bool] = field(default = False, metadata = {\"help\":\"Activate nested quantization for 4bit base models\"})\n",
        "  bnb_4bit_compute_dtype:Optional[str] = field(default = \"float16\", metadata = {\"help\":\"Compute dtype for 4bit base models\"})\n",
        "  bnb_4bit_quant_type:Optional[str] = field(default = \"nf4\", metadata = {\"help\":\"Quantization type fp4 or nf4 for 4bit base models\"})\n",
        "  bnb_4bit_use_double_quant:Optional[bool] = field(default = True, metadata = {\"help\":\"Use double quantization for 4bit base models\"})\n",
        "  use_flash_attn:Optional[str] = field(default=\"nf4\",metadata={\"help\":\"Enables Flash attention for training.\"})\n",
        "  use_peft_lora:Optional[bool] = field(default=True, metadata={\"help\":\"Enables LoRA for training.\"})\n",
        "  use_8bit_quantization:Optional[bool] = field(default=True, metadata={\"help\":\"Enables 8bit  for training.\"})\n",
        "  use_4bit_quantization:Optional[bool] = field(default=False, metadata={\"help\":\"Enables 4bit for training.\"})\n",
        "  use_reentrant:Optional[bool] = field(default = False,metadata={\"help\":\"Gradient Checkpointing param.\"})\n",
        "  use_unsloth:Optional[bool] = field(default = False,metadata={\"help\":\"Enables Unsloth for training .\"})\n",
        "  use_loftq:Optional[bool] = field(default = False ,matadata={\"help\":\"Enables LoftQ init for Lora Adapters\"} )\n",
        "  use_loftq_callback:Optional[bool] = field(default = False ,matadata={\"help\":\"Enables LoftQ callback comparing logits of base model to the ones from LoftQ\"})\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "  \"\"\"\n",
        "  Arguments pertaining to what data we are going to input our model for training and eval\n",
        "  \"\"\"\n",
        "  dataset_name:Optional[str] = field(\n",
        "      default = \"smangrul/hug_stack\",\n",
        "      metadata = {\n",
        "          \"help\":\"The name of the dataset to use (via the datasets library)\"\n",
        "      }\n",
        "  )\n",
        "  dataset_text_field:str = field(\n",
        "      default = \"text\",\n",
        "      metadata = {\n",
        "          \"help\":\"The name of the column in the datasets containing the full texts (for summarization)\"\n",
        "      }\n",
        "  )\n",
        "  max_seq_length:Optional[int] = field(default = 4096)\n",
        "  text_size:Optional[float] = field(default = 0.1)\n",
        "  fim_rate :Optional[float] = field(default = 0.5)\n",
        "  fim_spm_rate:Optional[float] = field(default = 0.5)\n",
        "  splits: Optional[str] = field(\n",
        "      default = \"train\",\n",
        "      metadata = {\n",
        "          \"help\":\"The dataset splits used for training\"\n",
        "      }\n",
        "  )\n",
        "#define chars per token so that it helps to\n",
        "def chars_token_ratio(dataset,tokenizer,data_column,nb_examples=400):\n",
        "  \"\"\" Estimate the average number of characters per token in the dataset\"\"\"\n",
        "  total_characters , total_token = 0,0\n",
        "  for _,example in tqdm(zip(range(nb_examples),iter(dataset)),total=nb_examples):\n",
        "    total_characters += len(example[data_column])\n",
        "    total_token += len(tokenizer(example[data_column]).tokens())\n",
        "  return total_characters/total_token\n",
        "\n",
        "class ConstantLengthDataset(IterableDataset):\n",
        "  \"\"\"\n",
        "  Iterable dataset that returns constant length chunks of tokens from the input dataset\n",
        "  Args:\n",
        "  tokenizer\n",
        "  dataset\n",
        "  infinite\n",
        "  seq_length\n",
        "  num_of_sequences\n",
        "  chars_per_token\n",
        "  fim_rate(float)\n",
        "  fim_spm_rate\n",
        "  seed\n",
        "  \"\"\"\n",
        "  def __init__(self,tokenizer,dataset,infinite= False,seq_length=1024,num_of_sequences=1024,chars_per_token=3.6,content_field=\"content\",fim_rate=0.5,\n",
        "               fim_spm_rate=0.5,seed=0,shuffle=False):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.concat_token_id = tokenizer.eos_token_id\n",
        "    self.dataset = dataset\n",
        "    self.infinite = infinite\n",
        "    self.seq_length = seq_length\n",
        "    self.current_size = 0\n",
        "    self.max_buffer_size = seq_length*chars_per_token*num_of_sequences\n",
        "    self.content_field = content_field\n",
        "    self.fim_rate = fim_rate\n",
        "    self.fim_spm_rate = fim_spm_rate\n",
        "    self.seed = seed\n",
        "    self.shuffle = shuffle\n",
        "    self.num_of_sequences = num_of_sequences\n",
        "    (\n",
        "     self.bos_token_id,\n",
        "     self.suffix_tok_id,\n",
        "     self.prefix_tok_id,\n",
        "     self.middle_tok_id,\n",
        "     self.pad_tok_id,\n",
        "    ) = fim.get_fim_token_ids(self.tokenizer)\n",
        "    if not self.suffix_tok_id and self.fim_rate>0:\n",
        "      print(\"FIM is not supported by tokenizer , disabling FIM\")\n",
        "      self.fim_rate = 0\n",
        "  def __iter__(self):\n",
        "    iterator = iter(self.dataset)\n",
        "    more_examples = True\n",
        "    np_rng = np.random.default_rng(self.seed)\n",
        "    while more_examples:\n",
        "      buffer,current_length = [],0\n",
        "      while True:\n",
        "        if buffer_len > self.max_buffer_size:\n",
        "          break\n",
        "          try:\n",
        "            buffer.append(next(iterator))[self.content_field]\n",
        "            buffer_len += len(buffer[-1])\n",
        "          except StopIteration:\n",
        "            if self.infinite:\n",
        "              iterator = iter(self.dataset)\n",
        "            else:\n",
        "              more_examples = False\n",
        "              break\n",
        "      tokenized_inputs = self.tokenizer(buffer,truncation=False,add_special_tokens=False)[\"input_ids\"]\n",
        "      all_token_ids = []\n",
        "      for tokenized_input in tokenized_inputs:\n",
        "        if self.fim_rate>0:\n",
        "          tokenized_input , np_rng = fim.permute(\n",
        "              tokenized_input,\n",
        "              np_rng,\n",
        "              self.suffix_tok_id,\n",
        "              self.prefix_tok_id,\n",
        "              self.middle_tok_id,\n",
        "              self.pad_tok_id,\n",
        "              fim_rate = self.fim_rate,\n",
        "              fim_spm_rate = self.fim_spm_rate,\n",
        "              truncate_or_pad = False,\n",
        "              bos_token_id = self.bos_token_id,\n",
        "          )\n",
        "        all_token_ids.extend(tokenized_input+[self.concat_token_id])\n",
        "        examples = []\n",
        "        for i in range(0,len(all_token_ids),self.seq_length):\n",
        "          input_ids = all_token_ids[i:i+self.seq_length]\n",
        "          if len(input_ids) == self.seq_length:\n",
        "            examples.append(input_ids)\n",
        "        if self.shuffle:\n",
        "          random.shuffle(examples)\n",
        "        for example in examples:\n",
        "          self.current_size +=1\n",
        "          yield {\"input_ids\":torch.LongTensor(example),\n",
        "                 \"labels\":torch.LongTensor(example)}\n",
        "\n",
        "def create_datasets(tokenier,args,seed):\n",
        "  dataset = load_dataset(args.dataset_name,split=args.splits)\n",
        "  dataset = dataset.train_test_split(test_size=args.text_size,seed=seed , shuffle = True)\n",
        "  train_data = dataset[\"train\"]\n",
        "  valid_data = dataset[\"test\"]\n",
        "  print(f\"size of the training dataset is {len(train_data)}\")\n",
        "  print(f\"size of the validation dataset is {len(valid_data)}\")\n",
        "  chars_per_token = chars_token_ratio(train_data,tokenizer,args.dataset_text_field)\n",
        "  print(f\"average number of characters per token is {chars_per_token:.2f}\")\n",
        "  train_dataset = ConstantLengthDataset(\n",
        "      tokenizer,\n",
        "      train_data,\n",
        "      infinite = True,\n",
        "      seq_length = args.max_seq_length,\n",
        "      chars_per_token = chars_per_token,\n",
        "      content_field = args.dataset_text_field,\n",
        "      fim_rate = args.fim_rate,\n",
        "      fim_spm_rate = args.fim_spm_rate,\n",
        "      seed = seed,\n",
        "      shuffle = True,\n",
        "      )\n",
        "  valid_dataset = ConstantLengthDataset(\n",
        "      tokenizer,\n",
        "      valid_data,\n",
        "      infinite = False,\n",
        "      seq_length = args.max_seq_length,\n",
        "      chars_per_token = chars_per_token,\n",
        "      content_field = args.dataset_text_field,\n",
        "      fim_rate = args.fim_rate,\n",
        "      fim_spm_rate = args.fim_spm_rate,\n",
        "      seed = seed,\n",
        "      )\n",
        "  print(f\"size of the training dataset is {train_dataset.current_size}\")\n",
        "  print(f\"size of the validation dataset is {valid_dataset.current_size}\")\n",
        "  return train_dataset,valid_dataset\n",
        "def get_mae(x,y):\n",
        "  return np.mean(np.abs(x-y))\n",
        "def get_mse(x,y):\n",
        "  return np.mean((x-y)**2)\n",
        "def error_report(x,y):\n",
        "  mae = get_mae(x,y)\n",
        "  mse = get_mse(x,y)\n",
        "  print(f\"MAE is {mae:.2f}\")\n",
        "  print(f\"MSE is {mse:.2f}\")\n",
        "def loftq_init(model,tokenizer,train_dataset,max_seq_length,args):\n",
        "  if args.use_loftq_callback:\n",
        "    compute_dtype = getattr(torch, args.bnb_4bit_compute_dtype)\n",
        "    #4 bit compute dtype for forward pass\n",
        "    #quant dtype to store model weights  in quantized format\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        torch_dtype=compute_dtype,\n",
        "    )\n",
        "    base_model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of = 8)\n",
        "    random_input_ids = torch.randint(0, len(train_dataset),size=(1,)).numpy.tolist()\n",
        "    random_inputs = [train_dataset[i] [\"content\"] for i in random_input_ids]\n",
        "    random_inputs = tokenizer(random_inputs,return_tensors=\"pt\",padding = True , truncation = \"max_length\" , max_length = max_seq_length)\n",
        "    logits_base = base_model(**random_inputs).logits\n",
        "    del base_model\n",
        "    gc.collect()\n",
        "\n",
        "    def loftq_callback(model,module_name):\n",
        "      \"\"\" Callback to replace weights with LoftQ if the mse is lower than the current best one\"\"\"\n",
        "      global current mse # defining global variable accessible across functions\n",
        "      logits = model(**random_inputs).logits\n",
        "      mse = get_mse(logits_base,logits)\n",
        "      if mse < current_mse:\n",
        "        current_mse = mse\n",
        "        print(f\"MSE improved for module{module_name} to {mse:.2f}\")\n",
        "        return True\n",
        "      print(f\"MSE did not improve for module {module_name} to {mse:.2f}\")\n",
        "      return False\n",
        "    replace_lora_weights_loftq(model,callback=loftq_callback)\n",
        "    logits_loftq_callback = model(**random_inputs).logits\n",
        "    error_report(logits_base,logits_loftq_callback)\n",
        "  else:\n",
        "    replace_lora_weights_loftq(model)\n",
        "\n",
        "def create_and_prepare_model(args,data_args,training_args):\n",
        "  device_map = None\n",
        "  bnb_config = None\n",
        "  load_in_8_bit = args.use_8bit_quantization\n",
        "  load_in_4bit = args.use_4bit_quantization\n",
        "  if args.use_unsloth:\n",
        "    from unsloth import FastLanguageModel\n",
        "  if args.use_4bit_quantization:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit = args.use_4bit_quantization,\n",
        "        bnb_4bit_quant_type = args.bnb_4bit_quant_type,\n",
        "        bnb_4bit_compute_dtype = compute_dtype,\n",
        "        bnb_4bit_use_double_quant = args.use_nested_quant\n",
        "    )\n",
        "    if compute_dtype == torch.float16 and args.use_4bit_quantization:\n",
        "      device_map = (\n",
        "          int(os.environ.get(\"LOCAL_RANK\" ,-1))\n",
        "          if torch.distrbuted.is_available() and torch.distributed.is_initialized()\n",
        "          else \"auto\"\n",
        "      )\n",
        "    if args.use_unsloth:\n",
        "      model , _ = FastLanguageModel.from_pretrained(\n",
        "          model_name = args.model_name_or_path,\n",
        "          max_seq_length = data_args.max_seq_length,\n",
        "          dtype = None,\n",
        "          load_in_4bit = load_in_4bit,\n",
        "      )\n",
        "      else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            args.model_name_or_path,\n",
        "            load_in_8bit = load_in_8bit,\n",
        "            quantization_config = bnb_config,\n",
        "            device_map = device_map,\n",
        "            trust_remote_code = True,\n",
        "            attn_implementation = \"flash_attention_2\" if args.use_flash_attn else \"eager\")\n",
        "      if ((args.use_4bit_quantization or args.use_8bit_quantization) and args.use_peft_lora and not agrs.use_unsloth):\n",
        "        model = prepare_model_for_kbit_training(model,\n",
        "                                                use_gradient_checkpointing = training_args.gradient_checkpointing,\n",
        "                                                gradient_checkpointing_kwargs = {\n",
        "                                                    \"use_reentrant\":args.use_reentrant\n",
        "                                                }\n",
        "                                                )\n",
        "      if args.use_peft_lora and not aargs.use_unsloth:\n",
        "          peft_config = LoraConfig(\n",
        "              lora_alpha = args.lora_alpha,\n",
        "              lora_dropout = args.lora_dropout,\n",
        "              r = args.lora_r,\n",
        "              bias = \"none\",\n",
        "              task_type = \"CAUSAL_LM\",\n",
        "              target_modules = args.lora_target_modules.split(\",\")\n",
        "              if args.lora_target_modules !=\"all-linear\"\n",
        "              else args.lora_target_modules,\n",
        "          )\n",
        "          model = get_peft_model(model,peft_config)\n",
        "      elif args.use_peft_lora and args.use_unsloth:\n",
        "        model = FastLanguageModel.get_peft_model(\n",
        "              lora_alpha = args.lora_alpha,\n",
        "              lora_dropout = args.lora_dropout,\n",
        "              r = args.lora_r,\n",
        "              target_modules = args.lora_target_modules.split(\",\")\n",
        "              if args.lora_target_modules !=\"all-linear\"\n",
        "              else args.lora_target_modules,\n",
        "              use_gradient_checkpointing = training_args.gradient_checkpointing,\n",
        "              random_state = training_args.seed,\n",
        "              max_seq_length = data_args.max_seq_length,\n",
        "              )\n",
        "      return model\n",
        "    def main (model_args,data_args,training_args):\n",
        "      set_seed(training_args.seed)\n",
        "      tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n",
        "      train_dataset,valid_dataset = create_datasets(tokenizer,data_args,training_args.seed)\n",
        "      train_dataset.start_iteration = 0\n",
        "      model = create_and_prepare_model(model_args,data_args,training_args)\n",
        "      #gradient ckpt\n",
        "      model.config.use_cache = not training_args.gradient_checkpointing\n",
        "      training_args.gradient_checkpointing = (training_args.gradient_checkpointing and not model_args.use_unsloth)\n",
        "      if training_args.gradient_checkpointing:\n",
        "        training_args.gradient_checkpointing_kwargs = {\n",
        "            \"use_reentrant\":model_args.use_reentrant\n",
        "        }\n",
        "      #trainer\n",
        "      trainer = Trainer(\n",
        "          model = model,\n",
        "          args = training_args,\n",
        "          train_dataset = train_dataset,\n",
        "          eval_dataset = valid_dataset,)\n",
        "      trainer.accelerator.print(f\"{trainer.model}\")\n",
        "      if model_args.use_peft_lora:\n",
        "        trainer.model.print_trainable_parameters()\n",
        "      #loftQ initialization when using QLora\n",
        "      if model_args.use_4bit_quantization and model_args.use_loftq:\n",
        "        loftq_init(trainer.model,tokenizer,train_dataset,data_args.max_seq_length,model_args)\n",
        "      #train\n",
        "      checkpoint = None\n",
        "      if training_args.resume_from_checkpoint is not None:\n",
        "        checkpoint = training_args.resume_from_checkpoint\n",
        "      trainer.train(resume_from_checkpoint=checkpoint)\n",
        "\n",
        "      # saving final model\n",
        "      if trainer.is_fsdp_enabled:\n",
        "          trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
        "      trainer.save_model()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = HfArgumentParser(\n",
        "        (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
        "    )\n",
        "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "        # If we pass only one argument to the script and it's the path to a json file,\n",
        "        # let's parse it to get our arguments.\n",
        "        model_args, data_args, training_args = parser.parse_json_file(\n",
        "            json_file=os.path.abspath(sys.argv[1])\n",
        "        )\n",
        "    else:\n",
        "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "    main(model_args, data_args, training_args)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}