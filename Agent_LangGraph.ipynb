{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvGazrkpPxT0xiD11LXN8v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Agent_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ6zbxHpWyL-"
      },
      "outputs": [],
      "source": [
        "#building agents from scratch .by god's grace. Jai Shri Ram . Jai Bajrangbali\n",
        "#building agents using MCP Protcol\n",
        "#extending the agent to react and using memory in this -> using langmem sdk from langrapgh -> 1102\n",
        "#https://www.youtube.com/watch?v=aHCDrAbH_go&t=120s - LangGraph\n",
        "#https://www.analyticsvidhya.com/blog/2025/03/langmem-sdk/ - Analytics Vidhya\n",
        "#https://docs.langchain.com/oss/python/langgraph/add-memory#add-short-term-memory\n",
        "#https://medium.com/@devwithll/simple-langgraph-implementation-with-memory-asyncsqlitesaver-checkpointer-fastapi-54f4e4879a2e - CODE FOR RUNNING CONV AI USING GRAPH\n",
        "!pip install langchain_core langchain-anthropic langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import os ,getpass\n",
        "def _set_env(var:str):\n",
        "    os.environ[var] = getpass.getpass(f\"Enter your {var}: \")\n",
        "_set_env(\"ANTHROPIC_API_KEY\")"
      ],
      "metadata": {
        "id": "MuLs5YP-XOMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#llm\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n"
      ],
      "metadata": {
        "id": "QqjNb4ycYRdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#schema for structured output\n",
        "from pydantic import BaseModel , Field\n",
        "class SearchQuery(BaseModel):\n",
        "    search_query: str = Field(None , description=\"Query that is optimized web search.\")\n",
        "    justification: str = Field(None , description=\"Why this query is relevant to user's request.\")\n",
        "\n",
        "#Augment the LLM with schema for structured output\n",
        "structured_llm = llm.with_structured_output(SearchQuery)\n",
        "\n",
        "#invoke the augmented LLM\n",
        "output = structured_llm.invoke(\"How does Calcium CT score relate to high chlorestrol levels?\")\n",
        "print(output.search_query)\n",
        "print(output.justification)"
      ],
      "metadata": {
        "id": "-k71NsC-Ykb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tool calling\n",
        "def multiply(a:int,b:int) ->int:\n",
        "  return a*b\n",
        "#augment the LLM with tools\n",
        "llm_with_tools = llm.bind_tools([multiply])\n",
        "#invoke the LLM with input that triggers the tool call\n",
        "msg = llm_with_tools.invoke(\"What is 2 times 3?\")\n",
        "print(msg)\n",
        "\n",
        "#get the tool call\n",
        "msg.tool_calls"
      ],
      "metadata": {
        "id": "7JOHCqIVa4R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt chaining\n",
        "#each llm processes the output of the other llm in a chain\n",
        "#when do you want to do this\n",
        "#use it in Conv AI .... in LLM-as-a-judge\n",
        "#!pip install typing_extensions\n",
        "from typing_extensions import TypedDict\n",
        "#graph state for passing thru one llm to another llm. this is KEY\n",
        "class State(TypedDict):\n",
        "  topic: str\n",
        "  joke: str\n",
        "  improved_joke: str\n",
        "  final_joke: str"
      ],
      "metadata": {
        "id": "eeiY2laLbr3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nodes\n",
        "def generate_joke(state:State):\n",
        "  \"\"\"First LLM call to generate initial joke\"\"\"\n",
        "  msg = llm.invoke(f\"Write a short joke {state('topic')}\")\n",
        "  return {\"joke\":msg.content}\n",
        "\n",
        "def improve_joke(state:State):\n",
        "  \"\"\"Second LLM call to improve the joke\"\"\"\n",
        "  msg = llm.invoke(f\"Make this joke funnier by adding wordplay{state('joke')}\")\n",
        "  return {\"improved_joke\":msg.content}\n",
        "\n",
        "def polish_joke(state:State):\n",
        "  \"\"\"Third LLM call for final polish of the joke content to make it better\"\"\"\n",
        "  msg = llm.invoke(f\"Add a surprising twist to this joke{state('joke')}\")\n",
        "  return {\"final_joke\": msg.content}\n",
        "\n",
        "#conditional edge function to check if the joke has a punchline\n",
        "def check_punchline(state:State): #conditional EDGE In Langgraph - what is the condition to move from one llm to another\n",
        "    \"\"\"Gate Function to check if the joke has a punchline\"\"\"\n",
        "    if \"?\" in state[\"improved_joke\"] or \"!\" in state[\"joke\"]:\n",
        "      return \"Pass\"\n",
        "    return \"Fail\""
      ],
      "metadata": {
        "id": "hOYkkSWIcm4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#langgraph simple workflow\n",
        "from langgraph.graph import StateGraph , START , END\n",
        "from IPython.display import Image, display\n",
        "\n",
        "#build workflow\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(generate_joke, generate_joke)\n",
        "workflow.add_node(improve_joke,improve_joke)\n",
        "workflow.add_node(polish_joke,polish_joke)\n",
        "\n",
        "#add edges to connect nodes\n",
        "workflow.add_edge(START,\"generate_joke\")\n",
        "workflow.add_conditional_edges(\"generate_joke\",check_punchline , {\"Pass\":\"improve_joke\", \"Fail\":END})\n",
        "workflow.add_edge(\"improve_joke\",\"polish_joke\")\n",
        "workflow.add_edge(\"polish_joke\",END)\n",
        "\n",
        "#compile workflow\n",
        "chain = workflow.compile()\n",
        "\n",
        "#show workflow\n",
        "display(Image(chain.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "qyyGyMPyf_U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "state = chain.invoke({\"topic\":\"cats\"})\n",
        "print(\"Intial Joke:\")\n",
        "print(state[\"joke\"])\n",
        "print(\"\\n=== === ===\\n\")\n",
        "if \"improved_joke\" in state:\n",
        "  print(\"Improved Joke:\")\n",
        "  print(state[\"improved_joke\"])\n",
        "  print(\"Final Joke:\")\n",
        "  print(state[\"final_joke\"])\n",
        "else:\n",
        "  print(\"joke failed quality gate = no punchline detected\")"
      ],
      "metadata": {
        "id": "y0zQE7VljCD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parallelization\n",
        "#one task and you fan out to sub task to separate LLMs to parallelize the task and do a final aggregration of the task\n",
        "#Graph State\n",
        "class State(TypedDict):\n",
        "  topic:str\n",
        "  joke: str\n",
        "  story: str\n",
        "  poem:str\n",
        "  combined_output: str"
      ],
      "metadata": {
        "id": "YbaHd2mHkHQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nodes\n",
        "def call_llm_1(state:State):\n",
        "  \"\"\" Fist LLM call to generate intial joke \"\"\"\n",
        "  msg = llm.invoke(f\"Write a short joke about {state('topic')}\")\n",
        "  return {\"joke\":msg.content}\n",
        "\n",
        "def call_llm_2(state:State):\n",
        "  \"\"\" Second LLM call to generate Story\"\"\"\n",
        "  msg = llm.invoke(f\"Write a short story about {state('topic')}\")\n",
        "  return {\"story\":msg.content}\n",
        "\n",
        "def call_llm_3(state:State):\n",
        "  \"\"\"Third LLM call to generate Poem\"\"\"\n",
        "  msg = llm.invoke(f\"Write a short poem about {state('topic')}\")\n",
        "  return {\"poem\":msg.content}\n",
        "\n",
        "def aggregator(state:State):\n",
        "  \"\"\" Combine the joke and story into a single output\"\"\"\n",
        "  combined = f\"Here's a story , joke and poem about {state['topic']}!\\n\\n\"\n",
        "  combined += f\"Joke: {state['joke']}\\n\\n\"\n",
        "  combined += f\"Story: {state['story']}\\n\\n\"\n",
        "  combined += f\"Poem: {state['poem']}\"\n",
        "  return {\"combined_output\":combined}"
      ],
      "metadata": {
        "id": "PVUaI_Tgk5D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build parallel workflow\n",
        "parallel_builder = StateGraph(State)\n",
        "\n",
        "#add nodes\n",
        "parallel_builder.add_node(\"call_llm_1\",call_llm_1)\n",
        "parallel_builder.add_node(\"call_llm_2\",call_llm_2)\n",
        "parallel_builder.add_node(\"call_llm_3\",call_llm_3)\n",
        "parallel_builder.add_node(\"aggregator\",aggregator)\n",
        "\n",
        "#add edges to connect nodes\n",
        "parallel_builder.add_edge(START,\"call_llm_1\")\n",
        "parallel_builder.add_edge(START,\"call_llm_2\")\n",
        "parallel_builder.add_edge(START,\"call_llm_3\")\n",
        "parallel_builder.add_edge(\"call_llm_1\",\"aggregator\")\n",
        "parallel_builder.add_edge(\"call_llm_2\",\"aggregator\")\n",
        "parallel_builder.add_edge(\"call_llm_3\",\"aggregator\")\n",
        "parallel_builder.add_edge(\"aggregator\",END)\n",
        "parallel_workflow = parallel_builder.compile()\n",
        "\n",
        "#show workflow\n",
        "display(Image(parallel_workflow.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "RT7yU3tMmVrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#routing\n",
        "#take a input and route the input to poem, story or joke generation based on user input and then aggregate\n",
        "from typing_extensions import Literal\n",
        "class Route(BaseModel):\n",
        "  step:Literal[\"poem\",\"story\",\"joke\"] = Field(None , description = \"The next step in the routing process\")\n",
        "#augment the LLM with schema for structured output\n",
        "router = llm.with_structured_output(Route)"
      ],
      "metadata": {
        "id": "Nwy86dxvnO8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#state for routing\n",
        "#one task and you fan out to sub task to separate LLMs to parallelize the task and do a final aggregration of the task\n",
        "#Graph State\n",
        "class State(TypedDict):\n",
        "  input:str\n",
        "  decision: str\n",
        "  output: str"
      ],
      "metadata": {
        "id": "JuAL-tGtpfiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from langchain_core.messages import HumanMessage , SystemMessage\n",
        "\n",
        "def call_llm_1(state:State):\n",
        "  \"\"\" Write a story \"\"\"\n",
        "  result = llm.invoke(f\"Write a short story about {state('input')}\")\n",
        "  return {\"output\":result.content}\n",
        "\n",
        "def call_llm_2(state:State):\n",
        "  \"\"\" Write a Joke \"\"\"\n",
        "  result = llm.invoke(f\"Write a short joke  about {state('input')}\")\n",
        "  return {\"output\":result.content}\n",
        "\n",
        "def call_llm_3(state:State):\n",
        "  \"\"\" Write a Poem  \"\"\"\n",
        "  result = llm.invoke(f\"Write a short poem  about {state('input')}\")\n",
        "  return {\"output\":result.content}\n",
        "\n",
        "def llm_call_router(state:State):\n",
        "  \"\"\" Route the input to the appropiate node \"\"\"\n",
        "  #run the augmented LLM with structured output to serve as routing logic\n",
        "  decision = router.invoke([SystemMessage(content = \"Route the input to Story , Joke or Poem based on the user's request.\"),\n",
        "                            HumanMessage(content = state(\"input\"))])\n",
        "  return {\"decision\": decision.step}\n",
        "\n",
        "#conditional edge function to route to the appropiate node #dotted line show conditional edge\n",
        "def route_decision(state:State):\n",
        "  #return the node name you want to visit next\n",
        "  if state[\"decision\"] == \"story\":\n",
        "    return call_llm_1\n",
        "  elif state[\"decision\"] == \"joke\":\n",
        "    return call_llm_2\n",
        "  elif state[\"decision\"] == \"poem\":\n",
        "    return call_llm_3\n",
        "  else:\n",
        "    return"
      ],
      "metadata": {
        "id": "xSgtnfSSpxNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build parallel workflow\n",
        "router_builder = StateGraph(State)\n",
        "\n",
        "#add nodes\n",
        "router_builder.add_node(\"call_llm_1\",call_llm_1)\n",
        "router_builder.add_node(\"call_llm_2\",call_llm_2)\n",
        "router_builder.add_node(\"call_llm_3\",call_llm_3)\n",
        "#router_builder.add_node(\"aggregator\",aggregator)\n",
        "\n",
        "#add edges to connect nodes\n",
        "router_builder.add_edge(\"call_llm_1\",END)\n",
        "router_builder.add_edge(\"call_llm_2\",END)\n",
        "router_builder.add_edge(\"call_llm_3\",END)\n",
        "\n",
        "router_workflow = router_builder.compile()\n",
        "#show workflow\n",
        "display(Image(router_builder.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "kRvsFMqzsLNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Orchestrator-Worker - LLM breaks down a task and delegate each task to a worker and synthensize to provide outcome\n",
        "from typing import Annotated , List\n",
        "import operator\n",
        "#schema for structure output to use in planning\n"
      ],
      "metadata": {
        "id": "UtAskB8mtJ22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluator-optimizer workflow\n",
        "#one LLM generates a response while another LLM evaluates and provide feedback in loop\n",
        "#schema for structured output to use in evaluation\n",
        "class Feedback(BaseModel):\n",
        "  grade:Literal[\"funny\",\"not funny\"] = Field(description = \"Decide if the joke is funny or not\",)\n",
        "  feedback :str = Field(description = \"Explain why the joke is funny or not\")\n",
        "\n",
        "#augment the llm with schema for structured output\n",
        "evaluator = llm.with_structured_output(Feedback)"
      ],
      "metadata": {
        "id": "AoYGl7oX5c9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Graph State\n",
        "class State(TypedDict):\n",
        "  joke:str\n",
        "  topic:str\n",
        "  feedback:str\n",
        "  funny_or_not:str"
      ],
      "metadata": {
        "id": "yefaGBr063fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nodes\n",
        "def llm_call_generator(state:State):\n",
        "  \"\"\"LLM generate a joke\"\"\"\n",
        "  if state.get(\"feedback\"):\n",
        "    msg = llm.invoke(f\"Write a joke about {state ['topic']} but take this feedback into account {state['feedback']}\")\n",
        "  else:\n",
        "    msg = llm.invoke(f\"Write a joke about {state ['topic']}\")\n",
        "  return {\"joke\":msg.content}\n",
        "\n",
        "def llm_call_evaluator(state:State):\n",
        "  \"\"\" LLM evaluates a joke \"\"\"\n",
        "  grade = evaluator.invoke(f\"Grade the joke{state['joke']}\")\n",
        "  return {\"funny_or_not\":grade.grade , \"feedback\":grade.feedback}\n",
        "\n",
        "#conditional edge function to route back to joke generator or end based upon feedback from the evaluator\n",
        "def route_joke():\n",
        "  \"\"\" Route the joke back to the joke generator or end based upon feedback from the evaluator\"\"\"\n",
        "  if state[\"funny_or_not\"] == \"funny\":\n",
        "    return \"Accepted\"\n",
        "  elif state[\"funny_or_not\"] == \"not funny\":\n",
        "    return \"Rejected + Feedback\""
      ],
      "metadata": {
        "id": "7VG5ZYEv7Qr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agent\n",
        "#remove scafolding and allow LLM to take actions\n",
        "#define tools using tool decorator\n",
        "#agent\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def multiply(a:int , b:int) ->int:\n",
        "  return a*b\n",
        "\n",
        "@tool\n",
        "def add(a:int , b:int) ->int:\n",
        "  return a+b\n",
        "\n",
        "@tool\n",
        "def divide(a:int , b:int) ->int:\n",
        "  return a/b\n",
        "\n",
        "@tool\n",
        "def subtract(a:int , b:int) ->int:\n",
        "  return a-b\n",
        "\n",
        "#augment the llm with tools\n",
        "tools = [add, multiply, divide, subtract]\n",
        "tools_by_name = {tool.name:tool for tool in tools}\n",
        "llm_with_tools = llm.bind_tools(tools)\n"
      ],
      "metadata": {
        "id": "7tT0nbOE9kak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import ToolMessage\n",
        "from IPython.display import Image, display\n",
        "\n",
        "#Nodes\n",
        "def llm_call(state:MessagesState):\n",
        "  \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
        "  return {\"messages\":[llm_with_tools.invoke([SystemMessage(content= \"You are a helpful Assistant tasked with performing arithmetic on a set of inputs\")]+ state[\"messages\"])]}\n",
        "\n",
        "def tool_node(state:dict):\n",
        "  \"\"\"Call a tool\"\"\"\n",
        "  result = []\n",
        "  for tool_call in state[\"messages\"][-1].tool_calls:\n",
        "    tool = tools_by_name[tool_call[\"name\"]]\n",
        "    observation = tool.invoke(tool_call[\"args\"])\n",
        "    result.append(ToolMessage(content = observation , tool_call_id = tool_call[\"id\"]))\n",
        "  return {\"messages\":result}\n",
        "\n",
        "def should_continue(state:MessagesState) -> Literal[\"enviornment\":\"END\"]:\n",
        "  \"\"\" decide if we should continue the loop or stop based on upon whether the LLM made a tool call\"\"\"\n",
        "  messages = state[\"messages\"]\n",
        "  last_message = messages[-1]\n",
        "  if last_message.tool_calls:\n",
        "        return \"Action\"\n",
        "  return \"END\""
      ],
      "metadata": {
        "id": "zlbibCL6_cLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "CODE OF USING PERSISTENCE MEMORY EITH REACT AGENT USING TOOLS AND ALSO STORING RETREIVING MEMORY\n",
        "* https://docs.langchain.com/oss/python/langgraph/add-memory#add-short-term-memory\n",
        "\n",
        "\n",
        "*   Checkpoints need to be used backed by Persistence Store like a Redis or Postgress of Mongodb\n",
        "*  All memory has to be stored in the persistence memory using a session_id\n",
        "*  These memory can be searched . Memory can be of a Conversational Memory and stored in this system at the end of the conversation   \n",
        "*   Langgraph uses State object for passing memory for in-session\n",
        "*   Langgraph uses Checkpoints for persistence memory and these can be passed to graph nodes or sub-agents\n",
        "*   Tools can be binded to model. USE CREATE_REACT_AGENT\n",
        "*   Memory Management should be Async when handling production grade Conv Agents\n",
        "*   Handle Rate Limits with foundational models\n",
        "*   Error handling try :except block with proper handling of error should be there\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1RtIW0B_XbOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#start code for React Agent using LT memory ....\n",
        "#ST Memory - In session can be handled via State but LT memory we need mem0 or something\n",
        "#we will build the code using mem0 also\n",
        "!pip install langchain\n",
        "!pip install langgraph\n",
        "!pip install -U langmem\n",
        "!pip install mcp\n",
        "!pip install -qU \"langchain[groq]\"\n"
      ],
      "metadata": {
        "id": "lFITbowO2wdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langmem import create_manage_memory_tool , create_search_memory_tool\n"
      ],
      "metadata": {
        "id": "4v0UWhUL5aql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Long term memory management tools to store memory and search memory\n",
        "#when you use MongoDB or Redis you replace this with DB schemas - which are nothing but JSON objects - Key Value Pairs\n",
        "memory_tool =[\n",
        "    create_manage_memory_tool(namespace = (\"agent_memory\",)),\n",
        "    create_search_memory_tool(namespace = (\"agent_memory_search\",))\n",
        "]"
      ],
      "metadata": {
        "id": "OIuiKyzp5Twx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os , getpass\n",
        "import dotenv\n",
        "#setup a memory store\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "#in session memory what langchain offers\n",
        "#if we want permanent memory across session we need to use a persistent memory store like Redis / Mongo-db etc ... refer code as below for that\n",
        "\n",
        "store = InMemoryStore(\n",
        "    index ={\n",
        "        \"dims\":1536,\n",
        "        \"embed\":\"openai:text-embedding-3-small\"\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "OjKlyCbj_tGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from typing import List , Literal\n",
        "@tool\n",
        "def add(a:int,b:int) ->int:\n",
        "  \"\"\"Adds two integers and returns the result.\"\"\"\n",
        "  return a+b"
      ],
      "metadata": {
        "id": "O0XkZxg2LA1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8695846"
      },
      "source": [
        "model = init_chat_model(\"qwen/qwen3-32b\", api_key= GROQ_API_KEY ,model_provider=\"groq\")\n",
        "#add memory checkpointer\n",
        "checkpointer = MemorySaver() #this checkpointer gets changed to Mongo or Redis when we want persistent store and this can be Async\n",
        "#in case of marketing agent we did these  things\n",
        "#1 -Used Redis as persistent store and in that mem0 for checkpointer\n",
        "#2 -Used compressing the context ....\n",
        "#3 -Used langmem to do so\n",
        "#4 -tool call binding as well\n",
        "#5 -search the memory\n",
        "#activate react agent\n",
        "#add the model and memory management tool into the agent - create and search memory\n",
        "#*args and **kwargs are used to allow functions to accept an arbitrary number of arguments\n",
        "agent = create_react_agent(model=model,tools = [add,*memory_tool],checkpointer=checkpointer,store=store)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#start executing the agent\n",
        "text = \"Hi, Please create marketing plan for reducing customer acquistion cost. This plan is for CMO Exec Level so plan should be executable\"\n",
        "session_id = 1\n",
        "result = agent.invoke({\"messages\":[{\"role\":\"user\",\"content\":text}]},config ={\"configurable\":{\"session_id\":session_id, \"thread_id\": str(session_id)}} )\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "FcFWdI6MGmB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding more text into the prompt\n",
        "text =\"also pls suggest campaigns\"\n",
        "session_id = 1\n",
        "result = agent.invoke({\"messages\":[{\"role\":\"user\",\"content\":text}]},config ={\"configurable\":{\"session_id\":session_id, \"thread_id\": str(session_id)}} )\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "g0faBYmbNhld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now change the session_id and check memory\n",
        "text = \"generate marketing plan for increasing sales\"\n",
        "session_id = 2\n",
        "result = agent.invoke({\"messages\":[{\"role\":\"user\",\"content\":text}]},config ={\"configurable\":{\"session_id\":session_id, \"thread_id\": str(session_id)}} )\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "xxpuKKd0OTjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#leveraging langmem\n",
        "namespace = {\"agent_memory\",\"{user_id}\"} # creataing a agent memory segregrated by user_id - This is what I did\n",
        "text = \"pls publish my marketing plan\"\n",
        "session_id = 2\n",
        "user_id = \"ab\"\n",
        "result = agent.invoke({\"messages\":[{\"role\":\"user\",\"content\":text}]},config ={\"configurable\":{\"session_id\":session_id, \"thread_id\": user_id}} )"
      ],
      "metadata": {
        "id": "9TOXxRe8hiSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "items = store.search(\"agenr_memory\",)\n",
        "for item in items:\n",
        "  print(item.namespace,item.value)"
      ],
      "metadata": {
        "id": "6SiHBlE9ocTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi\n",
        "!pip install langgraph-checkpoint-sqlite\n"
      ],
      "metadata": {
        "id": "xBhmFKWfFah8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#develop conv agent with memory and asyn using langgraph and langchain\n",
        "import os\n",
        "import uvicorn\n",
        "from dotenv import load_dotenv\n",
        "from typing import List, TypedDict , Annotated\n",
        "from contextlib import asynccontextmanager\n",
        "\n",
        "#Fast API Imports\n",
        "from fastapi import  FastAPI , Request , Form\n",
        "from fastapi.responses import HTMLResponse , RedirectResponse\n",
        "from fastapi.templating import Jinja2Templates\n",
        "from starlette import status\n",
        "\n",
        "#import langchain /langgraph imports\n",
        "from langchain_core.messages import AIMessage , HumanMessage , BaseMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
        "from langgraph.graph import StateGraph , END"
      ],
      "metadata": {
        "id": "FdoUOmog-juj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Langgraph state will have three things to store\n",
        "#user prompt\n",
        "#message history\n",
        "#response from LLM\n",
        "\n",
        "class ChatState(TypedDict):\n",
        "  user_prompt:str\n",
        "  messages:Annotated[List[BaseMessage],add_messages] #Annotated type(list) with metadata. #add_messages is a reducer\n",
        "  response:str\n"
      ],
      "metadata": {
        "id": "TPAJOCvtGNR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the virtual assistant\n",
        "\n",
        "async def process_user_prompt_node(state:ChatState):\n",
        "  user_message = state[\"user_prompt\"]\n",
        "  return {\"messages\":[HumanMessage(content=user_prompt)]}\n",
        "\n",
        "async def call_model_node(state:ChatState):\n",
        "  llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
        "  messages = state[\"messages\"]\n",
        "  if not messages or not isinstance(messages[-1],HumanMessage):\n",
        "    return{\"response\":\"no message generated by LLM\"}\n",
        "  try:\n",
        "    #call llm for response\n",
        "    response = await llm.ainvoke(messages) #async operation of calling LLM\n",
        "    #return the LLM response\n",
        "    return{\"response\":response.content}\n",
        "  except Exception as e:\n",
        "    return{\"response\":\"sorry, error enountered no response from LLM\"}\n",
        "\n",
        "async def process_bot_response_node(state:ChatState):\n",
        "  bot_response = state[\"response\"]\n",
        "  return {\"messages\":[AIMessage(content=bot_response)]}\n"
      ],
      "metadata": {
        "id": "IGWHdj9FHZfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FastAPI LifeSpan Function\n",
        "@asynccontextmanager\n",
        "async def lifespan(app:FastAPI):\n",
        "  print(\"Starting up:Intializing Resources\")\n",
        "\n",
        "  async with AsyncSqliteSaver.from_conn_string(SQLITE_DATABASE_PATH) as checkpointer:\n",
        "    print(\"AsyncSqliteSaver Connection Established\")\n",
        "    #intiate the graph\n",
        "    workflow = StateGraph(ChatState) #intialize the graph with persistent chat state #THANKS A LOT GOD\n",
        "\n",
        "    #add nodes\n",
        "    workflow.add_node(\"process_user_prompt\",process_user_prompt_node)\n",
        "    workflow.add_node(\"call_model\",call_model_node)\n",
        "    workflow.add_node(\"generate_response\",END)\n",
        "\n",
        "    #add edges\n",
        "    workflow.set_entry_point(\"process_user_prompt\")\n",
        "    workflow.add_edge(\"process_user_prompt\",\"call_model\")\n",
        "    workflow.add_edge(\"call_model\",\"generate_response\")\n",
        "    workflow.add_edge(\"generate_response\",END)\n",
        "\n",
        "    #compile the graph\n",
        "    app.state.graph = workflow.compile(checkpointer = checkpointer) #this checkpointer can be Redis or Postgress depending upon the persistent store\n",
        "    app.state.checkpointer = checkpointer\n",
        "  yield\n",
        "  print(\"Shutting down:closing resources\")\n"
      ],
      "metadata": {
        "id": "wQ_NlyuOLL74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#helper function for memory management/ storing chats and retrieving those\n",
        "async def get_chat_history_messages(request:Request,thread_id:str):\n",
        "   #request is of object type FastAPI request\n",
        "   config = {\"configurable\":{\"thread_id\":thread_id}}\n",
        "   chat_history_messages = []\n",
        "   app_graph = request.app.chat.graph\n",
        "   try:\n",
        "    current_state = await app_graph.aget_state(config)\n",
        "    if current_state and current_state.values.get(\"messages\"):\n",
        "      chat_history_messages = current_state.values[\"messages\"]\n",
        "      print(f\"loaded state in memory {len(chat_history_messages)} messages for session {thread_id}\")\n",
        "    else:\n",
        "      print(\"no loaded state\")\n",
        "   except Exception as e:\n",
        "      print(\"Exception\")\n",
        "   return chat_history_messages"
      ],
      "metadata": {
        "id": "LCZ3VQsFOlos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#intialize FastAPI App Intialization with LifeSpan\n",
        "\n",
        "app = FastAPI(lifespan = lifespan)\n",
        "\n",
        "#mount jinja templates\n",
        "templates = Jinja2Templates(directory = \"templates\")"
      ],
      "metadata": {
        "id": "VwPyHFQfQy03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FAST API end points\n",
        "SESSION_ID = 1\n",
        "@app.get(\"/\",response_class = HTMLResponse)\n",
        "async def read_root(request:Request):\n",
        "  \"\"\" Render chat page with Conversation History\"\"\"\n",
        "  #get the chat history for the current session\n",
        "  chat_history = await get_chat_history_messages(request,SESSION_ID)\n",
        "  return templates.TemplateResponse(\"chat.html\",{\"request\":request,\"chat_history\":chat_history})\n",
        "@app.post(\"/chat\")\n",
        "async def chat_endpoint(request:Request,user_prompt:str = Form(...)):\n",
        "  if not user_prompt:\n",
        "    return RedirectResponse(url=\"/\",status_code = status.HTTP_303_SEE_OTHER)\n",
        "  graph_input = {\"user_prompt\":user_prompt}\n",
        "  #create a configurable dictionary with the current session / thread ID\n",
        "  config = {\"configurable\":{\"thread_id\":SESSION_ID}}\n",
        "  app_graph = request.app.state.graph\n",
        "  try:\n",
        "    await app_graph.ainvoke(graph_input,config = config)\n",
        "  except Exception as e:\n",
        "    print(f\"error invoking graph for session id {SESSION_ID} : {e}\" )\n",
        "  return RedirectResponse(url=\"/\",status_code = status.HTTP_303_SEE_OTHER)\n"
      ],
      "metadata": {
        "id": "6HUsCu6WRKS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run the FastAPI app....\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "import asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Create a Uvicorn server configuration\n",
        "config = uvicorn.Config(\"app:app\", host=\"0.0.0.0\", reload=False, loop=\"asyncio\")\n",
        "\n",
        "# Create a Uvicorn server instance\n",
        "server = uvicorn.Server(config)\n",
        "\n",
        "# Run the server in the existing event loop\n",
        "# This will block until the server is stopped\n",
        "asyncio.run(server.serve())"
      ],
      "metadata": {
        "id": "yfM7lQMFTy2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code for MCP\n",
        "#why MCP - standardize away to access tools and give it to LLM\n",
        "#agents can be exposed MCP - this can enable Agent Factories - Agent as a tool Pattern\n",
        "#subagents can run in there own sandbox(E2B) and can complete there own task independently\n",
        "#I used subagents for Code and Text2sql  run in there own sandbox - this helps for security\n",
        "#Subagents chaining and comm is done thru message passing by the main agent / Orchestrator pattern ....\n",
        "#Subagents are registered in a registry https://www.vladsnewsletter.com/p/sub-agents\n",
        "#A2A communication can be thru Schema....\n",
        "#Master Agent forces a schema on sub agents and sub agents respond back on that schema\n",
        "#Agents can use VFS - Virtual File System , Todo-list , System prompt and sub agent\n",
        "\n",
        "#MCP\n",
        "\"\"\"\n",
        "1. MCP Prompt - what protocol , what are tool inputs , return - text , image etc....\n",
        "2. MCP Client\n",
        "3. MCP Server\n",
        "4. Resources\n",
        "\"\"\"\n",
        "\n",
        "#https://thenewstack.io/15-best-practices-for-building-mcp-servers-in-production/\n",
        "#https://thenewstack.io/how-elicitation-in-mcp-brings-human-in-the-loop-to-ai-tools/\n",
        "\n",
        "#Agents -> use MCP key factors:\n",
        "\"\"\"\n",
        "1.Use Aysnc\n",
        "2.Use Error handling / time out from MCP server\n",
        "3.OAuth Security\n",
        "4.Elicitation of response from Human .... Human + MCP\n",
        "5.use json-rpc or HTTPStreaming for faster response from MCP service\n",
        "6.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "tNfu95dW51Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#simple mcp server\n",
        "!pip install fastmcp"
      ],
      "metadata": {
        "id": "uT6jBGudAYyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastmcp import FastMCPServer\n",
        "from fastmcp import FastMCP\n",
        "\n",
        "server = FastMCP(\"Demo\")\n",
        "@server.tool()\n",
        "async def add(a:int,b:int) ->int:\n",
        "  return a+b\n",
        "@server.tool()\n",
        "async def multiply(a:int,b:int) ->int:\n",
        "  return a*b\n",
        "\n",
        "@server.tool()\n",
        "async def divide(a:int,b:int) ->int:\n",
        "  return a/b\n",
        "\n",
        "@server.tool()\n",
        "async def subtract(a:int,b:int) ->int:\n",
        "  return a-b\n",
        "\n",
        "@server.tool()\n",
        "async def greet_user_formal(name:str) ->str:\n",
        "\n",
        "  \"\"\"\n",
        "  A tool that returns a greeting message in a very long tone\n",
        "  Args:\n",
        "  name:str\n",
        "  Returns:str\n",
        "  \"\"\"\n",
        "  return f\"Hello {name}! How can I assist you today?\"\n",
        "\n",
        "@server.prompt\n",
        "def greet_user_prompt(name:str) ->str:\n",
        "  \"\"\" Generates  a message asking for a greeting\"\"\"\n",
        "  return f\"\"\"\n",
        "  Return a greeting message for a user called '{name}'\n",
        "  if the user is called 'Laurent' , use a formal style , else use a street style\n",
        "  \"\"\"\n",
        "if __name__ == \"_main()__\":\n",
        "  server.run(transport=\"stdio\")"
      ],
      "metadata": {
        "id": "hIiLMfRNHTmL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}