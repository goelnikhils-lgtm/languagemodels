{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMUov7pMb/I8O6dE+Dn0Ag",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Agent_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ6zbxHpWyL-"
      },
      "outputs": [],
      "source": [
        "#building agents from scratch .by god's grace. Jai Shri Ram . Jai Bajrangbali\n",
        "#building agents using MCP Protcol\n",
        "#extending the agent to react and using memory in this -> using langmem sdk from langrapgh -> 1102\n",
        "#https://www.youtube.com/watch?v=aHCDrAbH_go&t=120s - LangGraph\n",
        "#https://www.analyticsvidhya.com/blog/2025/03/langmem-sdk/ - Analytics Vidhya\n",
        "#https://docs.langchain.com/oss/python/langgraph/add-memory#add-short-term-memory\n",
        "!pip install langchain_core langchain-anthropic langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import os ,getpass\n",
        "def _set_env(var:str):\n",
        "    os.environ[var] = getpass.getpass(f\"Enter your {var}: \")\n",
        "_set_env(\"ANTHROPIC_API_KEY\")"
      ],
      "metadata": {
        "id": "MuLs5YP-XOMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#llm\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n"
      ],
      "metadata": {
        "id": "QqjNb4ycYRdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#schema for structured output\n",
        "from pydantic import BaseModel , Field\n",
        "class SearchQuery(BaseModel):\n",
        "    search_query: str = Field(None , description=\"Query that is optimized web search.\")\n",
        "    justification: str = Field(None , description=\"Why this query is relevant to user's request.\")\n",
        "\n",
        "#Augment the LLM with schema for structured output\n",
        "structured_llm = llm.with_structured_output(SearchQuery)\n",
        "\n",
        "#invoke the augmented LLM\n",
        "output = structured_llm.invoke(\"How does Calcium CT score relate to high chlorestrol levels?\")\n",
        "print(output.search_query)\n",
        "print(output.justification)"
      ],
      "metadata": {
        "id": "-k71NsC-Ykb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tool calling\n",
        "def multiply(a:int,b:int) ->int:\n",
        "  return a*b\n",
        "#augment the LLM with tools\n",
        "llm_with_tools = llm.bind_tools([multiply])\n",
        "#invoke the LLM with input that triggers the tool call\n",
        "msg = llm_with_tools.invoke(\"What is 2 times 3?\")\n",
        "print(msg)\n",
        "\n",
        "#get the tool call\n",
        "msg.tool_calls"
      ],
      "metadata": {
        "id": "7JOHCqIVa4R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt chaining\n",
        "#each llm processes the output of the other llm in a chain\n",
        "#when do you want to do this\n",
        "#use it in Conv AI .... in LLM-as-a-judge\n",
        "#!pip install typing_extensions\n",
        "from typing_extensions import TypedDict\n",
        "#graph state for passing thru one llm to another llm. this is KEY\n",
        "class State(TypedDict):\n",
        "  topic: str\n",
        "  joke: str\n",
        "  improved_joke: str\n",
        "  final_joke: str"
      ],
      "metadata": {
        "id": "eeiY2laLbr3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nodes\n",
        "def generate_joke(state:State):\n",
        "  \"\"\"First LLM call to generate initial joke\"\"\"\n",
        "  msg = llm.invoke(f\"Write a short joke {state('topic')}\")\n",
        "  return {\"joke\":msg.content}\n",
        "\n",
        "def improve_joke(state:State):\n",
        "  \"\"\"Second LLM call to improve the joke\"\"\"\n",
        "  msg = llm.invoke(f\"Make this joke funnier by adding wordplay{state('joke')}\")\n",
        "  return {\"improved_joke\":msg.content}\n",
        "\n",
        "def polish_joke(state:State):\n",
        "  \"\"\"Third LLM call for final polish of the joke content to make it better\"\"\"\n",
        "  msg = llm.invoke(f\"Add a surprising twist to this joke{state('joke')}\")\n",
        "  return {\"final_joke\": msg.content}\n",
        "\n",
        "#conditional edge function to check if the joke has a punchline\n",
        "def check_punchline(state:State): #conditional EDGE In Langgraph - what is the condition to move from one llm to another\n",
        "    \"\"\"Gate Function to check if the joke has a punchline\"\"\"\n",
        "    if \"?\" in state[\"improved_joke\"] or \"!\" in state[\"joke\"]:\n",
        "      return \"Pass\"\n",
        "    return \"Fail\""
      ],
      "metadata": {
        "id": "hOYkkSWIcm4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#langgraph simple workflow\n",
        "from langgraph.graph import StateGraph , START , END\n",
        "from IPython.display import Image, display\n",
        "\n",
        "#build workflow\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(generate_joke, generate_joke)\n",
        "workflow.add_node(improve_joke,improve_joke)\n",
        "workflow.add_node(polish_joke,polish_joke)\n",
        "\n",
        "#add edges to connect nodes\n",
        "workflow.add_edge(START,\"generate_joke\")\n",
        "workflow.add_conditional_edges(\"generate_joke\",check_punchline , {\"Pass\":\"improve_joke\", \"Fail\":END})\n",
        "workflow.add_edge(\"improve_joke\",\"polish_joke\")\n",
        "workflow.add_edge(\"polish_joke\",END)\n",
        "\n",
        "#compile workflow\n",
        "chain = workflow.compile()\n",
        "\n",
        "#show workflow\n",
        "display(Image(chain.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "qyyGyMPyf_U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "state = chain.invoke({\"topic\":\"cats\"})\n",
        "print(\"Intial Joke:\")\n",
        "print(state[\"joke\"])\n",
        "print(\"\\n=== === ===\\n\")\n",
        "if \"improved_joke\" in state:\n",
        "  print(\"Improved Joke:\")\n",
        "  print(state[\"improved_joke\"])\n",
        "  print(\"Final Joke:\")\n",
        "  print(state[\"final_joke\"])\n",
        "else:\n",
        "  print(\"joke failed quality gate = no punchline detected\")"
      ],
      "metadata": {
        "id": "y0zQE7VljCD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parallelization\n",
        "#one task and you fan out to sub task to separate LLMs to parallelize the task and do a final aggregration of the task\n",
        "#Graph State\n",
        "class State(TypedDict):\n",
        "  topic:str\n",
        "  joke: str\n",
        "  story: str\n",
        "  poem:str\n",
        "  combined_output: str"
      ],
      "metadata": {
        "id": "YbaHd2mHkHQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nodes\n",
        "def call_llm_1(state:State):\n",
        "  \"\"\" Fist LLM call to generate intial joke \"\"\"\n",
        "  msg = llm.invoke(f\"Write a short joke about {state('topic')}\")\n",
        "  return {\"joke\":msg.content}\n",
        "\n",
        "def call_llm_2(state:State):\n",
        "  \"\"\" Second LLM call to generate Story\"\"\"\n",
        "  msg = llm.invoke(f\"Write a short story about {state('topic')}\")\n",
        "  return {\"story\":msg.content}\n",
        "\n",
        "def call_llm_3(state:State):\n",
        "  \"\"\"Third LLM call to generate Poem\"\"\"\n",
        "  msg = llm.invoke(f\"Write a short poem about {state('topic')}\")\n",
        "  return {\"poem\":msg.content}\n",
        "\n",
        "def aggregator(state:State):\n",
        "  \"\"\" Combine the joke and story into a single output\"\"\"\n",
        "  combined = f\"Here's a story , joke and poem about {state['topic']}!\\n\\n\"\n",
        "  combined += f\"Joke: {state['joke']}\\n\\n\"\n",
        "  combined += f\"Story: {state['story']}\\n\\n\"\n",
        "  combined += f\"Poem: {state['poem']}\"\n",
        "  return {\"combined_output\":combined}"
      ],
      "metadata": {
        "id": "PVUaI_Tgk5D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build parallel workflow\n",
        "parallel_builder = StateGraph(State)\n",
        "\n",
        "#add nodes\n",
        "parallel_builder.add_node(\"call_llm_1\",call_llm_1)\n",
        "parallel_builder.add_node(\"call_llm_2\",call_llm_2)\n",
        "parallel_builder.add_node(\"call_llm_3\",call_llm_3)\n",
        "parallel_builder.add_node(\"aggregator\",aggregator)\n",
        "\n",
        "#add edges to connect nodes\n",
        "parallel_builder.add_edge(START,\"call_llm_1\")\n",
        "parallel_builder.add_edge(START,\"call_llm_2\")\n",
        "parallel_builder.add_edge(START,\"call_llm_3\")\n",
        "parallel_builder.add_edge(\"call_llm_1\",\"aggregator\")\n",
        "parallel_builder.add_edge(\"call_llm_2\",\"aggregator\")\n",
        "parallel_builder.add_edge(\"call_llm_3\",\"aggregator\")\n",
        "parallel_builder.add_edge(\"aggregator\",END)\n",
        "parallel_workflow = parallel_builder.compile()\n",
        "\n",
        "#show workflow\n",
        "display(Image(parallel_workflow.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "RT7yU3tMmVrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#routing\n",
        "#take a input and route the input to poem, story or joke generation based on user input and then aggregate\n",
        "from typing_extensions import Literal\n",
        "class Route(BaseModel):\n",
        "  step:Literal[\"poem\",\"story\",\"joke\"] = Field(None , description = \"The next step in the routing process\")\n",
        "#augment the LLM with schema for structured output\n",
        "router = llm.with_structured_output(Route)"
      ],
      "metadata": {
        "id": "Nwy86dxvnO8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#state for routing\n",
        "#one task and you fan out to sub task to separate LLMs to parallelize the task and do a final aggregration of the task\n",
        "#Graph State\n",
        "class State(TypedDict):\n",
        "  input:str\n",
        "  decision: str\n",
        "  output: str"
      ],
      "metadata": {
        "id": "JuAL-tGtpfiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from langchain_core.messages import HumanMessage , SystemMessage\n",
        "\n",
        "def call_llm_1(state:State):\n",
        "  \"\"\" Write a story \"\"\"\n",
        "  result = llm.invoke(f\"Write a short story about {state('input')}\")\n",
        "  return {\"output\":result.content}\n",
        "\n",
        "def call_llm_2(state:State):\n",
        "  \"\"\" Write a Joke \"\"\"\n",
        "  result = llm.invoke(f\"Write a short joke  about {state('input')}\")\n",
        "  return {\"output\":result.content}\n",
        "\n",
        "def call_llm_3(state:State):\n",
        "  \"\"\" Write a Poem  \"\"\"\n",
        "  result = llm.invoke(f\"Write a short poem  about {state('input')}\")\n",
        "  return {\"output\":result.content}\n",
        "\n",
        "def llm_call_router(state:State):\n",
        "  \"\"\" Route the input to the appropiate node \"\"\"\n",
        "  #run the augmented LLM with structured output to serve as routing logic\n",
        "  decision = router.invoke([SystemMessage(content = \"Route the input to Story , Joke or Poem based on the user's request.\"),\n",
        "                            HumanMessage(content = state(\"input\"))])\n",
        "  return {\"decision\": decision.step}\n",
        "\n",
        "#conditional edge function to route to the appropiate node #dotted line show conditional edge\n",
        "def route_decision(state:State):\n",
        "  #return the node name you want to visit next\n",
        "  if state[\"decision\"] == \"story\":\n",
        "    return call_llm_1\n",
        "  elif state[\"decision\"] == \"joke\":\n",
        "    return call_llm_2\n",
        "  elif state[\"decision\"] == \"poem\":\n",
        "    return call_llm_3\n",
        "  else:\n",
        "    return"
      ],
      "metadata": {
        "id": "xSgtnfSSpxNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build parallel workflow\n",
        "router_builder = StateGraph(State)\n",
        "\n",
        "#add nodes\n",
        "router_builder.add_node(\"call_llm_1\",call_llm_1)\n",
        "router_builder.add_node(\"call_llm_2\",call_llm_2)\n",
        "router_builder.add_node(\"call_llm_3\",call_llm_3)\n",
        "#router_builder.add_node(\"aggregator\",aggregator)\n",
        "\n",
        "#add edges to connect nodes\n",
        "router_builder.add_edge(\"call_llm_1\",END)\n",
        "router_builder.add_edge(\"call_llm_2\",END)\n",
        "router_builder.add_edge(\"call_llm_3\",END)\n",
        "\n",
        "router_workflow = router_builder.compile()\n",
        "#show workflow\n",
        "display(Image(router_builder.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "kRvsFMqzsLNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Orchestrator-Worker - LLM breaks down a task and delegate each task to a worker and synthensize to provide outcome\n",
        "from typing import Annotated , List\n",
        "import operator\n",
        "#schema for structure output to use in planning\n"
      ],
      "metadata": {
        "id": "UtAskB8mtJ22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluator-optimizer workflow\n",
        "#one LLM generates a response while another LLM evaluates and provide feedback in loop\n",
        "#schema for structured output to use in evaluation\n",
        "class Feedback(BaseModel):\n",
        "  grade:Literal[\"funny\",\"not funny\"] = Field(description = \"Decide if the joke is funny or not\",)\n",
        "  feedback :str = Field(description = \"Explain why the joke is funny or not\")\n",
        "\n",
        "#augment the llm with schema for structured output\n",
        "evaluator = llm.with_structured_output(Feedback)"
      ],
      "metadata": {
        "id": "AoYGl7oX5c9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Graph State\n",
        "class State(TypedDict):\n",
        "  joke:str\n",
        "  topic:str\n",
        "  feedback:str\n",
        "  funny_or_not:str"
      ],
      "metadata": {
        "id": "yefaGBr063fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nodes\n",
        "def llm_call_generator(state:State):\n",
        "  \"\"\"LLM generate a joke\"\"\"\n",
        "  if state.get(\"feedback\"):\n",
        "    msg = llm.invoke(f\"Write a joke about {state ['topic']} but take this feedback into account {state['feedback']}\")\n",
        "  else:\n",
        "    msg = llm.invoke(f\"Write a joke about {state ['topic']}\")\n",
        "  return {\"joke\":msg.content}\n",
        "\n",
        "def llm_call_evaluator(state:State):\n",
        "  \"\"\" LLM evaluates a joke \"\"\"\n",
        "  grade = evaluator.invoke(f\"Grade the joke{state['joke']}\")\n",
        "  return {\"funny_or_not\":grade.grade , \"feedback\":grade.feedback}\n",
        "\n",
        "#conditional edge function to route back to joke generator or end based upon feedback from the evaluator\n",
        "def route_joke():\n",
        "  \"\"\" Route the joke back to the joke generator or end based upon feedback from the evaluator\"\"\"\n",
        "  if state[\"funny_or_not\"] == \"funny\":\n",
        "    return \"Accepted\"\n",
        "  elif state[\"funny_or_not\"] == \"not funny\":\n",
        "    return \"Rejected + Feedback\""
      ],
      "metadata": {
        "id": "7VG5ZYEv7Qr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agent\n",
        "#remove scafolding and allow LLM to take actions\n",
        "#define tools using tool decorator\n",
        "#agent\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def multiply(a:int , b:int) ->int:\n",
        "  return a*b\n",
        "\n",
        "@tool\n",
        "def add(a:int , b:int) ->int:\n",
        "  return a+b\n",
        "\n",
        "@tool\n",
        "def divide(a:int , b:int) ->int:\n",
        "  return a/b\n",
        "\n",
        "@tool\n",
        "def subtract(a:int , b:int) ->int:\n",
        "  return a-b\n",
        "\n",
        "#augment the llm with tools\n",
        "tools = [add, multiply, divide, subtract]\n",
        "tools_by_name = {tool.name:tool for tool in tools}\n",
        "llm_with_tools = llm.bind_tools(tools)\n"
      ],
      "metadata": {
        "id": "7tT0nbOE9kak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import ToolMessage\n",
        "from IPython.display import Image, display\n",
        "\n",
        "#Nodes\n",
        "def llm_call(state:MessagesState):\n",
        "  \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
        "  return {\"messages\":[llm_with_tools.invoke([SystemMessage(content= \"You are a helpful Assistant tasked with performing arithmetic on a set of inputs\")]+ state[\"messages\"])]}\n",
        "\n",
        "def tool_node(state:dict):\n",
        "  \"\"\"Call a tool\"\"\"\n",
        "  result = []\n",
        "  for tool_call in state[\"messages\"][-1].tool_calls:\n",
        "    tool = tools_by_name[tool_call[\"name\"]]\n",
        "    observation = tool.invoke(tool_call[\"args\"])\n",
        "    result.append(ToolMessage(content = observation , tool_call_id = tool_call[\"id\"]))\n",
        "  return {\"messages\":result}\n",
        "\n",
        "def should_continue(state:MessagesState) -> Literal[\"enviornment\":\"END\"]:\n",
        "  \"\"\" decide if we should continue the loop or stop based on upon whether the LLM made a tool call\"\"\"\n",
        "  messages = state[\"messages\"]\n",
        "  last_message = messages[-1]\n",
        "  if last_message.tool_calls:\n",
        "        return \"Action\"\n",
        "  return \"END\""
      ],
      "metadata": {
        "id": "zlbibCL6_cLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "CODE OF USING PERSISTENCE MEMORY EITH REACT AGENT USING TOOLS AND ALSO STORING RETREIVING MEMORY\n",
        "* https://docs.langchain.com/oss/python/langgraph/add-memory#add-short-term-memory\n",
        "\n",
        "\n",
        "*   Checkpoints need to be used backed by Persistence Store like a Redis or Postgress of Mongodb\n",
        "*  All memory has to be stored in the persistence memory using a session_id\n",
        "*  These memory can be searched . Memory can be of a Conversational Memory and stored in this system at the end of the conversation   \n",
        "*   Langgraph uses State object for passing memory for in-session\n",
        "*   Langgraph uses Checkpoints for persistence memory and these can be passed to graph nodes or sub-agents\n",
        "*   Tools can be binded to model. USE CREATE_REACT_AGENT\n",
        "*   Memory Management can be Async\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1RtIW0B_XbOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#start code for React Agent using LT memory ....\n",
        "#ST Memory - In session can be handled via State but LT memory we need mem0 or something\n",
        "#we will build the code using mem0 also\n",
        "!pip install langchain\n",
        "!pip install langgraph\n",
        "!pip install -U langmem\n",
        "!pip install mcp\n",
        "!pip install -qU \"langchain[groq]\"\n"
      ],
      "metadata": {
        "id": "lFITbowO2wdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langmem import create_manage_memory_tool , create_search_memory_tool\n"
      ],
      "metadata": {
        "id": "4v0UWhUL5aql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Long term memory management tools to store memory and search memory\n",
        "#when you use MongoDB or Redis you replace this with DB schemas - which are nothing but JSON objects - Key Value Pairs\n",
        "memory_tool =[\n",
        "    create_manage_memory_tool(namespace = (\"agent_memory\",)),\n",
        "    create_search_memory_tool(namespace = (\"agent_memory_search\",))\n",
        "]"
      ],
      "metadata": {
        "id": "OIuiKyzp5Twx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os , getpass\n",
        "import dotenv\n",
        "#setup a memory store\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "#in session memory what langchain offers\n",
        "#if we want permanent memory across session we need to use a persistent memory store like Redis / Mongo-db etc ... refer code as below for that\n",
        "\n",
        "store = InMemoryStore(\n",
        "    index ={\n",
        "        \"dims\":1536,\n",
        "        \"embed\":\"openai:text-embedding-3-small\"\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "OjKlyCbj_tGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from typing import List , Literal\n",
        "@tool\n",
        "def add(a:int,b:int) ->int:\n",
        "  \"\"\"Adds two integers and returns the result.\"\"\"\n",
        "  return a+b"
      ],
      "metadata": {
        "id": "O0XkZxg2LA1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8695846"
      },
      "source": [
        "model = init_chat_model(\"qwen/qwen3-32b\", api_key= GROQ_API_KEY ,model_provider=\"groq\")\n",
        "#add memory checkpointer\n",
        "checkpointer = MemorySaver() #this checkpointer gets changed to Mongo or Redis when we want persistent store and this can be Async\n",
        "#in case of marketing agent we did these  things\n",
        "#1 -Used Redis as persistent store and in that mem0 for checkpointer\n",
        "#2 -Used compressing the context ....\n",
        "#3 -Used langmem to do so\n",
        "#4 -tool call binding as well\n",
        "#5 -search the memory\n",
        "#activate react agent\n",
        "#add the model and memory management tool into the agent - create and search memory\n",
        "#*args and **kwargs are used to allow functions to accept an arbitrary number of arguments\n",
        "agent = create_react_agent(model=model,tools = [add,*memory_tool],checkpointer=checkpointer,store=store)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#start executing the agent\n",
        "text = \"Hi, Please create marketing plan for reducing customer acquistion cost. This plan is for CMO Exec Level so plan should be executable\"\n",
        "session_id = 1\n",
        "result = agent.invoke({\"messages\":[{\"role\":\"user\",\"content\":text}]},config ={\"configurable\":{\"session_id\":session_id, \"thread_id\": str(session_id)}} )\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "FcFWdI6MGmB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding more text into the prompt\n",
        "text =\"also pls suggest campaigns\"\n",
        "session_id = 1\n",
        "result = agent.invoke({\"messages\":[{\"role\":\"user\",\"content\":text}]},config ={\"configurable\":{\"session_id\":session_id, \"thread_id\": str(session_id)}} )\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "g0faBYmbNhld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now change the session_id and check memory\n",
        "text = \"generate marketing plan for increasing sales\"\n",
        "session_id = 2\n",
        "result = agent.invoke({\"messages\":[{\"role\":\"user\",\"content\":text}]},config ={\"configurable\":{\"session_id\":session_id, \"thread_id\": str(session_id)}} )\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "xxpuKKd0OTjc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}