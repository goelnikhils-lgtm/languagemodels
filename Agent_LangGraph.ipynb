{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaeYkOoGaeizVWDKYReiaH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/Agent_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ6zbxHpWyL-"
      },
      "outputs": [],
      "source": [
        "#building agents from scratch .by god's grace. Jai Shri Ram . Jai Bajrangbali\n",
        "#building agents using MCP Protcol\n",
        "#extending the agent to react and using memory in this -> using langmem sdk from langrapgh -> 1102\n",
        "#https://www.youtube.com/watch?v=aHCDrAbH_go&t=120s - LangGraph\n",
        "#https://www.analyticsvidhya.com/blog/2025/03/langmem-sdk/ - Analytics Vidhya\n",
        "#https://docs.langchain.com/oss/python/langgraph/add-memory#add-short-term-memory\n",
        "#https://medium.com/@devwithll/simple-langgraph-implementation-with-memory-asyncsqlitesaver-checkpointer-fastapi-54f4e4879a2e - CODE FOR RUNNING CONV AI USING GRAPH\n",
        "!pip install langchain_core langchain-anthropic langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import os ,getpass\n",
        "def _set_env(var:str):\n",
        "    os.environ[var] = getpass.getpass(f\"Enter your {var}: \")\n",
        "_set_env(\"ANTHROPIC_API_KEY\")"
      ],
      "metadata": {
        "id": "MuLs5YP-XOMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#llm\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n"
      ],
      "metadata": {
        "id": "QqjNb4ycYRdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#schema for structured output\n",
        "from pydantic import BaseModel , Field\n",
        "class SearchQuery(BaseModel):\n",
        "    search_query: str = Field(None , description=\"Query that is optimized web search.\")\n",
        "    justification: str = Field(None , description=\"Why this query is relevant to user's request.\")\n",
        "\n",
        "#Augment the LLM with schema for structured output\n",
        "structured_llm = llm.with_structured_output(SearchQuery)\n",
        "\n",
        "#invoke the augmented LLM\n",
        "output = structured_llm.invoke(\"How does Calcium CT score relate to high chlorestrol levels?\")\n",
        "print(output.search_query)\n",
        "print(output.justification)"
      ],
      "metadata": {
        "id": "-k71NsC-Ykb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tool calling\n",
        "def multiply(a:int,b:int) ->int:\n",
        "  return a*b\n",
        "#augment the LLM with tools\n",
        "llm_with_tools = llm.bind_tools([multiply])\n",
        "#invoke the LLM with input that triggers the tool call\n",
        "msg = llm_with_tools.invoke(\"What is 2 times 3?\")\n",
        "print(msg)\n",
        "\n",
        "#get the tool call\n",
        "msg.tool_calls"
      ],
      "metadata": {
        "id": "7JOHCqIVa4R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt chaining\n",
        "#each llm processes the output of the other llm in a chain\n",
        "#when do you want to do this\n",
        "#use it in Conv AI .... in LLM-as-a-judge\n",
        "#!pip install typing_extensions\n",
        "from typing_extensions import TypedDict\n",
        "#graph state for passing thru one llm to another llm. this is KEY\n",
        "class State(TypedDict):\n",
        "  topic: str\n",
        "  joke: str\n",
        "  improved_joke: str\n",
        "  final_joke: str"
      ],
      "metadata": {
        "id": "eeiY2laLbr3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nodes\n",
        "def generate_joke(state:State):\n",
        "  \"\"\"First LLM call to generate initial joke\"\"\"\n",
        "  msg = llm.invoke(f\"Write a short joke {state('topic')}\")\n",
        "  return {\"joke\":msg.content}\n",
        "\n",
        "def improve_joke(state:State):\n",
        "  \"\"\"Second LLM call to improve the joke\"\"\"\n",
        "  msg = llm.invoke(f\"Make this joke funnier by adding wordplay{state('joke')}\")\n",
        "  return {\"improved_joke\":msg.content}\n",
        "\n",
        "def polish_joke(state:State):\n",
        "  \"\"\"Third LLM call for final polish of the joke content to make it better\"\"\"\n",
        "  msg = llm.invoke(f\"Add a surprising twist to this joke{state('joke')}\")\n",
        "  return {\"final_joke\": msg.content}\n",
        "\n",
        "#conditional edge function to check if the joke has a punchline\n",
        "def check_punchline(state:State): #conditional EDGE In Langgraph - what is the condition to move from one llm to another\n",
        "    \"\"\"Gate Function to check if the joke has a punchline\"\"\"\n",
        "    if \"?\" in state[\"improved_joke\"] or \"!\" in state[\"joke\"]:\n",
        "      return \"Pass\"\n",
        "    return \"Fail\""
      ],
      "metadata": {
        "id": "hOYkkSWIcm4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#langgraph simple workflow\n",
        "from langgraph.graph import StateGraph , START , END\n",
        "from IPython.display import Image, display\n",
        "\n",
        "#build workflow\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(generate_joke, generate_joke)\n",
        "workflow.add_node(improve_joke,improve_joke)\n",
        "workflow.add_node(polish_joke,polish_joke)\n",
        "\n",
        "#add edges to connect nodes\n",
        "workflow.add_edge(START,\"generate_joke\")\n",
        "workflow.add_conditional_edges(\"generate_joke\",check_punchline , {\"Pass\":\"improve_joke\", \"Fail\":END})\n",
        "workflow.add_edge(\"improve_joke\",\"polish_joke\")\n",
        "workflow.add_edge(\"polish_joke\",END)\n",
        "\n",
        "#compile workflow\n",
        "chain = workflow.compile()\n",
        "\n",
        "#show workflow\n",
        "display(Image(chain.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "qyyGyMPyf_U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "state = chain.invoke({\"topic\":\"cats\"})\n",
        "print(\"Intial Joke:\")\n",
        "print(state[\"joke\"])\n",
        "print(\"\\n=== === ===\\n\")\n",
        "if \"improved_joke\" in state:\n",
        "  print(\"Improved Joke:\")\n",
        "  print(state[\"improved_joke\"])\n",
        "  print(\"Final Joke:\")\n",
        "  print(state[\"final_joke\"])\n",
        "else:\n",
        "  print(\"joke failed quality gate = no punchline detected\")"
      ],
      "metadata": {
        "id": "y0zQE7VljCD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parallelization\n",
        "#one task and you fan out to sub task to separate LLMs to parallelize the task and do a final aggregration of the task\n",
        "#Graph State\n",
        "class State(TypedDict):\n",
        "  topic:str\n",
        "  joke: str\n",
        "  story: str\n",
        "  poem:str\n",
        "  combined_output: str"
      ],
      "metadata": {
        "id": "YbaHd2mHkHQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nodes\n",
        "def call_llm_1(state:State):\n",
        "  \"\"\" Fist LLM call to generate intial joke \"\"\"\n",
        "  msg = llm.invoke(f\"Write a short joke about {state('topic')}\")\n",
        "  return {\"joke\":msg.content}\n",
        "\n",
        "def call_llm_2(state:State):\n",
        "  \"\"\" Second LLM call to generate Story\"\"\"\n",
        "  msg = llm.invoke(f\"Write a short story about {state('topic')}\")\n",
        "  return {\"story\":msg.content}\n",
        "\n",
        "def call_llm_3(state:State):\n",
        "  \"\"\"Third LLM call to generate Poem\"\"\"\n",
        "  msg = llm.invoke(f\"Write a short poem about {state('topic')}\")\n",
        "  return {\"poem\":msg.content}\n",
        "\n",
        "def aggregator(state:State):\n",
        "  \"\"\" Combine the joke and story into a single output\"\"\"\n",
        "  combined = f\"Here's a story , joke and poem about {state['topic']}!\\n\\n\"\n",
        "  combined += f\"Joke: {state['joke']}\\n\\n\"\n",
        "  combined += f\"Story: {state['story']}\\n\\n\"\n",
        "  combined += f\"Poem: {state['poem']}\"\n",
        "  return {\"combined_output\":combined}"
      ],
      "metadata": {
        "id": "PVUaI_Tgk5D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build parallel workflow\n",
        "parallel_builder = StateGraph(State)\n",
        "\n",
        "#add nodes\n",
        "parallel_builder.add_node(\"call_llm_1\",call_llm_1)\n",
        "parallel_builder.add_node(\"call_llm_2\",call_llm_2)\n",
        "parallel_builder.add_node(\"call_llm_3\",call_llm_3)\n",
        "parallel_builder.add_node(\"aggregator\",aggregator)\n",
        "\n",
        "#add edges to connect nodes\n",
        "parallel_builder.add_edge(START,\"call_llm_1\")\n",
        "parallel_builder.add_edge(START,\"call_llm_2\")\n",
        "parallel_builder.add_edge(START,\"call_llm_3\")\n",
        "parallel_builder.add_edge(\"call_llm_1\",\"aggregator\")\n",
        "parallel_builder.add_edge(\"call_llm_2\",\"aggregator\")\n",
        "parallel_builder.add_edge(\"call_llm_3\",\"aggregator\")\n",
        "parallel_builder.add_edge(\"aggregator\",END)\n",
        "parallel_workflow = parallel_builder.compile()\n",
        "\n",
        "#show workflow\n",
        "display(Image(parallel_workflow.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "RT7yU3tMmVrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#routing\n",
        "#take a input and route the input to poem, story or joke generation based on user input and then aggregate\n",
        "from typing_extensions import Literal\n",
        "class Route(BaseModel):\n",
        "  step:Literal[\"poem\",\"story\",\"joke\"] = Field(None , description = \"The next step in the routing process\")\n",
        "#augment the LLM with schema for structured output\n",
        "router = llm.with_structured_output(Route)"
      ],
      "metadata": {
        "id": "Nwy86dxvnO8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#state for routing\n",
        "#one task and you fan out to sub task to separate LLMs to parallelize the task and do a final aggregration of the task\n",
        "#Graph State\n",
        "class State(TypedDict):\n",
        "  input:str\n",
        "  decision: str\n",
        "  output: str"
      ],
      "metadata": {
        "id": "JuAL-tGtpfiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from langchain_core.messages import HumanMessage , SystemMessage\n",
        "\n",
        "def call_llm_1(state:State):\n",
        "  \"\"\" Write a story \"\"\"\n",
        "  result = llm.invoke(f\"Write a short story about {state('input')}\")\n",
        "  return {\"output\":result.content}\n",
        "\n",
        "def call_llm_2(state:State):\n",
        "  \"\"\" Write a Joke \"\"\"\n",
        "  result = llm.invoke(f\"Write a short joke  about {state('input')}\")\n",
        "  return {\"output\":result.content}\n",
        "\n",
        "def call_llm_3(state:State):\n",
        "  \"\"\" Write a Poem  \"\"\"\n",
        "  result = llm.invoke(f\"Write a short poem  about {state('input')}\")\n",
        "  return {\"output\":result.content}\n",
        "\n",
        "def llm_call_router(state:State):\n",
        "  \"\"\" Route the input to the appropiate node \"\"\"\n",
        "  #run the augmented LLM with structured output to serve as routing logic\n",
        "  decision = router.invoke([SystemMessage(content = \"Route the input to Story , Joke or Poem based on the user's request.\"),\n",
        "                            HumanMessage(content = state(\"input\"))])\n",
        "  return {\"decision\": decision.step}\n",
        "\n",
        "#conditional edge function to route to the appropiate node #dotted line show conditional edge\n",
        "def route_decision(state:State):\n",
        "  #return the node name you want to visit next\n",
        "  if state[\"decision\"] == \"story\":\n",
        "    return call_llm_1\n",
        "  elif state[\"decision\"] == \"joke\":\n",
        "    return call_llm_2\n",
        "  elif state[\"decision\"] == \"poem\":\n",
        "    return call_llm_3\n",
        "  else:\n",
        "    return"
      ],
      "metadata": {
        "id": "xSgtnfSSpxNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build parallel workflow\n",
        "router_builder = StateGraph(State)\n",
        "\n",
        "#add nodes\n",
        "router_builder.add_node(\"call_llm_1\",call_llm_1)\n",
        "router_builder.add_node(\"call_llm_2\",call_llm_2)\n",
        "router_builder.add_node(\"call_llm_3\",call_llm_3)\n",
        "#router_builder.add_node(\"aggregator\",aggregator)\n",
        "\n",
        "#add edges to connect nodes\n",
        "router_builder.add_edge(\"call_llm_1\",END)\n",
        "router_builder.add_edge(\"call_llm_2\",END)\n",
        "router_builder.add_edge(\"call_llm_3\",END)\n",
        "\n",
        "router_workflow = router_builder.compile()\n",
        "#show workflow\n",
        "display(Image(router_builder.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "kRvsFMqzsLNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Orchestrator-Worker - LLM breaks down a task and delegate each task to a worker and synthensize to provide outcome\n",
        "from typing import Annotated , List\n",
        "import operator\n",
        "#schema for structure output to use in planning\n"
      ],
      "metadata": {
        "id": "UtAskB8mtJ22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluator-optimizer workflow\n",
        "#one LLM generates a response while another LLM evaluates and provide feedback in loop\n",
        "#schema for structured output to use in evaluation\n",
        "class Feedback(BaseModel):\n",
        "  grade:Literal[\"funny\",\"not funny\"] = Field(description = \"Decide if the joke is funny or not\",)\n",
        "  feedback :str = Field(description = \"Explain why the joke is funny or not\")\n",
        "\n",
        "#augment the llm with schema for structured output\n",
        "evaluator = llm.with_structured_output(Feedback)"
      ],
      "metadata": {
        "id": "AoYGl7oX5c9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Graph State\n",
        "class State(TypedDict):\n",
        "  joke:str\n",
        "  topic:str\n",
        "  feedback:str\n",
        "  funny_or_not:str"
      ],
      "metadata": {
        "id": "yefaGBr063fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nodes\n",
        "def llm_call_generator(state:State):\n",
        "  \"\"\"LLM generate a joke\"\"\"\n",
        "  if state.get(\"feedback\"):\n",
        "    msg = llm.invoke(f\"Write a joke about {state ['topic']} but take this feedback into account {state['feedback']}\")\n",
        "  else:\n",
        "    msg = llm.invoke(f\"Write a joke about {state ['topic']}\")\n",
        "  return {\"joke\":msg.content}\n",
        "\n",
        "def llm_call_evaluator(state:State):\n",
        "  \"\"\" LLM evaluates a joke \"\"\"\n",
        "  grade = evaluator.invoke(f\"Grade the joke{state['joke']}\")\n",
        "  return {\"funny_or_not\":grade.grade , \"feedback\":grade.feedback}\n",
        "\n",
        "#conditional edge function to route back to joke generator or end based upon feedback from the evaluator\n",
        "def route_joke():\n",
        "  \"\"\" Route the joke back to the joke generator or end based upon feedback from the evaluator\"\"\"\n",
        "  if state[\"funny_or_not\"] == \"funny\":\n",
        "    return \"Accepted\"\n",
        "  elif state[\"funny_or_not\"] == \"not funny\":\n",
        "    return \"Rejected + Feedback\""
      ],
      "metadata": {
        "id": "7VG5ZYEv7Qr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agent\n",
        "#remove scafolding and allow LLM to take actions\n",
        "#define tools using tool decorator\n",
        "#agent\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def multiply(a:int , b:int) ->int:\n",
        "  return a*b\n",
        "\n",
        "@tool\n",
        "def add(a:int , b:int) ->int:\n",
        "  return a+b\n",
        "\n",
        "@tool\n",
        "def divide(a:int , b:int) ->int:\n",
        "  return a/b\n",
        "\n",
        "@tool\n",
        "def subtract(a:int , b:int) ->int:\n",
        "  return a-b\n",
        "\n",
        "#augment the llm with tools\n",
        "tools = [add, multiply, divide, subtract]\n",
        "tools_by_name = {tool.name:tool for tool in tools}\n",
        "llm_with_tools = llm.bind_tools(tools)\n"
      ],
      "metadata": {
        "id": "7tT0nbOE9kak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import ToolMessage\n",
        "from IPython.display import Image, display\n",
        "\n",
        "#Nodes\n",
        "def llm_call(state:MessagesState):\n",
        "  \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
        "  return {\"messages\":[llm_with_tools.invoke([SystemMessage(content= \"You are a helpful Assistant tasked with performing arithmetic on a set of inputs\")]+ state[\"messages\"])]}\n",
        "\n",
        "def tool_node(state:dict):\n",
        "  \"\"\"Call a tool\"\"\"\n",
        "  result = []\n",
        "  for tool_call in state[\"messages\"][-1].tool_calls:\n",
        "    tool = tools_by_name[tool_call[\"name\"]]\n",
        "    observation = tool.invoke(tool_call[\"args\"])\n",
        "    result.append(ToolMessage(content = observation , tool_call_id = tool_call[\"id\"]))\n",
        "  return {\"messages\":result}\n",
        "\n",
        "def should_continue(state:MessagesState) -> Literal[\"enviornment\":\"END\"]:\n",
        "  \"\"\" decide if we should continue the loop or stop based on upon whether the LLM made a tool call\"\"\"\n",
        "  messages = state[\"messages\"]\n",
        "  last_message = messages[-1]\n",
        "  if last_message.tool_calls:\n",
        "        return \"Action\"\n",
        "  return \"END\""
      ],
      "metadata": {
        "id": "zlbibCL6_cLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "CODE OF USING PERSISTENCE MEMORY EITH REACT AGENT USING TOOLS AND ALSO STORING RETREIVING MEMORY\n",
        "* https://docs.langchain.com/oss/python/langgraph/add-memory#add-short-term-memory\n",
        "\n",
        "\n",
        "*   Checkpoints need to be used backed by Persistence Store like a Redis or Postgress of Mongodb\n",
        "*  All memory has to be stored in the persistence memory using a session_id\n",
        "*  These memory can be searched . Memory can be of a Conversational Memory and stored in this system at the end of the conversation   \n",
        "*   Langgraph uses State object for passing memory for in-session\n",
        "*   Langgraph uses Checkpoints for persistence memory and these can be passed to graph nodes or sub-agents\n",
        "*   Tools can be binded to model. USE CREATE_REACT_AGENT\n",
        "*   Memory Management should be Async when handling production grade Conv Agents\n",
        "*   Handle Rate Limits with foundational models\n",
        "*   Error handling try :except block with proper handling of error should be there\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1RtIW0B_XbOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#start code for React Agent using LT memory ....\n",
        "#ST Memory - In session can be handled via State but LT memory we need mem0 or something\n",
        "#we will build the code using mem0 also\n",
        "!pip install langchain\n",
        "!pip install langgraph\n",
        "!pip install -U langmem\n",
        "!pip install mcp\n",
        "!pip install -qU \"langchain[groq]\"\n"
      ],
      "metadata": {
        "id": "lFITbowO2wdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langmem import create_manage_memory_tool , create_search_memory_tool\n"
      ],
      "metadata": {
        "id": "4v0UWhUL5aql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Long term memory management tools to store memory and search memory\n",
        "#when you use MongoDB or Redis you replace this with DB schemas - which are nothing but JSON objects - Key Value Pairs\n",
        "memory_tool =[\n",
        "    create_manage_memory_tool(namespace = (\"agent_memory\",)),\n",
        "    create_search_memory_tool(namespace = (\"agent_memory_search\",))\n",
        "]"
      ],
      "metadata": {
        "id": "OIuiKyzp5Twx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os , getpass\n",
        "import dotenv\n",
        "#setup a memory store\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "#in session memory what langchain offers\n",
        "#if we want permanent memory across session we need to use a persistent memory store like Redis / Mongo-db etc ... refer code as below for that\n",
        "\n",
        "store = InMemoryStore(\n",
        "    index ={\n",
        "        \"dims\":1536,\n",
        "        \"embed\":\"openai:text-embedding-3-small\"\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "OjKlyCbj_tGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from typing import List , Literal\n",
        "@tool\n",
        "def add(a:int,b:int) ->int:\n",
        "  \"\"\"Adds two integers and returns the result.\"\"\"\n",
        "  return a+b"
      ],
      "metadata": {
        "id": "O0XkZxg2LA1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8695846"
      },
      "source": [
        "model = init_chat_model(\"qwen/qwen3-32b\", api_key= GROQ_API_KEY ,model_provider=\"groq\")\n",
        "#add memory checkpointer\n",
        "checkpointer = MemorySaver() #this checkpointer gets changed to Mongo or Redis when we want persistent store and this can be Async\n",
        "#in case of marketing agent we did these  things\n",
        "#1 -Used Redis as persistent store and in that mem0 for checkpointer\n",
        "#2 -Used compressing the context ....\n",
        "#3 -Used langmem to do so\n",
        "#4 -tool call binding as well\n",
        "#5 -search the memory\n",
        "#activate react agent\n",
        "#add the model and memory management tool into the agent - create and search memory\n",
        "#*args and **kwargs are used to allow functions to accept an arbitrary number of arguments\n",
        "agent = create_react_agent(model=model,tools = [add,*memory_tool],checkpointer=checkpointer,store=store)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#start executing the agent\n",
        "text = \"Hi, Please create marketing plan for reducing customer acquistion cost. This plan is for CMO Exec Level so plan should be executable\"\n",
        "session_id = 1\n",
        "result = agent.invoke({\"messages\":[{\"role\":\"user\",\"content\":text}]},config ={\"configurable\":{\"session_id\":session_id, \"thread_id\": str(session_id)}} )\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "FcFWdI6MGmB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding more text into the prompt\n",
        "text =\"also pls suggest campaigns\"\n",
        "session_id = 1\n",
        "result = agent.invoke({\"messages\":[{\"role\":\"user\",\"content\":text}]},config ={\"configurable\":{\"session_id\":session_id, \"thread_id\": str(session_id)}} )\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "g0faBYmbNhld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now change the session_id and check memory\n",
        "text = \"generate marketing plan for increasing sales\"\n",
        "session_id = 2\n",
        "result = agent.invoke({\"messages\":[{\"role\":\"user\",\"content\":text}]},config ={\"configurable\":{\"session_id\":session_id, \"thread_id\": str(session_id)}} )\n",
        "print(result[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "xxpuKKd0OTjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#leveraging langmem\n",
        "namespace = {\"agent_memory\",\"{user_id}\"} # creataing a agent memory segregrated by user_id - This is what I did\n",
        "text = \"pls publish my marketing plan\"\n",
        "session_id = 2\n",
        "user_id = \"ab\"\n",
        "result = agent.invoke({\"messages\":[{\"role\":\"user\",\"content\":text}]},config ={\"configurable\":{\"session_id\":session_id, \"thread_id\": user_id}} )"
      ],
      "metadata": {
        "id": "9TOXxRe8hiSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "items = store.search(\"agenr_memory\",)\n",
        "for item in items:\n",
        "  print(item.namespace,item.value)"
      ],
      "metadata": {
        "id": "6SiHBlE9ocTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi\n",
        "!pip install langgraph-checkpoint-sqlite\n"
      ],
      "metadata": {
        "id": "xBhmFKWfFah8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#develop conv agent with memory and asyn using langgraph and langchain\n",
        "import os\n",
        "import uvicorn\n",
        "from dotenv import load_dotenv\n",
        "from typing import List, TypedDict , Annotated\n",
        "from contextlib import asynccontextmanager\n",
        "\n",
        "#Fast API Imports\n",
        "from fastapi import  FastAPI , Request , Form\n",
        "from fastapi.responses import HTMLResponse , RedirectResponse\n",
        "from fastapi.templating import Jinja2Templates\n",
        "from starlette import status\n",
        "\n",
        "#import langchain /langgraph imports\n",
        "from langchain_core.messages import AIMessage , HumanMessage , BaseMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
        "from langgraph.graph import StateGraph , END"
      ],
      "metadata": {
        "id": "FdoUOmog-juj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Langgraph state will have three things to store\n",
        "#user prompt\n",
        "#message history\n",
        "#response from LLM\n",
        "\n",
        "class ChatState(TypedDict):\n",
        "  user_prompt:str\n",
        "  messages:Annotated[List[BaseMessage],add_messages] #Annotated type(list) with metadata. #add_messages is a reducer\n",
        "  response:str\n"
      ],
      "metadata": {
        "id": "TPAJOCvtGNR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the virtual assistant\n",
        "\n",
        "async def process_user_prompt_node(state:ChatState):\n",
        "  user_message = state[\"user_prompt\"]\n",
        "  return {\"messages\":[HumanMessage(content=user_prompt)]}\n",
        "\n",
        "async def call_model_node(state:ChatState):\n",
        "  llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
        "  messages = state[\"messages\"]\n",
        "  if not messages or not isinstance(messages[-1],HumanMessage):\n",
        "    return{\"response\":\"no message generated by LLM\"}\n",
        "  try:\n",
        "    #call llm for response\n",
        "    response = await llm.ainvoke(messages) #async operation of calling LLM\n",
        "    #return the LLM response\n",
        "    return{\"response\":response.content}\n",
        "  except Exception as e:\n",
        "    return{\"response\":\"sorry, error enountered no response from LLM\"}\n",
        "\n",
        "async def process_bot_response_node(state:ChatState):\n",
        "  bot_response = state[\"response\"]\n",
        "  return {\"messages\":[AIMessage(content=bot_response)]}\n"
      ],
      "metadata": {
        "id": "IGWHdj9FHZfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FastAPI LifeSpan Function\n",
        "@asynccontextmanager\n",
        "async def lifespan(app:FastAPI):\n",
        "  print(\"Starting up:Intializing Resources\")\n",
        "\n",
        "  async with AsyncSqliteSaver.from_conn_string(SQLITE_DATABASE_PATH) as checkpointer:\n",
        "    print(\"AsyncSqliteSaver Connection Established\")\n",
        "    #intiate the graph\n",
        "    workflow = StateGraph(ChatState) #intialize the graph with persistent chat state #THANKS A LOT GOD\n",
        "\n",
        "    #add nodes\n",
        "    workflow.add_node(\"process_user_prompt\",process_user_prompt_node)\n",
        "    workflow.add_node(\"call_model\",call_model_node)\n",
        "    workflow.add_node(\"generate_response\",END)\n",
        "\n",
        "    #add edges\n",
        "    workflow.set_entry_point(\"process_user_prompt\")\n",
        "    workflow.add_edge(\"process_user_prompt\",\"call_model\")\n",
        "    workflow.add_edge(\"call_model\",\"generate_response\")\n",
        "    workflow.add_edge(\"generate_response\",END)\n",
        "\n",
        "    #compile the graph\n",
        "    app.state.graph = workflow.compile(checkpointer = checkpointer) #this checkpointer can be Redis or Postgress depending upon the persistent store\n",
        "    app.state.checkpointer = checkpointer\n",
        "  yield\n",
        "  print(\"Shutting down:closing resources\")\n"
      ],
      "metadata": {
        "id": "wQ_NlyuOLL74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#helper function for memory management/ storing chats and retrieving those\n",
        "async def get_chat_history_messages(request:Request,thread_id:str):\n",
        "   #request is of object type FastAPI request\n",
        "   config = {\"configurable\":{\"thread_id\":thread_id}}\n",
        "   chat_history_messages = []\n",
        "   app_graph = request.app.chat.graph\n",
        "   try:\n",
        "    current_state = await app_graph.aget_state(config)\n",
        "    if current_state and current_state.values.get(\"messages\"):\n",
        "      chat_history_messages = current_state.values[\"messages\"]\n",
        "      print(f\"loaded state in memory {len(chat_history_messages)} messages for session {thread_id}\")\n",
        "    else:\n",
        "      print(\"no loaded state\")\n",
        "   except Exception as e:\n",
        "      print(\"Exception\")\n",
        "   return chat_history_messages"
      ],
      "metadata": {
        "id": "LCZ3VQsFOlos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#intialize FastAPI App Intialization with LifeSpan\n",
        "\n",
        "app = FastAPI(lifespan = lifespan)\n",
        "\n",
        "#mount jinja templates\n",
        "templates = Jinja2Templates(directory = \"templates\")"
      ],
      "metadata": {
        "id": "VwPyHFQfQy03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FAST API end points\n",
        "SESSION_ID = 1\n",
        "@app.get(\"/\",response_class = HTMLResponse)\n",
        "async def read_root(request:Request):\n",
        "  \"\"\" Render chat page with Conversation History\"\"\"\n",
        "  #get the chat history for the current session\n",
        "  chat_history = await get_chat_history_messages(request,SESSION_ID)\n",
        "  return templates.TemplateResponse(\"chat.html\",{\"request\":request,\"chat_history\":chat_history})\n",
        "@app.post(\"/chat\")\n",
        "async def chat_endpoint(request:Request,user_prompt:str = Form(...)):\n",
        "  if not user_prompt:\n",
        "    return RedirectResponse(url=\"/\",status_code = status.HTTP_303_SEE_OTHER)\n",
        "  graph_input = {\"user_prompt\":user_prompt}\n",
        "  #create a configurable dictionary with the current session / thread ID\n",
        "  config = {\"configurable\":{\"thread_id\":SESSION_ID}}\n",
        "  app_graph = request.app.state.graph\n",
        "  try:\n",
        "    await app_graph.ainvoke(graph_input,config = config)\n",
        "  except Exception as e:\n",
        "    print(f\"error invoking graph for session id {SESSION_ID} : {e}\" )\n",
        "  return RedirectResponse(url=\"/\",status_code = status.HTTP_303_SEE_OTHER)\n"
      ],
      "metadata": {
        "id": "6HUsCu6WRKS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run the FastAPI app....\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "import asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Create a Uvicorn server configuration\n",
        "config = uvicorn.Config(\"app:app\", host=\"0.0.0.0\", reload=False, loop=\"asyncio\")\n",
        "\n",
        "# Create a Uvicorn server instance\n",
        "server = uvicorn.Server(config)\n",
        "\n",
        "# Run the server in the existing event loop\n",
        "# This will block until the server is stopped\n",
        "asyncio.run(server.serve())"
      ],
      "metadata": {
        "id": "yfM7lQMFTy2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code for MCP\n",
        "#why MCP - standardize away to access tools and give it to LLM\n",
        "#agents can be exposed MCP - this can enable Agent Factories - Agent as a tool Pattern\n",
        "#subagents can run in there own sandbox(E2B) and can complete there own task independently\n",
        "#I used subagents for Code and Text2sql  run in there own sandbox - this helps for security\n",
        "#Subagents chaining and comm is done thru message passing by the main agent / Orchestrator pattern ....\n",
        "#Subagents are registered in a registry https://www.vladsnewsletter.com/p/sub-agents\n",
        "#A2A communication can be thru Schema....\n",
        "#Master Agent forces a schema on sub agents and sub agents respond back on that schema\n",
        "#Agents can use VFS - Virtual File System , Todo-list , System prompt and sub agent\n",
        "\n",
        "#MCP\n",
        "\"\"\"\n",
        "1. MCP Prompt - what protocol , what are tool inputs , return - text , image etc....\n",
        "2. MCP Client\n",
        "3. MCP Server\n",
        "4. Resources\n",
        "\"\"\"\n",
        "\n",
        "#https://thenewstack.io/15-best-practices-for-building-mcp-servers-in-production/\n",
        "#https://thenewstack.io/how-elicitation-in-mcp-brings-human-in-the-loop-to-ai-tools/\n",
        "\n",
        "#Agents -> use MCP key factors:\n",
        "\"\"\"\n",
        "1.Use Aysnc\n",
        "2.Use Error handling / time out from MCP server\n",
        "3.OAuth Security\n",
        "4.Elicitation of response from Human .... Human + MCP\n",
        "5.use json-rpc or HTTPStreaming for faster response from MCP service\n",
        "6.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "tNfu95dW51Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#simple mcp server\n",
        "!pip install fastmcp"
      ],
      "metadata": {
        "id": "uT6jBGudAYyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR OWN CUSTOM MCP SERVER\n",
        "\n",
        "from fastmcp import FastMCP\n",
        "\n",
        "server = FastMCP(\"Demo\")\n",
        "@server.tool()\n",
        "async def add(a:int,b:int) ->int:\n",
        "  return a+b\n",
        "@server.tool()\n",
        "async def multiply(a:int,b:int) ->int:\n",
        "  return a*b\n",
        "\n",
        "@server.tool()\n",
        "async def divide(a:int,b:int) ->int:\n",
        "  return a/b\n",
        "\n",
        "@server.tool()\n",
        "async def subtract(a:int,b:int) ->int:\n",
        "  return a-b\n",
        "\n",
        "@server.tool()\n",
        "async def greet_user_formal_tool(name:str) ->str:\n",
        "\n",
        "  \"\"\"\n",
        "  A tool that returns a greeting message in a very formal  tone\n",
        "  Args:\n",
        "  name(str): The name of person to greet\n",
        "  Returns:\n",
        "    str: A formal greeting message for the given name\n",
        "  \"\"\"\n",
        "  return f\"Hello {name}! How can I assist you today?\"\n",
        "\n",
        "@server.tool()\n",
        "async def greet_user_street_style_tool(name:str) ->str:\n",
        "\n",
        "  \"\"\"\n",
        "  A tool that returns a greeting message in street style\n",
        "  Args:\n",
        "  name(str):\n",
        "  Returns:\n",
        "    str:A street style greeting message for the given name\n",
        "  \"\"\"\n",
        "  return f\"Yo {name}! Wassup ? You good?\"\n",
        "\n",
        "@server.prompt\n",
        "def greet_user_prompt(name:str) ->str: # this enables formal or street style depending upon the\n",
        "  \"\"\" Generates  a message asking for a greeting\"\"\"\n",
        "  return f\"\"\"\n",
        "  Return a greeting message for a user called '{name}'\n",
        "  if the user is called 'Laurent' , use a formal style , else use a street style\n",
        "  \"\"\"\n",
        "if __name__ == \"_main()__\":\n",
        "  server.run()"
      ],
      "metadata": {
        "id": "hIiLMfRNHTmL",
        "outputId": "1ffdbfd7-3937-4c46-cfde-03dbb6eeee02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#langchain code to use MCP server as a client\n",
        "!pip install langchain-mcp-adapters"
      ],
      "metadata": {
        "id": "2OHY6rt7MCpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#call MCP tools\n",
        "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
        "from langchain.agents import create_agent\n",
        "\n",
        "client = MultiServerMCPClient(\n",
        "    {\n",
        "        \"math\":{\n",
        "            \"transport\":\"stdio\", #local subprocess communication\n",
        "            \"command\":\"python\",\n",
        "            #absolute path to your math_server.py\n",
        "            \"args\":[\"path/to/math_server.py\"]\n",
        "        },\n",
        "        \"greeting\":{\n",
        "            \"transport\":\"stdio\",\n",
        "            \"command\":\"python\",\n",
        "            \"args\":[\"path/to/greeting_server.py\"]\n",
        "        },\n",
        "        \"weather\":{\n",
        "            \"transport\":\"stdio\",\n",
        "            \"command\":\"python\",\n",
        "            \"args\":[\"path/to/weather_server.py\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        ")\n",
        "tools = await client.get_tools()\n",
        "agent = create_agent(\"claude-sonnet-4-5-20250929\",tools=tools)\n",
        "math_response = await agent.invoke({\"messages\":[{\"role\":\"user\", \"content\":\"what's(3+5)X12\"}]})\n",
        "weather_response = await agent.invoke({\"messages\":[{\"role\":\"user\", \"content\":\"what is the weather in NYC\"}]})"
      ],
      "metadata": {
        "id": "SQ7eE-G3MW2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DEEP AGENT FOR LONG HORIZON TASK\n",
        "class TaskManager():\n",
        "  \"\"\"\n",
        "  Manages task decomposition , tracking and progress monitoring\n",
        "  \"\"\"\n",
        "  def create_task(self, title , description , priority , depedencies):\n",
        "      \"\"\" Register a new task in execution plan\"\"\"\n",
        "  def update_task_status(self, title , description , priority , depedencies):\n",
        "      \"\"\"Update task completion status and add execution notes\"\"\"\n",
        "  def get_pending_task(self, title , description , priority , depedencies):\n",
        "    \"\"\"Retreive all incomplete tasks ordered by priority\"\"\"\n",
        "  def get_task_dependencies(self, title , description , priority , depedencies):\n",
        "    \"\"\"check if task dependencies are satisfied \"\"\"\n",
        "  def generate_progress_report(self, title , description , priority , depedencies):\n",
        "    \"\"\"create comprehensive progress summary\"\"\"\n"
      ],
      "metadata": {
        "id": "afQD2HISLNXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FileSystemManager():\n",
        "  def write_data(self,path,content,metadata = None):\n",
        "  def read_data(self,path,content,metadata = None):\n",
        "  def search_files(self,path,content,metadata = None):\n",
        "  def archive_context(self,path,content,metadata = None):\n",
        "  def create_checkpoint(self,path,content,metadata = None):\n",
        "\n"
      ],
      "metadata": {
        "id": "49TCSXzYM18J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sub agent communication protocol\n",
        "class SubAgentCoordinator:\n",
        "  \"\"\" Manages sub-agent lifecycle and communication\"\"\"\n",
        "\n",
        "  def spawn_agent(self,agent_type,task_specification):\n",
        "    \"\"\"Instantiate specialist agent for specific task\"\"\"\n",
        "  def send_task(self,agent_type,task_specification):\n",
        "    \"\"\"Instantiate specialist agent for specific task\"\"\"\n",
        "  def receive_result(self,agent_type,task_specification):\n",
        "    \"\"\"Collect completed work from sub-agent\"\"\"\n",
        "  def terminate_agent(self,agent_type,task_specification):\n",
        "    \"\"\"Cleanup sub-agent resources\"\"\"\n"
      ],
      "metadata": {
        "id": "8sl7vczkNTYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langgraph  openai anthropic\n",
        "!pip install python-dotenv pydantic\n",
        "!pip install chromadb\n",
        "\n",
        "!pip install pandas numpy matplotlib\n",
        "!pip install requests beautifulsoup4"
      ],
      "metadata": {
        "id": "XQxfTjqbOj25",
        "outputId": "5e309e83-7632-48e8-a37e-bc67794df0fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Collecting anthropic\n",
            "  Downloading anthropic-0.72.1-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-1.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.17.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "INFO: pip is looking at multiple versions of langgraph-prebuilt to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Downloading langgraph_checkpoint-2.1.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
            "INFO: pip is still looking at multiple versions of langgraph-prebuilt to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_text_splitters-0.3.10-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting pip>=25.2 (from langchain-text-splitters<1.0.0,>=0.3.9->langchain)\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-1.0.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "  Downloading langgraph-1.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-1.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Downloading langgraph-1.0.1-py3-none-any.whl (155 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.1-py3-none-any.whl (28 kB)\n",
            "Downloading anthropic-0.72.1-py3-none-any.whl (357 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m357.4/357.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, anthropic, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed anthropic-0.72.1 langgraph-1.0.1 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.1 langgraph-sdk-0.2.9 ormsgpack-1.12.0\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.3.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.10)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.28.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.3.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=4c1a81ff157abfcaeb829154a27aa12bc28317c70bcc0231a703ff4f8c195cd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, urllib3, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, posthog, opentelemetry-semantic-conventions, onnxruntime, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 chromadb-1.3.4 coloredlogs-15.0.1 durationpy-0.10 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 mmh3-5.2.0 onnxruntime-1.23.2 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "opentelemetry",
                  "urllib3"
                ]
              },
              "id": "2eec37c9b509475ab25bc2ad8debd7f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1881629830.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install chromadb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pandas numpy matplotlib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install requests beautifulsoup4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    957\u001b[0m     opt_names = {\n\u001b[1;32m    958\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodulename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     }\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \"\"\"\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \"\"\"\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#config file\n",
        "#AGENT CONFIG\n",
        "ORCHESTRATOR_MODEL = \"gpt-4-turbo-preview\"\n",
        "SUBAGENT_MODEL = \"gpt-3.5-turbo\"\n",
        "MAX_EXECUTION_TIME = 3600\n",
        "CHECKPOINT_INTERVAL = 3000\n",
        "\n",
        "#FILE SYSTEM TO BE USED AGENTS\n",
        "WORKSPACE_PATH = ./workspace\n",
        "MAX_FILE_SIZE = 10485760 #10 MB\n",
        "ARCHIVE_THRESHOLD = 100485760 # 100 MB\n",
        "\n",
        "#logging\n",
        "LOG_LEVEL = INFO\n",
        "LOG_FILE = ./logs/agent_execution.log"
      ],
      "metadata": {
        "id": "0G5YwNTyPPuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SlV8ZVJNQqnG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}