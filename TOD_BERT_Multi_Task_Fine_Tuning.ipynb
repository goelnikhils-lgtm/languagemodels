{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goelnikhils-lgtm/languagemodels/blob/main/TOD_BERT_Multi_Task_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# --- 1. CONFIGURATION AND UTILITIES ---\n",
        "\n",
        "# Replace with your actual pre-trained model path or a compatible BERT model\n",
        "TOD_BERT_MODEL = 'bert-base-uncased'\n",
        "NUM_SLOTS = 50  # Number of possible (slot, value) pairs for DST\n",
        "NUM_ACTIONS = 10 # Number of possible dialogue actions for Policy\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def make_dummy_data(tokenizer):\n",
        "    \"\"\"Creates a dummy dataset simulating turn-level dialogue data.\"\"\"\n",
        "    # Context: Full dialogue history up to the current turn\n",
        "    # Target 1 (DST): Binary vector for which slots are active\n",
        "    # Target 2 (Policy): Integer index for the next action\n",
        "    # Target 3 (Generation): Tokenized ground truth response\n",
        "\n",
        "    dialogues = [\n",
        "        (\"Hello, I need a flight to London.\", [1, 0, 0], 2, \"Which city are you flying from?\"),\n",
        "        (\"I'm leaving from New York.\", [1, 1, 0], 3, \"And what date are you looking for?\"),\n",
        "        (\"I'll take any date.\", [1, 1, 1], 4, \"I'm searching for flights now.\"),\n",
        "    ]\n",
        "\n",
        "    data = []\n",
        "    for context, dst_label, policy_label, response in dialogues:\n",
        "        # Encoder Input (Context)\n",
        "        enc_input = tokenizer(context, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
        "\n",
        "        # Decoder Input (Response Tokens)\n",
        "        dec_input = tokenizer(response, truncation=True, padding='max_length', max_length=64, return_tensors='pt')\n",
        "\n",
        "        data.append({\n",
        "            'input_ids': enc_input['input_ids'].squeeze(),\n",
        "            'attention_mask': enc_input['attention_mask'].squeeze(),\n",
        "            'dst_labels': torch.tensor(dst_label).float(), # Multi-label binary\n",
        "            'policy_labels': torch.tensor(policy_label).long(), # Multi-class index\n",
        "            'gen_labels': dec_input['input_ids'].squeeze(), # Sequence of tokens\n",
        "        })\n",
        "    return data\n",
        "\n",
        "class DialogueDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# --- 2. THE MULTI-HEAD MODEL ---\n",
        "\n",
        "class TODBERTSystem(nn.Module):\n",
        "    def __init__(self, num_slots, num_actions, vocab_size):\n",
        "        super().__init__()\n",
        "        # Pre-trained BERT Encoder (The TOD-BERT backbone)\n",
        "        # Note: In a real TOD-BERT setup, you'd load the checkpoint after MLM/RCL.\n",
        "        self.encoder = AutoModel.from_pretrained(TOD_BERT_MODEL)\n",
        "        hidden_size = self.encoder.config.hidden_size\n",
        "\n",
        "        # --- 1. Dialogue State Tracking (DST) Head ---\n",
        "        # Predicts the probability of each slot being active (Multi-label classification)\n",
        "        self.dst_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 2, num_slots),\n",
        "        )\n",
        "\n",
        "        # --- 2. Dialogue Policy Head ---\n",
        "        # Predicts the next discrete action (Multi-class classification)\n",
        "        self.policy_head = nn.Linear(hidden_size, num_actions)\n",
        "\n",
        "        # --- 3. Response Generation (Gen) Head ---\n",
        "        # Simple token-level prediction for sequence generation (Language Modeling)\n",
        "        # This acts as a decoder head, mapping the encoder output to the vocabulary.\n",
        "        self.gen_head = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, gen_labels):\n",
        "        # 1. Forward pass through the shared BERT encoder\n",
        "        # We use the encoder's output for all tasks\n",
        "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "\n",
        "        # Use the [CLS] token output for DST and Policy\n",
        "        cls_output = encoder_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Use the full sequence output for Generation\n",
        "        sequence_output = encoder_outputs.last_hidden_state\n",
        "\n",
        "        # 2. DST Prediction (Multi-label logits)\n",
        "        dst_logits = self.dst_head(cls_output)\n",
        "\n",
        "        # 3. Policy Prediction (Multi-class logits)\n",
        "        policy_logits = self.policy_head(cls_output)\n",
        "\n",
        "        # 4. Generation Prediction (Token logits)\n",
        "        # This simplified head takes the encoded sequence output and predicts the next token\n",
        "        gen_logits = self.gen_head(sequence_output)\n",
        "\n",
        "        return dst_logits, policy_logits, gen_logits\n",
        "\n",
        "# --- 3. TRAINING LOOP EXECUTION ---\n",
        "\n",
        "def train_tod_bert_system():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(TOD_BERT_MODEL)\n",
        "    dummy_data = make_dummy_data(tokenizer)\n",
        "    dataset = DialogueDataset(dummy_data)\n",
        "    dataloader = DataLoader(dataset, batch_size=4)\n",
        "\n",
        "    # Initialize Model and Optimizer\n",
        "    model = TODBERTSystem(NUM_SLOTS, NUM_ACTIONS, tokenizer.vocab_size).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "    # Define Loss Functions\n",
        "    # BCE for DST (Multi-label)\n",
        "    dst_loss_fn = nn.BCEWithLogitsLoss()\n",
        "    # CrossEntropy for Policy (Multi-class)\n",
        "    policy_loss_fn = nn.CrossEntropyLoss()\n",
        "    # CrossEntropy for Generation (Token prediction)\n",
        "    gen_loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    print(f\"Starting fine-tuning on {DEVICE}...\")\n",
        "\n",
        "    # Training Loop (1 Epoch for demonstration)\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move inputs and targets to device\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        dst_labels = batch['dst_labels'].to(DEVICE)\n",
        "        policy_labels = batch['policy_labels'].to(DEVICE)\n",
        "        gen_labels = batch['gen_labels'].to(DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        dst_logits, policy_logits, gen_logits = model(input_ids, attention_mask, gen_labels)\n",
        "\n",
        "        # --- LOSS CALCULATION ---\n",
        "\n",
        "        # 1. DST Loss (Multi-Label Classification)\n",
        "        dst_loss = dst_loss_fn(dst_logits, dst_labels)\n",
        "\n",
        "        # 2. Policy Loss (Multi-Class Classification)\n",
        "        policy_loss = policy_loss_fn(policy_logits, policy_labels)\n",
        "\n",
        "        # 3. Generation Loss (Sequence Prediction)\n",
        "        # Flatten logits and labels for token-level CrossEntropy Loss\n",
        "        # We predict the next token based on the sequence history\n",
        "        gen_logits = gen_logits[:, :-1, :].contiguous()\n",
        "        gen_labels = gen_labels[:, 1:].contiguous()\n",
        "\n",
        "        gen_loss = gen_loss_fn(\n",
        "            gen_logits.view(-1, gen_logits.size(-1)),\n",
        "            gen_labels.view(-1)\n",
        "        )\n",
        "\n",
        "        # --- Total Loss (Multi-Task Learning) ---\n",
        "        # The total loss is the weighted sum of individual losses.\n",
        "        # Weights (1.0) are often used initially, but can be tuned.\n",
        "        total_loss = (1.0 * dst_loss) + (1.0 * policy_loss) + (1.0 * gen_loss)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Batch Loss: {total_loss.item():.4f} | DST: {dst_loss.item():.4f} | Policy: {policy_loss.item():.4f} | Gen: {gen_loss.item():.4f}\")\n",
        "\n",
        "    print(\"Fine-tuning simulation complete.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set logging level to avoid excessive logs from transformers library\n",
        "    from transformers import logging as hf_logging\n",
        "    hf_logging.set_verbosity_error()\n",
        "    train_tod_bert_system()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ZCmn4O3qCF4I"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}