{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvGacs2DJhoa"
      },
      "outputs": [],
      "source": [
        "#step1\n",
        "# pip install -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "pkgs = [\n",
        "    \"huggingface_hub\",\n",
        "    \"tokenizers\",\n",
        "    \"torch\",\n",
        "        ]\n",
        "for p in pkgs:\n",
        "  print(f\"{p} version:{version(p)}\")"
      ],
      "metadata": {
        "id": "Dok_ghs0KPlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e228336-2a45-48fe-8bdd-6396d3413b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface_hub version:0.34.4\n",
            "tokenizers version:0.22.0\n",
            "torch version:2.8.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_REASONING_MODEL = True"
      ],
      "metadata": {
        "id": "tXluINUZK4Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype = cfg[\"dtype\"], bias = False)\n",
        "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype = cfg[\"dtype\"], bias = False)\n",
        "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype = cfg[\"dtype\"], bias = False)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x_fc1 = self.fc1(x)\n",
        "      x_fc2 = self.fc2(x)\n",
        "      x = nn.functional.silu(x_fc1) * x_fc2\n",
        "      return self.fc3(x)"
      ],
      "metadata": {
        "id": "H2cIl5SKK-zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, emb_dim,eps = 1e-6,bias = False,qwen3_compatible = True):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.qwen3_compatible = qwen3_compatible\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.ones(emb_dim)) if bias else None\n",
        "\n",
        "  def forward(self,x):\n",
        "    input_dtype = x.dtype\n",
        "    if self.qwen3_compatible:\n",
        "      x = x.to(torch.float32)\n",
        "    variance = x.pow(2).mean(-1,keepdim = True)\n",
        "    norm_x = x * torch.rsqrt(variance + self.eps)\n",
        "    norm_x = x * self.scale\n",
        "\n",
        "    if self.shift is not None:\n",
        "      norm_x = norm_x + self.shift\n",
        "    return norm_x.to(input_dtype)"
      ],
      "metadata": {
        "id": "3suh7ZQ0MyKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rope - rotatory position embeddings - rope captures relatve position information information\n",
        "#use position interpolation increase the inference context length .... use a scaling factor\n",
        "#NTK - adaptive frequency scaling method - use 1 as scaling factor for high frequency tokens , use 1/n as the factor for low frequency tokens\n",
        "#YARN - adaptive frequency scaling method -\n",
        "#adaptive frequency method increases the context length of inference window of transformer , even when transformer is trained at lower context length\n",
        "#NTK helps with 64K context lenght and also helps in find needle in haystack evaluation for LLM\n",
        "\n",
        "def compute_rope_params(head_dim,theta_base = 10_000,context_length = 4096, dtype= torch.float32):\n",
        "  assert head_dim % 2 == 0, \"Embedding dimnesion must be even\"\n",
        "\n",
        "  #compute the inverse frequencies #rotation frequency\n",
        "  inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype = dtype)[:(head_dim//2)].float()/head_dim))\n",
        "\n",
        "  #generate position indices\n",
        "  positions = torch.arange(context_length, dtype = dtype)\n",
        "\n",
        "  #Compute the angles\n",
        "  angles = positions[:,None]*inv_freq[None,:] #shape (context_length , head_dim//2)\n",
        "  #expand angles to match the head_dim\n",
        "  angles = torch.cat([angles, angles], dim = 1) #shape (context_length, head_dim)\n",
        "  #precompute sine and cosine\n",
        "  cos = torch.cos(angles)\n",
        "  sin = torch.sin(angles)\n",
        "  return cos,sin\n",
        "\n",
        "def apply_rope(x,cos,sin):\n",
        "    #x: (batch_size,num_heads,seq_len,head_dim)\n",
        "    batch_size , num_heads, seq_len, head_dim = x.shape\n",
        "    assert head_dim %2 ==0 , \"head_dim must be even\"\n",
        "\n",
        "    #split x into first half and second half\n",
        "    x1 = x[...,:head_dim//2] #first half\n",
        "    x2 = x[...,head_dim//2:] #second half\n",
        "\n",
        "    #adjust sin and cos shapes\n",
        "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0) #shape (1,1,seq_len,head_dim)\n",
        "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    #apply the rotatory transformation\n",
        "    rotated = torch.cat((-x2,x1),dim=-1)\n",
        "    x_rotated = (x*cos) + (rotated*sin)\n",
        "\n",
        "    #Its ok to use Lower precision after applying cos and sin rotation\n",
        "    return x_rotated.to(dtype=x.dtype)"
      ],
      "metadata": {
        "id": "o28S8xXyN0vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GQA\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "  def __init__(\n",
        "      self,d_in,num_heads,num_kv_groups,head_dim=None, qk_norm = False , dtype = None\n",
        "      ): #multiple query heads share same key and value projections\n",
        "      #the benefit of GQA is faster inference and better understanding on the text ...\n",
        "      #no of heads = no of group of queries....\n",
        "    super().__init__()\n",
        "    assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
        "    self.num_heads = num_heads\n",
        "    self.num_kv_groups = num_kv_groups\n",
        "    self.group_size = num_heads // num_kv_groups #\n",
        "\n",
        "    if head_dim is None:\n",
        "      assert d_in % num_heads == 0, \"d_in must be divisible by num_heads if head_dim is not specified\"\n",
        "      head_dim = d_in // num_heads\n",
        "\n",
        "    self.head_dim = head_dim\n",
        "    self.d_out = num_heads*head_dim\n",
        "\n",
        "    self.W_query = nn.Linear(d_in, self.d_out, bias = False, dtype = dtype)\n",
        "    self.W_key = nn.Linear(d_in, num_kv_groups*head_dim, bias = False, dtype = dtype)\n",
        "    self.W_value = nn.Linear(d_in, num_kv_groups*head_dim, bias = False, dtype = dtype)\n",
        "\n",
        "    self.out_proj = nn.Linear(self.d_out, d_in, bias = False, dtype = dtype) #o_proj -> output projection in case of MHA\n",
        "    if qk_norm:\n",
        "      self.q_norm = RMSNorm(head_dim, eps = 1e-6)\n",
        "      self.k_norm = RMSNorm(head_dim, eps = 1e-6)\n",
        "    else:\n",
        "      self.q_norm = self.k_norm = None\n",
        "\n",
        "  def forward(self,x,mask,cos,sin):\n",
        "    b, num_tokens, _ = x.shape\n",
        "\n",
        "    #apply projections #expand # linear transformations\n",
        "    queries = self.W_query(x) #(b,num_tokens,num_heads*head_dim) #projections - this projections helps to capture different aspects of text\n",
        "    keys = self.W_key(x)  #projections\n",
        "    values = self.W_value(x) #projections\n",
        "\n",
        "    #queries are getting projected to more dimensions and this helps....\n",
        "\n",
        "    #Reshape\n",
        "    queries = queries.view(b,num_tokens,self.num_heads,self.head_dim).transpose(1,2)\n",
        "    keys = keys.view(b,num_tokens,self.num_kv_groups,self.head_dim).transpose(1,2)\n",
        "    values = values.view(b,num_tokens,self.num_kv_groups,self.head_dim).transpose(1,2)\n",
        "\n",
        "    #Optional Normalization\n",
        "    if self.q_norm:\n",
        "      queries = self.q_norm(queries)\n",
        "    if self.k_norm:\n",
        "      keys = self.k_norm(keys)\n",
        "\n",
        "    #Apply RoPE\n",
        "    queries = apply_rope(queries,cos,sin)\n",
        "    keys = apply_rope(keys,cos,sin)\n",
        "\n",
        "    #Expand K and V to match query\n",
        "    keys = keys.repeat_interleave(self.group_size,dim = 1)\n",
        "    values = values.repeat_interleave(self.group_size,dim = 1)\n",
        "\n",
        "    #Attention\n",
        "    attn_scores = queries @ keys.transpose(2,3)\n",
        "    attn_scores = attn_scores.masked_fill(mask,-torch.inf)\n",
        "    attn_weights = torch.softmax(attn_scores/self.head_dim**0.5,dim = -1)\n",
        "    context = (attn_weights @ values).transpose(1,2).reshape(b,num_tokens,self.d_out)\n",
        "    return self.out_proj(context) # compression layer to compress output from various attention head layers...."
      ],
      "metadata": {
        "id": "UGN36y2PrFWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class Transformer\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.att=GroupedQueryAttention(\n",
        "        d_in = cfg[\"emb_dim\"],\n",
        "        num_heads = cfg[\"n_heads\"],\n",
        "        head_dim = cfg[\"head_dim\"],\n",
        "        num_kv_groups = cfg[\"n_kv_groups\"],\n",
        "        qk_norm = cfg[\"qk_norm\"],\n",
        "        dtype = cfg[\"dtype\"]\n",
        "    )\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = RMSNorm(cfg[\"emb_dim\"],eps = 1e-6)\n",
        "    self.norm2 = RMSNorm(cfg[\"emb_dim\"],eps = 1e-6)\n",
        "\n",
        "  def forward(self,x,mask,cos,sin):\n",
        "      #Shortcut connection for attention block\n",
        "      shortcut = x\n",
        "      x = self.norm1(x)\n",
        "      x = self.att(x,mask,cos,sin)\n",
        "      x = x + shortcut\n",
        "\n",
        "      #Shortcut connection for feed-forward block\n",
        "      shortcut = x\n",
        "      x = self.norm2(x)\n",
        "      x = self.ff(x)\n",
        "      x = x + shortcut\n",
        "      return x"
      ],
      "metadata": {
        "id": "_5ioAsS3w35w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Qwen3.0 Model\n",
        "class Qwen3Model(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    #Main model parameters\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"] , dtype = cfg[\"dtype\"])\n",
        "    self.trf_blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "    self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias = False , dtype = cfg[\"dtype\"])\n",
        "\n",
        "    #Resuable utilities\n",
        "    if cfg[\"head_dim\"] is None:\n",
        "      head_dim = cfg[\"emb_dim\"]//cfg[\"n_heads\"]\n",
        "    else:\n",
        "      head_dim = cfg[\"head_dim\"]\n",
        "    cos, sin = compute_rope_params(head_dim = head_dim, theta_base = cfg[\"rope_base\"] ,context_length = cfg[\"context_length\"])\n",
        "    self.register_buffer(\"cos\",cos,persistent = False)\n",
        "    self.register_buffer(\"sin\",sin, persistent = False)\n",
        "    self.cfg = cfg\n",
        "\n",
        "  def forward(self,in_idx):\n",
        "    #Forward pass\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    x = tok_embeds\n",
        "    num_tokens = x.shape[1]\n",
        "    mask = torch.triu(torch.ones(num_tokens,num_tokens,device=x.device , dtype = torch.bool),diagonal = 1)\n",
        "\n",
        "    for block in self.trf_blocks:\n",
        "      x = block(x,mask,self.cos,self.sin)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
        "    return logits"
      ],
      "metadata": {
        "id": "xMOFjljzyIaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Intialize Model\n",
        "CHOOSE_MODEL = \"0.6B\"\n",
        "\n",
        "if CHOOSE_MODEL == \"0.6B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,           # Vocabulary size\n",
        "        \"context_length\": 40_960,        # Context length that was used to train the model\n",
        "        \"emb_dim\": 1024,                 # Embedding dimension\n",
        "        \"n_heads\": 16,                   # Number of attention heads\n",
        "        \"n_layers\": 28,                  # Number of layers\n",
        "        \"hidden_dim\": 3072,              # Size of the intermediate dimension in FeedForward\n",
        "        \"head_dim\": 128,                 # Size of the heads in GQA\n",
        "        \"qk_norm\": True,                 # Whether to normalize queries and keys in GQA\n",
        "        \"n_kv_groups\": 8,                # Key-Value groups for grouped-query attention\n",
        "        \"rope_base\": 1_000_000.0,        # The base in RoPE's \"theta\"\n",
        "        \"dtype\": torch.bfloat16,         # Lower-precision dtype to reduce memory usage\n",
        "    }\n",
        "\n",
        "elif CHOOSE_MODEL == \"1.7B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,\n",
        "        \"context_length\": 40_960,\n",
        "        \"emb_dim\": 2048,                 # 2x larger than above\n",
        "        \"n_heads\": 16,\n",
        "        \"n_layers\": 28,\n",
        "        \"hidden_dim\": 6144,              # 2x larger than above\n",
        "        \"head_dim\": 128,\n",
        "        \"qk_norm\": True,\n",
        "        \"n_kv_groups\": 8,\n",
        "        \"rope_base\": 1_000_000.0,\n",
        "        \"dtype\": torch.bfloat16,\n",
        "    }\n",
        "\n",
        "elif CHOOSE_MODEL == \"4B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,\n",
        "        \"context_length\": 40_960,\n",
        "        \"emb_dim\": 2560,                 # 25% larger than above\n",
        "        \"n_heads\": 32,                   # 2x larger than above\n",
        "        \"n_layers\": 36,                  # 29% larger than above\n",
        "        \"hidden_dim\": 9728,              # ~3x larger than above\n",
        "        \"head_dim\": 128,\n",
        "        \"qk_norm\": True,\n",
        "        \"n_kv_groups\": 8,\n",
        "        \"rope_base\": 1_000_000.0,\n",
        "        \"dtype\": torch.bfloat16,\n",
        "    }\n",
        "\n",
        "elif CHOOSE_MODEL == \"8B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,\n",
        "        \"context_length\": 40_960,\n",
        "        \"emb_dim\": 4096,                 # 60% larger than above\n",
        "        \"n_heads\": 32,\n",
        "        \"n_layers\": 36,                  # 26% larger than above\n",
        "        \"hidden_dim\": 12288,\n",
        "        \"head_dim\": 128,\n",
        "        \"qk_norm\": True,\n",
        "        \"n_kv_groups\": 8,\n",
        "        \"rope_base\": 1_000_000.0,\n",
        "        \"dtype\": torch.bfloat16,\n",
        "    }\n",
        "\n",
        "elif CHOOSE_MODEL == \"14B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,\n",
        "        \"context_length\": 40_960,\n",
        "        \"emb_dim\": 5120,                 # 25% larger than above\n",
        "        \"n_heads\": 40,                   # 25% larger than above\n",
        "        \"n_layers\": 40,                  # 11% larger than above\n",
        "        \"hidden_dim\": 17408,             # 42% larger than above\n",
        "        \"head_dim\": 128,\n",
        "        \"qk_norm\": True,\n",
        "        \"n_kv_groups\": 8,\n",
        "        \"rope_base\": 1_000_000.0,\n",
        "        \"dtype\": torch.bfloat16,\n",
        "    }\n",
        "\n",
        "elif CHOOSE_MODEL == \"32B\":\n",
        "    QWEN3_CONFIG = {\n",
        "        \"vocab_size\": 151_936,\n",
        "        \"context_length\": 40_960,\n",
        "        \"emb_dim\": 5120,\n",
        "        \"n_heads\": 64,                   # 60% larger than above\n",
        "        \"n_layers\": 64,                  # 60% larger than above\n",
        "        \"hidden_dim\": 25600,             # 47% larger than above\n",
        "        \"head_dim\": 128,\n",
        "        \"qk_norm\": True,\n",
        "        \"n_kv_groups\": 8,\n",
        "        \"rope_base\": 1_000_000.0,\n",
        "        \"dtype\": torch.bfloat16,\n",
        "    }\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"{CHOOSE_MODEL} is not supported.\")"
      ],
      "metadata": {
        "id": "vqkUiO23DGA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Intializing Model\n",
        "torch.manual_seed(123)\n",
        "model = Qwen3Model(QWEN3_CONFIG)"
      ],
      "metadata": {
        "id": "BydwBxCHDgjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "4mU_2bITDqsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51957e0-24c6-426c-d8aa-5fd6f1937222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen3Model(\n",
              "  (tok_emb): Embedding(151936, 1024)\n",
              "  (trf_blocks): ModuleList(\n",
              "    (0-27): 28 x TransformerBlock(\n",
              "      (att): GroupedQueryAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=2048, bias=False)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
              "        (q_norm): RMSNorm()\n",
              "        (k_norm): RMSNorm()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (fc1): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "        (fc2): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "        (fc3): Linear(in_features=3072, out_features=1024, bias=False)\n",
              "      )\n",
              "      (norm1): RMSNorm()\n",
              "      (norm2): RMSNorm()\n",
              "    )\n",
              "  )\n",
              "  (final_norm): RMSNorm()\n",
              "  (out_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.tensor([1,2,3]).unsqueeze(0))"
      ],
      "metadata": {
        "id": "HS9Dmo6Y_Rhi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce121ce-36b6-4eda-82f4-137837fc2ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
              "         [nan, nan, nan,  ..., nan, nan, nan],\n",
              "         [nan, nan, nan,  ..., nan, nan, nan]]], dtype=torch.bfloat16,\n",
              "       grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters:{total_params:,}\")\n",
        "#account for weight tying\n",
        "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
        "print(f\"\\nTotal number of unique parameters:{total_params_normalized:,}\")"
      ],
      "metadata": {
        "id": "SkutLE1H_lq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27508c5d-a48b-4a80-d061-b8e9ff8b6dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters:751,632,384\n",
            "\n",
            "Total number of unique parameters:596,049,920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "def model_memory_size(model,input_dtype=torch.float32):\n",
        "    total_params = 0\n",
        "    total_grads = 0\n",
        "    for param in model.parameters():\n",
        "        #calculate total number of elements per parameter\n",
        "        param_size = param.numel()\n",
        "        total_params += param_size\n",
        "        #check if gradients are stored for this paramter\n",
        "        if param.requires_grad:\n",
        "            total_grads += param_size\n",
        "    # calculate buffer size (non-parameters that require memory)\n",
        "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
        "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
        "    # We assume paramaters and gradients are stored in the same type as input dtype\n",
        "    element_size = torch.tensor(0,dtype=input_dtype).element_size()\n",
        "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
        "    #Convert bytes to gigabytes\n",
        "    total_memory_gb = total_memory_bytes/(1024**3)\n",
        "    return total_memory_gb\n",
        "\n",
        "print(f\"float32(PyTorch Default):{model_memory_size(model,input_dtype=torch.float32):.2f} GB\")\n",
        "print(f\"bfloat16:{model_memory_size(model,input_dtype=torch.bfloat16):.2f} GB\")"
      ],
      "metadata": {
        "id": "Ec5dwXZhGnDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a7a8ea-c6d3-4ab9-8f5d-1fc043fb74d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float32(PyTorch Default):5.64 GB\n",
            "bfloat16:2.82 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "Lpv57UfOJJUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Pretrained weights\n",
        "def load_weights_into_qwen(model, param_config, params):\n",
        "  def assign(left,right,tensor_name = \"unknown\"):\n",
        "    if left.shape != right.shape:\n",
        "      raise ValueError(f\"Shape mismatch in tensor '{tensor_name}'.Left :{left.shape},Right :{right.shape}\")\n",
        "    return torch.nn.Parameter(right.clone().detach() if isinstance(right,torch.Tensor) else torch.tensor(right))\n",
        "  model.tok_emb.weight = assign(model.tok_emb.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
        "\n",
        "  for l in range(param_config[\"n_layers\"]):\n",
        "    block = model.trf_blocks[1]\n",
        "    att = block.att\n",
        "\n",
        "  #Q,K,V projections\n",
        "  att.W_query.weight = assign(att.W_query.weight, params[f\"model.layers.{l}.self_attn.q_proj.weight\"], f\"model.layers.{l}.self_attn.q_proj.weight\")\n",
        "  att.W_key.weight = assign(att.W_key.weight, params[f\"model.layers.{l}.self_attn.k_proj.weight\"], f\"model.layers.{l}.self_attn.k_proj.weight\")\n",
        "  att.W_value.weight = assign(att.W_value.weight, params[f\"model.layers.{l}.self_attn.v_proj.weight\"], f\"model.layers.{l}.self_attn.v_proj.weight\")\n",
        "\n",
        "  #Output projection\n",
        "  att.out_proj.weight = assign(att.out_proj.weight, params[f\"model.layers.{l}.self_attn.o_proj.weight\"], f\"model.layers.{l}.self_attn.o_proj.weight\")\n",
        "\n",
        "  #QK norms\n",
        "  if hasattr(att,\"q_norm\") and att.q_norm is not None:\n",
        "    att.q_norm.scale = assign(att.q_norm.scale, params[f\"model.layers.{l}.self_attn.q_norm.weight\"], f\"model.layers.{l}.self_attn.q_norm.weight\")\n",
        "  if hasattr(att,\"k_norm\") and att.k_norm is not None:\n",
        "    att.k_norm.scale = assign(att.k_norm.scale, params[f\"model.layers.{l}.self_attn.k_norm.weight\"], f\"model.layers.{l}.self_attn.k_norm.weight\")\n",
        "\n",
        "  #Attention layernorm\n",
        "  block.norm1.scale = assign(block.norm1.scale, params[f\"model.layers.{l}.input_layernorm.weight\"], f\"model.layers.{l}.mlp.gate_proj.weight\")\n",
        "  #feed forward weights\n",
        "  block.ff.fc1.weight = assign(block.ff.fc1.weight, params[f\"model.layers.{l}.mlp.gate_proj.weight\"], f\"model.layers.{l}.mlp.gate_proj.weight\")\n",
        "  block.ff.fc2.weight = assign(block.ff.fc2.weight, params[f\"model.layers.{l}.mlp.down_proj.weight\"], f\"model.layers.{l}.mlp.down_proj.weight\")\n",
        "  block.ff.fc3.weight = assign(block.ff.fc3.weight, params[f\"model.layers.{l}.mlp.up_proj.weight\"], f\"model.layers.{l}.mlp.up_proj.weight\")\n",
        "\n",
        "  block.norm2.scale = assign(block.norm2.scale, params[f\"model.layers.{l}.post_attention_layernorm.weight\"], f\"model.layers.{l}.post_attention_layernorm.weight\")\n",
        "  #Final normalization and output head\n",
        "  model.final_norm.scale = assign(model.final_norm.scale, params[\"model.norm.weight\"], \"model.norm.weight\")\n",
        "  if \"lm_head.weight\" in params:\n",
        "    model.out_head.weight = assign(model.out_head.weight, params[\"lm_head.weight\"], \"lm_head.weight\")\n",
        "  else:\n",
        "    print(\"Model uses weight tying\")\n",
        "    model.out_head.weight = assign(model.out_head.weight, params[\"model.tok_emb.weight\"], \"model.embed_tokens.weight\")"
      ],
      "metadata": {
        "id": "jvLYarkSzURt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from safetensors.torch import load_file\n",
        "from huggingface_hub import hf_hub_download , snapshot_download\n",
        "\n",
        "if USE_REASONING_MODEL:\n",
        "  repo_id = f\"Qwen/Qwen3-{CHOOSE_MODEL}\"\n",
        "else:\n",
        "  repo_id = f\"Qwen/Qwen3-{CHOOSE_MODEL}-Base\"\n",
        "local_dir = Path(repo_id).parts[-1]\n",
        "\n",
        "if CHOOSE_MODEL == \"0.6B\":\n",
        "  weights_file = hf_hub_download(repo_id = repo_id,filename = \"model.safetensors\",local_dir = local_dir)\n",
        "  weight_dic = load_file(weights_file)\n",
        "else:\n",
        "  repo_dir= snapshot_download(repo_id= repo_id,local_dir = local_dir)\n",
        "  index_path = os.path.join(repo_dir,\"model.safetensors.index.json\")\n",
        "  with open(index_path,\"r\") as f:\n",
        "    index = json.load(f)\n",
        "  weight_dic = {}\n",
        "  for filename in set(index[\"weight_map\"].values()):\n",
        "    shard_path = os.path.join(repo_dir,filename)\n",
        "    shard = load_file(shard_path)\n",
        "    weight_dic.update(shard)\n",
        "#print(\"weight_dic\",weight_dic)\n",
        "load_weights_into_qwen(model,QWEN3_CONFIG,weight_dic)\n",
        "model.to(device)\n",
        "del weight_dic"
      ],
      "metadata": {
        "id": "OeW_Pe1_-b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "e29d4a3e-885a-47b0-f3a1-9a46ebb9764e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Shape mismatch in tensor 'model.layers.27.mlp.down_proj.weight'.Left :torch.Size([3072, 1024]),Right :torch.Size([1024, 3072])",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-272792570.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mweight_dic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#print(\"weight_dic\",weight_dic)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mload_weights_into_qwen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQWEN3_CONFIG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mweight_dic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-315811854.py\u001b[0m in \u001b[0;36mload_weights_into_qwen\u001b[0;34m(model, param_config, params)\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;31m#feed forward weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"model.layers.{l}.mlp.gate_proj.weight\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"model.layers.{l}.mlp.gate_proj.weight\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"model.layers.{l}.mlp.down_proj.weight\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"model.layers.{l}.mlp.down_proj.weight\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"model.layers.{l}.mlp.up_proj.weight\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"model.layers.{l}.mlp.up_proj.weight\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-315811854.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(left, right, tensor_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensor_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"unknown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape mismatch in tensor '{tensor_name}'.Left :{left.shape},Right :{right.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model.embed_tokens.weight\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.embed_tokens.weight\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Shape mismatch in tensor 'model.layers.27.mlp.down_proj.weight'.Left :torch.Size([3072, 1024]),Right :torch.Size([1024, 3072])"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load tokenizer\n",
        "import re\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "class Qwen3Tokenizer:\n",
        "   _SPECIALS = [\n",
        "        \"<|endoftext|>\",\n",
        "        \"<|im_start|>\", \"<|im_end|>\",\n",
        "        \"<|object_ref_start|>\", \"<|object_ref_end|>\",\n",
        "        \"<|box_start|>\", \"<|box_end|>\",\n",
        "        \"<|quad_start|>\", \"<|quad_end|>\",\n",
        "        \"<|vision_start|>\", \"<|vision_end|>\",\n",
        "        \"<|vision_pad|>\", \"<|image_pad|>\", \"<|video_pad|>\",\n",
        "    ]\n",
        "   _SPLIT_RE = re.compile(r\"(<\\|[^>]+?|>)\")\n",
        "   def __init__(self , tokenizer_file_path = \"tokenizer.json\",repo_id = None , apply_chat_template = True , add_generation_prompt = False , add_thinking = False):\n",
        "      self.apply_chat_template = apply_chat_template\n",
        "      self.add_generation_prompt = add_generation_prompt\n",
        "      self.add_thinking = add_thinking\n",
        "\n",
        "      tok_file = Path(tokenizer_file_path)\n",
        "      self._tok = Tokenizer.from_file(str(tok_file))\n",
        "      self._special_to_id = {t: self._tok.token_to_id(t) for t in self._SPECIALS}\n",
        "\n",
        "      self.pad_token_id = self._special_to_id[\"<|endoftext|>\"]\n",
        "      self.eos_token_id = self.pad_token_id\n",
        "\n",
        "      if repo_id and \"Base\" not in repo_id:\n",
        "        eos_token = \"<|im_end|>\"\n",
        "      else:\n",
        "        eos_token = \"<|endoftext|>\"\n",
        "      if eos_token in self._special_to_id:\n",
        "         self.eos_token_id = self._special_to_id[eos_token]\n",
        "\n",
        "   def encode(self,text,chat_wrapped):\n",
        "    if chat_wrapped is None:\n",
        "      chat_wrapped = self.apply_chat_template\n",
        "\n",
        "    stripped = text.strip()\n",
        "    if stripped in self._special_to_id and \"\\n\" not in stripped:\n",
        "      return [self._special_to_id[stripped]]\n",
        "\n",
        "    if chat_wrapped:\n",
        "      text = self._wrap_chat(text)\n",
        "    ids =[]\n",
        "    for part in filter(None , self._SPLIT_RE.split(text)):\n",
        "      if part in self._special_to_id:\n",
        "        ids.append(self._special_to_id[part])\n",
        "      else:\n",
        "        ids.extend(self._tok.encode(part).ids)\n",
        "    return ids\n",
        "\n",
        "   def decode(self,ids):\n",
        "    return self._tok.decode(ids,skip_special_tokens = True)\n",
        "   def _wrap_chat(self,user_msg):\n",
        "    s = f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
        "    if self.add_generation_prompt:\n",
        "      s += \"<|im_start|>assistant\"\n",
        "      if self.add_thinking:\n",
        "        s += \"\\n\"\n",
        "      else:\n",
        "        s += \"\\n<think>\\n\\n</think>\\n\\n\"\n",
        "    return s\n"
      ],
      "metadata": {
        "id": "cYcOhw39Fxzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_REASONING_MODEL:\n",
        "    tokenizer_file_path = f\"Qwen3-{CHOOSE_MODEL}/tokenizer.json\"\n",
        "else:\n",
        "    tokenizer_file_path = f\"Qwen3-{CHOOSE_MODEL}-Base/tokenizer.json\"\n",
        "\n",
        "hf_hub_download(\n",
        "    repo_id=repo_id,\n",
        "    filename=\"tokenizer.json\",\n",
        "    local_dir=local_dir,\n",
        ")\n",
        "\n",
        "tokenizer = Qwen3Tokenizer(\n",
        "    tokenizer_file_path=tokenizer_file_path,\n",
        "    repo_id=repo_id,\n",
        "    add_generation_prompt=USE_REASONING_MODEL,\n",
        "    add_thinking=USE_REASONING_MODEL\n",
        ")"
      ],
      "metadata": {
        "id": "JSD_dBrbTKAe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}